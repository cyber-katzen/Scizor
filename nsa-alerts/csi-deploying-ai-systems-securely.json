{
  "markdown": "  \nThis document is marked TLP: CLEAR . Recipients may share this information without restriction . Information is \nsubject to standard copyright rules. For more on the Traffic Light Protocol, see cisa.gov/tlp/ . \n \nU/OO/ 143395 -24 | PP-24-1538  | April 2024 Ver. 1.0  TLP: CLEAR  \nTLP: CLEAR  Joint Cybersecurity Information  \n \n \n \n \nDeploying AI Systems Securely  \nBest Practices for Deploying Secure and Resilient AI Systems  \nExecutive s ummary  \nDeploying artificial i ntelligence (AI) systems securely requires careful setup  and \nconfiguration that depends on the complexity of the AI system , the resources required  \n(e.g., funding, technical  expertise) , and the infrastructure used ( i.e., on premise s, cloud, \nor hybrid). This report expands upon the  secure deployment  and secure operation and \nmaintenance sections of the  Guidelines for secure AI system development  and \nincorporates  mitigation considerations from Engaging with Artificial Intelligence (AI) . It is \nfor organizations deploying and operating AI systems designed and developed by \nanother entity . The best practices may not be applicable to all environments,  so the \nmitigations should be adapted to specific use cases and threat profiles . [1], [2]  \nAI security is a rapidly evolving area of research. As agencies, industry, and academia \ndiscover potential  weaknesses in AI technology  and techniques to exploit  them , \norganizations will need to update their  AI systems to address the  changing  risks, in \naddition to applying traditional IT best practices to AI systems.  \nThis report was authored by the U.S. National Security Agency s Artificial Intelligence \nSecurity Center  (AISC) , the Cybersecurity and Infrastructure Security Agency (CISA), \nthe Federal Bureau of Investigation (FBI), the Australian Signals Directorates Australian \nCyber Security Centre (ACSC), the Canadian Centre for Cyber Security (CCCS), the \nNew Zeal and National Cyber Security Centre (NCSC -NZ), and the United Kingdoms \nNational Cyber Security Centre  (NCSC -UK). The goals of the AISC  and the  report are \nto: \n1. Improve  the confidentiality, integrity , and availability of AI system s;  \n2. Assure that known cyberse curity vulnerabilities  in AI systems are appropriately \nmitigated ; and  \n3. Provide methodologies and controls to protect, detect, and respond to malicious \nactivity against AI system s and related  data and services . \n\n \n \nU/OO/ 143395 -24 | PP-24-1538  | April 2024  Ver. 1.0  2 Deploying AI Systems Securely  \nTLP: CLEAR  \nTLP: CLEAR  Scope  and audience  \nThe term AI systems throughout this report refers to machine learning (ML) based \nartificial intelligence (AI)  systems.  \nThese best practices are most applicable to organizations deploy ing and operat ing \nexternally developed  AI systems on premises or in private cloud environments, \nespecially those in high -threat,  high-value  environments. They are not applicable for \norganizations who are not deploying AI systems  themselves and instead are leveraging \nAI systems deployed by others . \nNot all of the guidelines will be direct ly applicable to all organizations or environments. \nThe level of sophistication and the methods of attack will vary depending on the \nadversary targeting the AI system, so organizations should consider the guid ance  \nalongside their use cases and threat profi le. \nSee Guidelines for secure AI system development  for design and development  aspects  \nof AI systems. [1] \nIntroduction  \nThe rapid adoption, deployment, and use of AI  capabilities c an make t hem highly \nvaluable targets  for malicious cyber actors . Actors, who have historically used data theft \nof sensitive information and intellectual property to advance their interests, may seek to \nco-opt deployed AI systems and apply them to malicious ends.  \nMalicious actors targeting AI syst ems may use  attack vectors unique to AI systems, as \nwell as standard techniques  used against tradit ional IT.  Due to the large variety of \nattack vectors, defenses need to be diverse and comprehensive. Advanced malicious \nactors often combine multiple vectors to execute operations that are more complex. \nSuch combinations can more effectively penetrate layered defenses.  \nOrganizations should consider t he following best practices to secure the deployment \nenvironment, continuously protect the AI system, and securely operate and maintain the \nAI system.  \nThe best practices below align wit h the cross -sector Cybersecurity Performance Goals \n(CPGs) developed by CISA and the National Institute of Standards and Technology \n(NIST). The CPGs provide a minimum set of practices and protections that CISA and \nNIST recommend all organizations implement.  CISA and NIST based the CPGs on \n\n \n \nU/OO/ 143395 -24 | PP-24-1538  | April 2024  Ver. 1.0  3 Deploying AI Systems Securely  \nTLP: CLEAR  \nTLP: CLEAR  existing cybersecurity frameworks and guidance to protect against the most common \nand impactful threats, tactics, techniques, and procedures. Visit CISAs Cross -Sector \nCybersecurity Performance Goals  for more information on the CPGs, including \nadditional recommended baseline protections.  \nSecure t he deployment e nvironment  \nOrganizations typically  deploy AI systems within  existing IT infrastructure. Before \ndeployment, they  should ensure that the IT environment applies sound security \nprinciples , such as  robust  governance, a well -designed architecture, and secure \nconfigurations.  For example, ensure that the person responsible and accountable for AI \nsystem cybersecurity is the same person responsible and accountable for the \norganizations cybersecurity in general  [CPG 1.B ]. \nThe security best practices and requirements for IT environments apply to AI systems , \ntoo. The following best practic es are particularly important to apply  to the AI systems \nand the IT environments the organization deploys them in . \nManage d eployment environment governance  \n If an organization outside of IT is deploying or operating the AI system, w ork with the \nIT service department to identify the deployment environment and confirm it meets \nthe organizations IT standards .  \n Understand the organizations risk level and ensure that the AI system and its \nuse is within the organizations risk tolerance  overall and within the risk \ntolerance for the specific IT environment hosting the AI system.  Assess and \ndocument applicable threats , potential impacts , and risk acceptance . [3], [4] \n Identify the roles and responsibilities for each stakeholder  along with how \nthey are accountable for fulfilling them ; identifying these stakehold ers is  \nespecially important should the organization manage their IT environment \nseparately from their AI system . \n Identify the IT environments  security boundaries  and how the AI system fit s \nwithin them.  \n Require the primary developer of the AI system to provide a threat model for the ir \nsystem .  \n\n \n \nU/OO/ 143395 -24 | PP-24-1538  | April 2024  Ver. 1.0  4 Deploying AI Systems Securely  \nTLP: CLEAR  \nTLP: CLEAR   The AI system deployment team should leverage the threat model as a guide \nto implement security best practices, assess potential threats, and plan \nmitigations. [5], [6] \n Consider deployment environment  security requirements when developing  contract s \nfor AI system products or services.  \n Promote a collaborative culture for all parties involved, including the data science, \ninfrastructure, and cybersecurity teams in particular, to allow for teams to voice  any \nrisks or concerns and for the organization to address them appropriately.  \nEnsure a  robust d eployment environment architecture  \n Establish  security  protections for the boundaries between the IT environment and \nthe AI system  [CPG 2.F ]. \n Identify and address blind spots in boundary protections and other security -relevant \nareas  in the AI system the threat model  identifies . For example, ensure the use of an \naccess control system for the AI model weights and limit access  to a set of privil eged \nusers with two -person control (TPC) and two -person integrity (TPI)  [CPG 2.E ]. \n Identify and protect all proprietary data source s the organization will  use in AI model \ntraining or fine -tuning.  Examine the list of data sources, when available, for models \ntrained by others. Maintaining a catalog of trusted and valid data sources will help \nprotect against potential data poisoning or b ackdoor attacks.  For data acquired from \nthird parties, consider contractual or service level agreement (SLA) stipulations as \nrecommended by CPG 1.G  and CPG 1.H . \n Apply secure by design principles and Zero Trust (ZT) frameworks to the \narchitecture to manage risks to and from the AI system. [7], [8], [9] \nHarden  deployment e nvironment  configuration s \n Apply existing security best practices to the deployment environment. This includes \nsandboxing the environment running ML models within hardened containers or \nvirtual m achines (VMs)  [CPG 2.E ], monitor ing the network  [CPG 2.T ], configuring  \nfirewalls with allow lists  [CPG 2.F ], and other best practices , such as those in  NSAs \nTop Ten Cloud Mitigation Strategies  for cloud deployments . \n Review hardware vendor guidance and notifications (e.g., for GPUs, CPU s, \nmemory ) and apply software patches and updates to minimize the risk of exploitation \nof vulnerabiliti es, preferably via the Common Security Advisory Framework (CSAF) . \n[10] \n\n \n \nU/OO/ 143395 -24 | PP-24-1538  | April 2024  Ver. 1.0  5 Deploying AI Systems Securely  \nTLP: CLEAR  \nTLP: CLEAR   Secure sensitive AI information (e.g. , AI model weights, output s, and logs) by \nencrypting the data at rest, and store encryption keys in a hardware sec urity m odule \n(HSM) for later on -demand decryption  [CPG 2.L ].  \n Implement strong authentication mechanisms, access controls, and secure \ncommunication protocols, such as  by using the  latest  version of Transport Layer \nSecurity ( TLS) to encrypt data in transit  [CPG 2.K ].  \n Ensure the use of phishing -resistant multifactor authentication  (MFA) for access  to \ninformation and services.  [2] Monitor for and respond to fraudulent authentication \nattempts  [CPG 2.H ]. [11] \n Understand and mitigate how malicious actors  exploit weak security controls by \nfollowing the mitigations in  Weak Security Controls and Practices Routinely \nExploited for Initial Access .  \nProtect deployment networks from threats  \nAdopt a ZT mindset , which  assume s a breach is inevitable or has already occurred. \nImplement detecti on and response capabilities, enabling quick identification and \ncontainment of compromises.  [8], [9] \n Use well -tested , high-performing  cybersecurity solutions to identify attempts to gain \nunauthorized access  efficiently and enhance the speed and accuracy of incident \nassessments  [CPG 2.G ].  \n Integrate an incident detection system to help prioritize incidents  [CPG 3.A ]. Also \nintegrate a means to immediately block access by users suspected of being \nmalicious  or to disconnect all inbound connections to the AI models and systems in \ncase of a major incident  when a quick response is warranted .  \nContinuously  protect the AI system  \nModels are software, and, like all other software, may have vulnerabilities, other \nweaknesses, or malicious code or properties.  \nValidate the AI system before and during use  \n Use cryptographic methods, digital signatures, and checksums to confirm each \nartifacts origin  and integrity  (e.g., encrypt safetensors to protect their integrity and  \nconfidentiality ), protecting sensitive information from unauthori zed access during AI \nprocesses.  [14] \n\n \n \nU/OO/ 143395 -24 | PP-24-1538  | April 2024  Ver. 1.0  6 Deploying AI Systems Securely  \nTLP: CLEAR  \nTLP: CLEAR   Create  hashes and encrypt ed copies of  each release of the AI model and system  for \narchiv al in a tamper -proof location, stor ing the hash values and/or encryption keys \ninside a secure vault or HSM to prevent  access to both the encryption keys and the \nencrypted data and model at the same location. [1] \n Store all forms of code (e.g., source code, executable code, infrastructure as code ) \nand artifacts (e.g., models, parameters, configurations, data, tests ) in a version \ncontrol system  with proper access control s to ensure only validated code is use d \nand any changes  are tracked . [1] \n Thoroughly test the AI model for robustness, accuracy, an d potential vulnerabilities \nafter modification . Apply techniques, such as adversarial testing, to evaluate the \nmodel's resilience against compromise attempts. [4] \n Prepare for  automated rollbacks and use advanced deployments with a human -in-\nthe-loop as a failsafe to boost reliability, efficiency, and enable continuous delivery \nfor AI systems. In the context of an AI system, rollback capabilities ensure that if a \nnew model or update introduces problems or if the AI system  is compromised , the \norganization can quickly revert to the last known good state to minimize the impact \non users.  \n Evaluate and secure the supply chain for any external AI models and data , making \nsure they adhere to organizational standards and risk management policies, and \npreferring ones developed according to secure by design principles . Make sure that \nthe risks are understood and accepted for parts of the supply chain that cannot \nadhere to organizational standards and policies.  [1], [7]  \n Do not run models right away in the enterprise environment. Carefully inspect \nmodels, especially imported pre -trained models, inside a secure development zone  \nprior to considering them for tuning, training, and deployment.  Use organization -\napproved AI -specific scanners, if and when available, for the detection of potential \nmalicious code to assure model validity before deployment.  \n Consider a utomat ing detection, analysis, and response capabilities, making IT and \nsecurity teams more efficient by giving them insights that enabl e quick and targeted \nreactions to  potential cyber incidents . Perform continuous scans of AI models and \ntheir hosting IT environments to identify possible tampering.  \n When considering whether to use other AI capabilities to make automation \nmore efficient, carefully weigh the risks and benefits, and ensure there is a \nhuman -in-the-loop where needed.  \n\n \n \nU/OO/ 143395 -24 | PP-24-1538  | April 2024  Ver. 1.0  7 Deploying AI Systems Securely  \nTLP: CLEAR  \nTLP: CLEAR  Secure exposed APIs  \n If the AI system exposes application programming interfac es (APIs ), secure them by \nimplementing authentication and authorization mechanisms for API access. Use \nsecure protocols , such as  HTTPS  with encryption and authentication  [CPG 2.C, 2.D, \n2.G, 2.H]. [1] \n Implement validation and sanit ization protocols for all input data to reduce the risk of \nundesired, suspicious , incompatible , or malicious  input being passed to the AI \nsystem  (e.g., prompt injection attacks ). [1] \nActively monitor  model behavior  \n Collect logs  to cover inputs, outputs, intermediate states, and errors; automate alerts \nand tri ggers  [CPG 2.T ].  \n Monitor the model's architecture and configuration settings for any unauthorized \nchanges or unexpected modifications that might compromise the model's \nperformance or s ecurity . [1] \n Monitor for attempts to access or elicit data from the AI model or aggregate \ninference responses.  [1] \nProtect model weights  \n Harden interfaces for access ing model weight s to increase the effort it would take for \nan adversary to exfiltrate the weight s. For example, ensure APIs return only the \nminimal data required for the task to inhibit model inversion.  \n Implement hardware protections for model weight storage as feasible . For example,  \ndisable hardware communication capabilities that are not needed and protect \nagainst emanation or side channel te chniques .  \n Aggressively isolate weight storage . For example, store model weights  in a \nprotected storage vault, in a highly restricted zone (HRZ) (i.e., a separate dedicated \nenclave ), or using an HSM  [CPG 2.L ]. [12] \nSecure AI operation and maintenanc e \nFollo w organization -approved IT process es and procedures  to deploy the A I system in \nan approved manner, ensuring the fol lowing controls are implemented.  \n\n \n \nU/OO/ 143395 -24 | PP-24-1538  | April 2024  Ver. 1.0  8 Deploying AI Systems Securely  \nTLP: CLEAR  \nTLP: CLEAR  Enforce strict access controls  \n Prevent unauthorized access or tampering with the AI model. Apply role -based \naccess controls (RBAC) , or preferably attribute -based access controls (ABAC) \nwhere feasible,  to limit access to authorized personnel only.  \n Distinguish between users and administrators. Require  MFA and privileged \naccess workstations ( PAWs ) for administrative access  [CPG 2.H ]. \nEnsure user awareness and training  \nEducate users, administrators, and developers ab out security best practices, such as \nstrong password management, phishing prevention, and secure data handling. Promote \na security -aware culture to minimize the risk of human error. If possible, use a credential \nmanagement system to limit, manage, and moni tor credential use to minimize risks \nfurther [CPG 2.I ]. \nConduct a udits and penetration testing  \n Engage external security experts to conduct a udits and penetration testing on ready -\nto-deploy AI systems . This helps identify vulnerabilities and weaknesses that may \nhave been overlooked internally.  [13], [15] \nImplement robust logging and monitoring  \n Monitor the systems behavior, inputs, and outputs with robust monitoring and \nlogging mechanisms to detect any abnormal  behavior or potential security incidents  \n[CPG 3.A ]. [16] Watch for data drift  or high frequency or repetitive inputs (as  these \ncould be  signs of model compromise or automated compromise attempts ). [17] \n Establish alert systems to notify administrators of potential oracle -style adversarial \ncompromise attempts , security breaches , or anomalies. Timely detection and \nresponse to cyber incidents are critical in safeguarding AI systems . [18] \nUpdate and patch regularly  \n When updating  the model to a new/different version, run a full evaluation to ensure \nthat accuracy, performance, and security te sts are within acceptable lim its before \nredeploying.  \n\n \n \nU/OO/ 143395 -24 | PP-24-1538  | April 2024  Ver. 1.0  9 Deploying AI Systems Securely  \nTLP: CLEAR  \nTLP: CLEAR  Prepare for  high availability (H A) and disaster recovery (DR)   \n Use an immutable backup storage system , depending on the requirements of the \nsystem,  to ensure that  every object, especially log data, is immutable and cannot be \nchange d [CPG 2.R ]. [2] \nPlan s ecure delete  capabilities  \n Perform a utonomous a nd irretrievable deletion of  components , such as training and \nvalidation models or  cryptographic keys , without any retention  or remnants at  the \ncompletion of any process where data and models are exposed or accessible. [19] \nConclusion  \nThe authoring agencies  advise organizations deployin g AI systems to implement robust \nsecurity measures capable of both preventing theft of sensitive data and mitigating \nmisuse  of AI systems . For example, model weights, the learnable parameters of a deep \nneural network , are a particularly critical component to protect . They uniquely represent \nthe result of many costly and challenging prerequisites for training advanced AI models , \nincluding significant compute  resources; collected , processed , and potentially sensitive \ntraining data;  and algorithmic optimizatio ns. \nAI systems are software systems. As such, deploy ing organizations  should prefer \nsystems that are secure by design, where the designer and developer  of the AI system \ntakes an active interest in the positive security outcomes for the system once in \nopera tion. [7] \nAlthough comprehensive implementation of security measures for all relevant attack \nvectors is necessary to avoid significant security gap s, and best practices will change as \nthe AI field and techniques evolve, the following summarizes  some  particularly \nimportant measures:  \n Conduct ongoing compromise assessment s on all devices  where privileged access  \nis used or critical services are performe d. \n Harden  and update  the IT deployment environment.  \n Review the source of AI model s and supply chain  security.  \n Validate the AI system before deployment.  \n Enforce strict access controls and API security for the AI system, employing the \nconcepts of least privilege and defense -in-depth.  \n\n \n \nU/OO/ 143395 -24 | PP-24-1538  | April 2024  Ver. 1.0  10 Deploying AI Systems Securely  \nTLP: CLEAR  \nTLP: CLEAR   Use robust logging, monitoring, and user and entity behavior analytics (UEBA) to \nidentify insider threats  and other malicious activities .  \n Limit and protect  access to the model weights , as they are the essence of the AI \nsystem.  \n Maintain awareness of current and emerging threats , especially in the rapidly \nevolving AI field , and ensure the organization s AI systems are hardened to avoid \nsecurity gaps and vulnerabilities.  \nIn the end, securing an AI system involves an ongoing pr ocess of identifying risks, \nimplementing appropriate mitigations, and monitoring for issues. By taking the steps \noutlined in this report  to secure the deployment and operation of AI systems, an \norganization can significantly reduce the risks involved. Thes e steps help protect the \norganizations intellectual property, models , and data from theft or misuse.  \nImplementing good security practices from the start will set the organization on the right \npath for deploying AI systems success fully. \nWorks cited  \n[1] Nationa l Cyber Security Centre et al. Guidelines for secure AI system development. 2023. \nhttps://www.ncsc.gov.uk/files/Guidelines -for-secure -AI-system -development.pdf   \n[2] Australian Signals Directorate et al. Engaging with Artificial Intelligence (AI). 2024. \nhttps://www.cyber.gov.au/sites/default/f iles/2024 -\n01/Engaging%20with%20Artificial%20Intelligence%20%28AI%29.pdf   \n[3] MITRE. ATLAS (Adversarial Threat Landscape for Artificial -Intelligence Systems) Matrix  version \n4.0.0 . 2024. https://atlas.mitre. org/matrices/ATLAS  \n[4] National Institute of Standards and Technology. AI Risk Management Framework  1.0. 2023. \nhttps://www.nist.gov/itl/ai -risk-management -framework   \n[5] The Open Worldwide Appli cation Security Project (OWASP). LLM AI Cybersecurity & \nGovernance Checklist. 2024. https: //owasp.org/www -project -top-10-for-large -language -model -\napplications/llm -top-10-governance -doc/LLM_AI_Security_and_Governance_Checklist -v1.pdf   \n[6] The Open Worldwide Application Security Project (OWASP). OWASP Machine Learning \nSecurity Top Ten Security Risks . 2023. https://owasp.org/www -project -machine -learning -security -\ntop-10/  \n[7] Cybersecurity and Infrastructure Security Agency. Secure by Design. 2023. \nhttps://www.cisa.gov/securebydesign   \n[8] National Security Agency. Embracing a Zero Trust Security Model. 2021. \nhttps://media.defense.gov/2021/Feb/25/2002588479/ -1/-\n1/0/CSI_EMBRACING_ZT_SECURITY_MODEL_UOO115131 -21.PDF  \n[9] Cybersecurity and Infrastructure Security Agency. Zero Trust Maturity Model. 2022. \nhttps://www.cisa.gov/zero -trust-maturity -model   \n[10] Cybersecurity and Infrastructure Security Agency. Transforming the Vulnerability Management \nLandscape. 2022. https://www.cisa.gov/news -events/news/transforming -vulnerability -\nmanagement -landscape   \n\n \n \nU/OO/ 143395 -24 | PP-24-1538  | April 2024  Ver. 1.0  11 Deploying AI Systems Securely  \nTLP: CLEAR  \nTLP: CLEAR  [11] Cybersecurity and Infrastructure Security Agency. Implementing Phishing -Resistant MFA. 2022. \nhttps://www.cisa.gov/sites/default/files/publications/fact -sheet -implementing -phishing -resistant -\nmfa-508c.pdf   \n[12] Canadian Centre for C yber Security. Baseline security requirements for network security zones \nVer. 2.0 (ITSP.80.022). 2021. https://www.cyber.gc.ca/en /guidance/baseline -security -\nrequirements -network -security -zones -version -20-itsp80022   \n[13] Ji, Jessica. What Does AI Red -Teaming Actually Mean? 2023. \nhttps://cset.george town.edu/article/what -does -ai-red-teaming -actually -mean/   \n[14] Hugging Face GitHub. Safetensors. 2024. https://github.com/huggingface/safetensors . \n[15] Michael Feffer, Anusha Sinha, Zachary C. Lipton, Hoda H eidari. Red -Teaming for Generative AI: \nSilver Bullet or Security Theater? 2024. https://arxiv.org/abs/2401.15897   \n[16] Google. Google's Secure AI Framework (SAIF). 2023. https://safety.google/cybersecurity -\nadvancements/saif/   \n[17] Government Accountability Office (GAO). Artificial Intelligence: An Accountability Framework for  \nFederal Agencies and Other Entities . 2021. https://www.gao.gov/assets/gao -21-519sp.pdf   \n[18] RiskInsight . Attacking AI? A real -life example!. 2023.  https://riskinsight -\nwavestone.com/en/2023/06/attacking -ai-a-real-life-example   \n[19] National Cyber Security Centre. Principles for the security of machine learning. 2022. \nhttps://www.ncsc.gov.uk/files/Principles -for-the-security -of-machine -learning.pdf  \nDisclaimer of endorsement  \nThe information and opinions contained in this document are provided \"as is\" and without any warranties or \nguarantees. Reference herein to any s pecific commercial products, process, or service by trade name, trademark, \nmanufacturer, or otherwise, does not constitute or imply its endorsement, recommendation, or favoring by the United \nStates Government, and this guidance shall not be used for advert ising or product endorsement purposes.   \nPurpose  \nThis document was developed  in furtherance of the authoring organizations  cyber security missions, including their  \nresponsibilities to identify and disseminate threats, and to develop and issue cybersecurity specifications and \nmitigations. This information may be shared broadly to reach all appropriate stakeholders.  \nContact  \nU.S. organizations:  \nNSA Cybersecurity Report Feedback: CybersecurityReports@nsa.gov  \nNSA General Cybersecurity Inquiries or Customer Requests: Cybersecurity_Requests@nsa.gov  \nDefense Industrial Base Inquiries and Cybersecurity Services: DIB_Defense@cyber.nsa.gov  \nNSA Media Inquiries / Press Desk: 443 -634-0721, MediaRelations@nsa.gov  \nReport incidents and anomalous activity to CISA 24/7 Operations Center at report@cisa.gov or (888) 282 -\n0870 and/or to the FBI via your local FBI field office . \nAustralian organizations:  For more information or to report a cybersecurity incident, visit cyber.gov.au  or call 1300 \n292 371 (1300 CYBER1).  \nCanadian organizations: For more information contact the Cyber Centre at contact@cyber.gc.ca  or report a cyber \nsecurity incident to our portal at https://www.cyber.gc.ca/en/i ncident -management . \nNew Zealand organizations: Report cyber security incidents to incidents@ncsc.govt.nz or call 04 498 7654.  \nUnited Kingdom organizations: Report a significant cyber security incident at ncsc.gov.uk/report -an-incident \n(monitored 24 hours) or, for urgent assistance, call 03000 200 973.  \n\n",
  "cves": [],
  "techniques": [],
  "advisory": "cybersecurity-alerts",
  "title": "csi-deploying-ai-systems-securely",
  "source": "nsa",
  "id": "de756a15c0fd79bdd5126781f420d0053900d65c838cb40c876bb826d8b49bb7"
}