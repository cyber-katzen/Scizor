{
  "markdown": " \nU/OO/134094- 20          PP-20-0901 21 APRIL 2020  \nCybersecurity Information  \nNational  \nSecurity  \nAgency  \nDetect and Prevent Web Shell Malware  \nSummary  \nCyber actors have increased the use of web shell malware for computer network exploitation [1][2][3][4]. Web shell \nmalware is software deployed by a hacker, usually on a victims web server. It can be used to execute arbitrary system \ncommands, which are commonly sent over HTTP or HTTPS. W eb shell attacks pose a serious risk to DoD components . \nAttackers often create web shells by adding or  modifying a file in an existing web application. Web shells provide \nattackers with persistent access to a compromised network using communication channels disguised to blend in with legitimate traffic. Web shell malware is a long- standing, pervasive threat that continues to evade many security tools.  \nCyber actors deploy web shells by exploiting web application vulnerabilities or uploading to otherwise compromised systems. Web shells can serve as persistent backdoors or as relay nodes to route attacker com mands to other systems. \nAttackers frequently chain together web shells on multiple compromised systems to route traffic across networks, such as \nfrom internet -facing systems to internal networks  [5]. \nIt is a common misperception that only internet -facing s ystems are targeted for web shells. Attackers frequently deploy \nweb shells on non- internet facing web servers, such as internal content management systems or network device \nmanagement interfaces. Internal web applications are often more susceptible to compromise due to lagging patch \nmanagement or permissive security requirements.  \nThough the term web shells is predominantly associated with malware, it can also refer to web- based system \nmanagement tools used legitimately by administrators. While not the foc us of this guidance, these benign web shells may \npose a danger to organizations as weaknesses in these tools can result in system compromise. Administrators should use \nsystem management software leveraging enterprise authentication methods, secure communic ation channels, and \nsecurity hardening.  \nMitigating Actions (DETECTION)  \nWeb shells are difficult to detect as they are easily modified by attackers and often employ encryption, encoding, and \nobfuscation. A defense- in-depth approach using multiple detection capabilities is most likely to discover web shell \nmalware. Detection methods for web shells may falsely flag benign files. When a potential web shell is detected, administrators should validate the files origin and authenticity. Detection techniques include:  \nKnown- Good Comparison  \nWeb shells primarily target existing web applications and rely on creating or modifying files. The best method of detecting \nthese web shells is to compare a verified benign version of the web application (i.e., a known- good) against the \nproduction version. Discrepancies should be manually reviewed for authenticity. Additional information and scripts to \nenable known- good comparison are available in Appendix A  and are maintained on \nhttps://github.com/nsacyber/Mitigating- Web- Shells . \nWhen adjudicating discrepancies with a known- good image, administrators are cautioned against trusting timestamps on \nsuspicious systems. Some attackers use a technique known as timestomping [6] to alter created and modified times in \norder to add legitimacy to web shell files. Administrators should not assume that a modification is authentic simply \nbecause it appears to have occurred during a maintenance period. However, as an initial tri age method, administrators \nmay choose to prioritize verification of files with unusual timestamps.  \nWeb Traffic Anomaly Detection  \nWhile attackers often design web shells to blend in with normal web traffic, some characteristics are difficult to imitate without advanced knowledge. These characteristics include user agent strings and client Internet Protocol (IP) address \nspace. Prior to having a presence on a network, attackers are unlikely to know which user agents or IP addresses are \n\n \n \nU/OO/134094- 20          PP-20-0901 21 APRIL 2020  \n 2 NSA & ASD: Detect and Prevent Web Shell Malware  \ntypical for a web server , so web shell requests will appear anomalous. In addition, web shells routing attacker traffic will \ndefault to the web servers user agent and IP address, which should be unusual in network traffic. Uniform Resource \nIdentifiers (URIs) exclusively accessed by anomalous user agents are potentially web shells. Finally, some attackers \nneglect to disguise web shell request referer [sic] headers1 as normal traffic. Consequently, requests with missing or \nunusual referer headers could indicate web shell presence. Centralized log- querying capabilities, such as Security \nInformation and Event Management (SIEM) systems,  provide a means to implement this analytic. If such a capability is \nnot available, administrators may use scripting to parse web server logs to identify possible web shell URIs. Example \nSplunk2 queries ( Appendix B ), scripts for analyzing log data ( Appendix C ), and  additional information about detecting \nweb traffic anomalies are maintained at https://github.com/nsacyber/Mitigating- Web -Shells . \nSignature -Based Detection  \nFrom the host perspective, signat ure-based detection is unreliable because web shells may be obfuscated and are easy \nto modify. However, some cyber actors use popular web shells (e.g., China Chopper, WSO, C99, B374K, R57) with \nminimal modification. In these cases, fingerprint or expression -based detection may be possible. A collection of Snort3 \nrules to detect common web shell files, scanning instructions, and additional information about signature- based detection \nare maintained at https://github.com/nsacyber/Mitigating- Web- Shells . \nFrom the network perspective, signature- based detection of web shells is unreliable because web shell communications \nare frequently obfuscated or encrypted. Additionally, hard- coded values like variable names are easily modified to further \nevade detection. While unlikely to discover unknown web shells, signature- based network detection can help identify \nadditional infections of a known web shell. Appendix D provides a collection of signatures to detect network \ncommunication from common, unmodified or slightly modified web shells sometimes deployed by attackers. This list is \nalso maintained at https://github.com/nsacyber/Mitigating- Web-Shells . \nUnexpected Network Flows  \nIn some cases, attackers use web shells on systems other than web servers (e.g., workstations). These web shells \noperate on rogue web server applications and can evade file- based detection by running exclusively in memory (i.e., \nfileless execution). While functionally similar to a traditional Remote Access Tool (RAT), these types of web shells allow \nattackers to easily chain malicious traffic through a uniform platform. These types of web shells can be detected on well -\nmanaged networks because they listen a nd respond on previously unused ports. Additionally, if an attacker is using a \nperimeter web server to tunnel traffic into a network, connections would be made from a perimeter device to an internal \nnode. If administrators know which nodes on their network  are acting as web servers, then network analysis can reveal \nthese types of unexpected flows. A variety of tools including vulnerability scanners (e.g., Nessus4), intrusion detection \nsystems (e.g., Snort), and network security monitors (e.g., Zeek5 [formerly Bro]) can reveal the presence of \nunauthorized web servers in a network. Maintaining a thorough and accurate depiction of expected network activity can \nenhance defenses against many types of attack. The Snort rule in Appendix E and maintained at \nhttps://github.com/nsacyber/Mitigating- Web- Shells  can be tailored for a specific network to identify unexpected network \nflows.   \nEndpoint Detection and Response (EDR) Capabilities  \nSome EDR and enhanced host logging solutions may be able to detect web shells based on system call or process \nlineage abnormalities. These security products monitor each process on the endpoint including invoked system calls. Web \nshells usually cause the web server process to exhibit unusual behavior. For instance, it is uncommon for most benign \nweb servers to launch the ipconfig utility, but this is a common reconnaissance technique enabled by web shells. EDRs \nhave different automated capabilities and querying interfac es, so organizations are encouraged to review documentation \nor discuss web shell detection with the vendor. Appendix F illustrates how Sysmons enhanced process logging data can \n                                            \n1 Referer is an HTTP header specified in Internet Engineering Task Force RFC 7231  \n2 Splunk is a registered trademark of Splunk, Inc.  \n3 Snort is a registered trademark of Cisco Technologies, Inc.  \n4 Nessus is a registered trademark of Tenable Network Security, Inc.  \n5 Zeek is a trademark of the Zeek Project  \n\n \n \nU/OO/134094- 20          PP-20-0901 21 APRIL 2020  \n 3 NSA & ASD: Detect and Prevent Web Shell Malware  \nbe used to identify process abnormalities in a Microsoft Windows6 environment. Similarly, Appendix G illustrates how \nauditd can be used to identify process abnormalities in a Linux7 environment.  Guidance for these identifying process \nabnormalities in these environments is also maintained at https://github.com/nsacyber/Mitigating- Web- Shells . \nOther Anomalous Network Traffic Indicators  \nWeb shell traffic may exhibit other detectable abnormal characteristics depending on the attacker . In particular, unusually \nlarge responses (possible data exfiltration), recurring off -peak access times (possible non- local work schedule), and \ngeographically disparate requests (possible foreign operator) could indicate URIs of potential web shells. However, these \ncharacteristics are hi ghly subjective and likely to flag many benign URIs. Administrators may choose to implement these \ndetection analytics if the baseline characteristic is uniform for their environment.  \nMitigating Actions ( PREVENTION ) \nPreventing web shells should be a priorit y for both internet -facing and internal web servers. Good cyber hygiene and a \ndefense- in-depth approach based on the mitigations below provide significant hardening against web shells. Prevention \ntechniques include:   \nWeb Application Update Prioritization \nAttackers  sometimes target vulnerabilities in internet -facing and internal web applications within 24 hours of a patch \nrelease. Update these applications as soon as patches are available. Whenever possible, enable automatic updating and \nconfigure frequent u pdate cadence (at least daily). Deploy manual updates on a frequent basis when automatic updating \nis not possible.  Appendix H lists some commonly exploited vulnerabilities.  \nWeb Application Permissions  \nWeb services should follow  the least privilege security  paradigm. In particular, web applications should not have \npermission to write directly to a web accessible directory or modify web accessible code. Attackers are unable to upload a \nweb shell to a vulnerable application if the web server blocks access to t he web accessible directory. To preserve \nfunctionality, some web applications require configuration changes to save uploads to a non- web accessible area. Prior to \nimplementing this mitigation, consult documentation or discuss changes with the web applicati on vendor . \nFile Integrity Monitoring \nIf administrators are unable to harden web application permissions as described above, file integrity monitoring can achieve a similar effect. File integrity software can block file changes to web accessible directories  or alert when changes \noccur. Additionally, monitoring software has the benefit of allowing certain file changes but blocking others. For example, if \nan internal web application handles only Portable Document Format ( PDF) files, integrity monitoring can bl ock uploads \nwithout a .pdf extension. Appendix I provides  a set of Host Intrusion Prevention System  (HIPS) rules for use with \nMcAfee\n8 Host Based Security System (HBSS) to enforce file integrity on  web accessible directories. These  rules, \nimplementation instructions, and additional information about file integrity monitoring are maintained at \nhttps://github.com/nsacyber/Mitigating- Web- Shells . \nIntrusion Prevention  \nIntrusion Prevention Systems (IPS) and Web Application Firewalls (WAF) each add a layer of defense for web \napplications by blocking some known attacks. Organizations should implement these appliances to block known malicious \nuploads. If possible, administrator s are encouraged to implement the OWASP9 Core Rule Set, which includes patterns \nfor blocking certain malicious uploads. As with any signature- based blocking, attackers will find ways to evade detection, \n                                            \n6 Microsoft and Windows are registered trademarks of the Microsoft Corporation  \n7 Linux is a registered trademark of the Linux Foundation  \n8 McAfee is a registered trademark of McAfee, LLC  \n9 OWASP is a trademark of the OWASP Foundation  \n\n \n \nU/OO/134094- 20          PP-20-0901 21 APRIL 2020  \n 4 NSA & ASD: Detect and Prevent Web Shell Malware  \nso this approach is only one part of a defense- in-depth strategy. Note that IPS and WAF appliances may block the initial \ncompromise but are unlikely to detect web shell traffic.  \nTo maximize protection, security appliances should be tailored to individual web applications rather than using a single \nsolution across all web servers. For instance, a security appliance configured for an organizations content management \nsystem can include application specific rules to harden targeted weaknesses that should not apply to other web \napplications. Additionally, secur ity appliances should receive updates to enable real time mitigations for emerging threats.  \nNetwork Segregation  \nNetwork segregation is a complex architectural challenge that can have significant benefits when done correctly. Network segregation hinders web shell propagation by preventing connections between unrelated network segments. The simplest \nform of network segregation is isolatin g a demilitarized zone (DMZ) subnet to quarantine internet -facing servers. \nAdvanced forms of network segregation use software -defined networking (SDN) to enable a Zero Trust\n10 architecture, \nwhich  requir es explicit authorization for communication between nodes. While web shells could still affect a targeted \nserver, network segmentation prevents attackers  from chaining web shells  to reach deeper into an organizations network. \nFor additional information about network segregation, see Segregate Networks and Functions  [7] on nsa.gov .  \nHarden Web Servers  \nSecure configuration of web servers  and web applications  can prevent web shells and other compromises. Administrators \nshould block access to unused ports or services. Employed services should be restricted to expected clients  if possible. \nAdditionally, routine vulnerability scans can help to identify unknown weaknesses in an environment. Some host -based \nsecurity systems provide advanced features, such as machine learning and file reputation, which provide some protection \nagainst web shells. Organizations should take advantage of these advanced security features when possible.  \n \nMitigating Actions ( RESPONSE and RECOVERY) \nWhile some web shells do not persist, running entirely from memory, and others exist only as binaries or scripts in a web \ndirectory, still others can be deeply rooted with sophisticated persistence mechanisms. Regardless, they may be part of a \nmuch larger intrusion campaign. A critical focus once a web shell is discovered should be on how far the attacker \npenetrated within the network.  Packet capture (PCAP) and network flow data can help to determine if the web shell was \nbeing used to pivot within the network, and to where. If such a pivot is cleaned up without discovering the full extent of the intrusion and evicting the attacker, that access may be regained through other channels either immediately or at a later \ntime.   \n  \n                                            \n10 Zero Trust is a model where both internal and external resources are treated as potentially malicious and thus each system verifies all access \n\n \n \nU/OO/134094- 20          PP-20-0901 21 APRIL 2020  \n 5 NSA & ASD: Detect and Prevent Web Shell Malware  \nAppendix A: Scripts to Compare a Production Website to a Known-Good Image  \nThe scripts below can be used to compare the directory of  an active website against a known- good image of that site. \nThis script requires file level access to both the production site and the known- good image, so it should be run on the web \nserver hosting the site or on a connected system that has a mapped drive to the web server. The script should be run with \nsufficient privileges to read the files in both directories. Alternatively, for Windows systems, Microsoft developed the \nWinDiff utility (available at https://support.microsoft.com/en -us/help/159214/how -to-use-the-windiff -exe-utility ), which \nallows directory comparison using a Graphical User Interface (GUI).  \n \nMICROSOFT POWERSHELL11 \nUSAGE  .\\dirChecker.ps1 -knownGood  <known -good  image path> -productionImage <production image path>   \nSCRIPT  <# \n.DESCRIPTION  \nThe script looks for files changes/additions between a production directory (target) and  a known -good directory.  \n \n.PARAMETER knownGood \nPath of the known-good directory.  \n \n.PARAMETER productionImage  \nPath of the production directory (target).  \n \n-- Output -- \nFile analysis started.  \nAny file listed below is a new or changed file.  \n \nC:\\inetput \\wwwroot \\index 2.aspx  \n \nFile analysis completed.  \n#> \nparam (  \n    [Parameter(Mandatory=$TRUE)][ValidateScript({Test -Path $_ -PathType 'Container'})][String] $knownGood,  \n    [Parameter(Mandatory=$TRUE)][ValidateScript({Test -Path $_ -PathType 'Container'})][String] $productionImage  \n) \n \n# Recursevely get all files in bot h directories, for each file calculate hash.  \n$good = Get -ChildItem -Force -Recurse -Path $knownGood | ForEach-Object { Get -FileHash -Path $_.FullName }  \n$prod = Get -ChildItem -Force -Recurse -Path $productionImage | ForEach-Object { Get -FileHash -Path $_.Fu llName }  \n \nWrite -Host \"File analysis started.\"  \nWrite -Host \"Any file listed below is a new or changed file.`n\"  \n \n# Compare files hashes, select new or changed files, and print the path+filename.  \n(Compare-Object $good $prod -Property hash -PassThru | Where-O bject{$_.SideIndicator -eq '=>'}).Path  \n \nWrite -Host \"`nFile analysis completed.\"  \n \nLINUX DIFF UTILITY  \nUSAGE  diff -r -q <known -good  image path> <production image path>  \nCMD  diff -r -q /path/to/ good /image /path/to/production/site  \n  \n                                            \n11 PowerShell is a regis tered trademark of Microsoft Corporation  \n\n \n \nU/OO/134094- 20          PP-20-0901 21 APRIL 2020  \n 6 NSA & ASD: Detect and Prevent Web Shell Malware  \n \nAppendix B: Splunk Queries for Detecting Anomalous URIs in Web Traffic  \nPrior to having a presence on the network, attackers are unlikely to be able to disguise web shell traffic as typical traffic  for \na targeted web server. In these cases, requests to the web shell are likely to have an unusual user agent string. In some \nenvironments, the attackers IP address may also appear uncharacteristic for typical network traffic. The queries below \ncan highlight URIs requested by unusual user agents and client IP addresses. Administrators are encouraged to tailor \nthese queries to individual environments including targeting individual web applications or servers rather than running the \nquery for an entire network. In rare cases, certain web applications may generate unique URIs per requ est which would \nlimit the effectiveness of these queries.  \nSPLUNK QUERY TO IDENTIFY U RIS ACCESSED BY FEW USER AGENTS AND IP A DDRESSES  \nRATIONALE  Unlike benign URIs, web shell URIs are likely to have few user agents  \nQUERY  \n(APACHE12) sourcetype=\" access_combined  \n| fillnull value= - comment(Fill all empty fields with -)  \n| search status>=\"200\" status <\"300\"  uri!=- clientip!= - `comment(\"Only successful codes 200 -299, eliminate blank \nURIs and client IPs\")`  \n| stats min(_time) as start max(_time) as stop dc(useragent) as dc_user_agent values(useragent) as \nvalues_user_agent dc(clientip) as dc_src values(clientip) as values_src count by uri `comment(\"Find first and last \ntime the grouping was found, number of distinct User Agent strings and IP addresses used to access that URI\")`  \n| convert ctime(start) ctime(stop) `comment(\"Convert the times to a readable format\")`  \n| search dc_src<=5 OR dc_user_agent<=5 `comment(\"O nly URIs with <=5 unique user agents or IP addresses \")` \n| table start stop uri dc_user_agent  values_user_agent dc_src values_src  \n \nQUERY  \n(IIS13) sourcetype=\" iis\" \n| fillnull value= - comment(Fill all empty fields with -)  \n| search sc_ status>=\"200\" sc_status <\"300\"  cs_uri_stem !=- c_ip!=- `comment(\"Only successful codes 200 -299, \neliminate blank URIs and client IPs\")`  \n| stats min(_time) as start max(_time) as stop dc( cs_User_Agent ) as dc_user_agent values( cs_User_Agent ) as \nvalues_user_agent dc(c _ip) as dc_src values(c _ip) as values_src count by cs_uri_stem  `comment(\"Find first and \nlast time the gr ouping was found, number of distinct User Agent strings and IP addresses used to access that \nURI\")`  \n| convert ctime(start) ctime(stop) `comment(\"Convert the times to a readable format\")`  \n| search dc_src<=5 OR dc_user_agent<=5 `comment(\"Only URIs with <=5 u nique user agents or IP addresses\")`  \n| table start stop cs_uri_stem  dc_user_agent values_user_agent dc_src values_src  \n \n \nSPLUNK QUERY TO IDENTIFY U SER AGENTS UNCOMMON FOR A TARGET WEB SER VER \nRATIONALE  Particularly f or internal web applications, uncommon user agents can indicate web shell activity  \nQUERY  \n(APACHE) sourcetype=\" access_combined\"  \n| fillnull value= - comment(Fill all empty fields with -)  \n| search status>=\"200\" status <\"300\" `  comment(\"Only successful  codes 200-299 )` \n| stats count by useragent `comment(\"Group User Agent strings to determine frequency\")`  \n| sort + count `comment(\"Sort count in ascending order\")`  \n| head 10 `comment(\"Limit results to top 10. This can be changed to see  more or fewer results\")`  \n \nQUERY  \n(IIS) sourcetype=\" iis\" sc_status>=\"200\" AND sc_status<\"300\" ` comment(\"Only successful codes 200 -299)`  \n| fillnull value= - comment(Fill all empty fields with -)  \n| search sc_ status>=\"200\" sc_status <\"300\"  `comment(\"Only successful codes 200-299\")`  \n| stats count by cs_User_Agent  `comment(\"Group User Agent strings to determine frequency\")`  \n| sort + count `comment(\"Sort count in ascending order\")`  \n| head 10 `comment(\"Limit results to top 10. This can be changed to see more or fewer results\")`  \n \n \n                                            \n12 Apache is a registered trademark of the Apache Software Foundation  \n13 Internet Information Services (IIS) is a trademark of the Microsoft Corporation  \n\n \n \nU/OO/134094- 20          PP-20-0901 21 APRIL 2020  \n 7 NSA & ASD: Detect and Prevent Web Shell Malware  \n \nSPLUNK QUERY TO IDENTIFY U RIS WITH AN UNCOMMON  HTTP REFERER  \nRATIONALE  Web shell URIs are li kely to have uncommon HTTP refe rers \nQUERY  \n(APACHE) sourcetype=\" access_combined\"  \n| fillnull value= - comment(Fill all empty fields with - (needed to make blank referer  fields searchable) ) \n| search status>=\"200\" status <\"300\" `comment(\" Only success ful codes 200-299 \")` \n| stats dc(uri) as dc_URIs values(uri) as All_URIs count by referer `comment(\"Counts number of times each URI \nrequest is associated with a unique referer \")`  \n| table referer, All_URIs, dc_URIs  \n| sort + dc_URIs `comment(\"Sort count in ascending order\")`  \n| head 10 `comment(\"Limit results to top 10. This can be changed to see  more or fewer results\")`  \n \nQUERY  \n(IIS) sourcetype=\"  iis\"  \n| fillnull value= - comment(Fill all empty fields with - (needed to make blank referer fields searchable) ) \n| search sc_ status >=\"200\" sc_status <\"300\" `comment(\"Only successful codes 200-299\")`  \n| stats dc( cs_uri_stem ) as dc_URIs values( cs_uri_stem ) as All_URIs count by cs_Referer  `comment(\"Counts \nnumber of times each URI request is associated with a unique referer\")`  \n| table cs_Referer , All_URIs, dc_URIs  \n| sort + dc_URIs `comment(\"Sort count in ascending order\")`  \n| head 10 `comment(\"Limit results to top 10. This ca n be changed to see more or fewer results\")`  \n \n \nSPLUNK QUERY TO IDENTIFY U RIS MISSING AN HTTP REFERER  \nRATIONALE  Web shell URIs are likely to have missing HTTP referrers  \nQUERY  \n(APACHE) sourcetype=\" access_combined \"  \n| fillnull value= - comment(Fill all empty fields with - (needed to make blank  referer fields searchable) ) \n| search status>=200 status<300 referrer= - uri!=/ `comment(\"Only successful codes 200-299 and blank referrer \nnot from root webpage\")  \n| stats count by referer, uri `comment(\"Counts number of times each URI request is associated with a unique \nreferer\")`  \n| table uri , count  \n| sort - count `comment(\"Sort count in descending order\")`  \n| head 10 `comment(\"Limit results to top 10. This can be changed to add more or fewer results\")`  \n \nQUERY  \n(IIS) sourcetype=\"iis\"  \n| fillnull value= - comment(Fill all empty fields with - (needed to make blank referer fields searchable) ) \n| search sc_ status >=\"200\"  sc_status <\"300\"  sc_Referer= - cs_uri_stem!=\"/\" `comment(\"Only looking for successful \nstatus codes 200-299  and blank referer not from the root webpage\")`  \n| stats count by cs_Referer, cs_uri_stem `comment(\"Counts number of times each URI request is associated with a \nunique referer\")`  \n| table cs_uri_stem, count  \n| sort - count `comment(\"Sort count in descending order\")`  \n| head 10 `comment(\"Limit results to top 10. This can be changed to add more or fewer results\")`  \n \n \n  \n\n \n \nU/OO/134094- 20          PP-20-0901 21 APRIL 2020  \n 8 NSA & ASD: Detect and Prevent Web Shell Malware  \nAppendix C: Internet Information Services  (IIS) Log Analysis Tool  \nPrior to having a presence on the network, attackers are unlikely to be able to disguise web shell traffic as typical traffic for \na targeted web server. In these cases, requests to the web shell are likely to have an unusual user agent string. In some environments, the attackers IP address may also appear uncharacteristic for typical network traffic. The PowerShell and \nPython scripts below can highlight URIs requested by unusual user agents and client IP addresses. In rare cases, certain \nweb applications may generate unique URIs per request,  which would limit the effectiveness of these queries.   \nMICROSOFT POWERSHELL SCRIPT TO ANALYZE I IS LOGS  \nUSAGE  .\\LogCheck.ps1 -logDir <path to IIS log directory>  \nSCRIPT  #Default parameters  \nParam  (  \n     [ValidateScript({Test -Path $_ -PathType 'Container'})][string]$logDir = \"C: \\inetpub\\ logs\\\",  \n     [ValidateRange(1,100)][int]$percentile = 5  \n) \n \nIf ($ExecutionContext.SessionState.LanguageMode -eq \"ConstrainedLanguage\")  \n    { Throw \"Use Full Language Mode (https://devblogs.microsoft.com/powershell/powershell -constrained- language-\nmode/)\" }  \n \nfunction analyzeLogs ( $field ) {  \n    $URIs = @{}  \n    $files = Get -ChildItem -Path $logDir -File -Recurse  \n    If ($files.Length -eq 0)  { \"No log files at the given location `n$($_)\"; Exit }  \n \n    #Parse each file for relevant data. If data not present, continue to next file  \n    $files | Foreach -Object {  \n        Try {  \n            $file = New -Object System.IO.StreamReader -Arg $_.FullName  \n            $Cols = @()  \n            While ($line = $file.R eadLine()) {  \n                If ($line -like \"#F*\") {  \n                    $Cols = getHeaders($line)  \n                } ElseIf ($Cols.Length -gt 0 -and $line - notlike \"#*\" ) {  \n                    $req = $line | ConvertFrom -Csv -Header $Cols  -Delimiter ' '  \n                    If ( IrrelevantRequest $req ) { Continue; }  \n                    #If target field seen for this URI, update our data; otherwise create data object for this URI/field  \n                    If ($URIs.ContainsKey($req.uri) -and $URIs[ $req.ur i ].ContainsKey($req.$field) )  \n                        { $URIs[ $req.uri ].Set_Item( $req.$field, $URIs[ $req.uri ][ $req.$field ] + 1 ) }  \n                    ElseIf ($URIs.ContainsKey($req.uri))   \n                        { $URIs[ $req.uri ].Add( $req.$fi eld, 1 ) }  \n                    Else  \n                        { $URIs.Add($req.uri, @{ $($req.$field) = 1 }) }  \n                }  \n            }  \n            $file.close()  \n        } Catch {  \n            Echo \"Unable to parse log file $($_.FullName)`n$($_)\"  \n        }  \n    } \n \n    Echo \"These URIs are suspicious because they have the least number of $($field)s requesting them:\"  \n    $nth_index = [math]::ceiling( ($URIs.Count) * ([decimal]$percentile / 100))  \n \n    #Count the unique fields for each URI  \n    ForEach ($key in $($uris.keys)) { $uris.Set_Item( $key, $uris.$key.Count) }  \n     \n    $i = 0;  \n    $URIs.GetEnumerator() | sort Value | Foreach-Object {  \n        $i++ \n\n \n \nU/OO/134094- 20          PP-20-0901 21 APRIL 2020  \n 9 NSA & ASD: Detect and Prevent Web Shell Malware  \n        If($i -gt $nth_index) { Break; }  \n        Echo    $($_.Name) is requested by $($_.Value) $($field)(s)\"  \n   } \n} \n \nFunction getHeaders ( $s ) {  \n    $s = (($s.TrimEnd()) -replace \"#Fields: \", \"\" -replace \" -\",\"\" -replace \" \\(\",\"\" -replace \" \\)\",\"\") \n    $s = $s -replace scstatus\",\"status\" -replace csuristem\",\"uri\" -replace csUserAgent\",\"agent\" -replace cip\",\"ip\"  \n    Return $s.Split(' ')  } \n \nFunction IrrelevantRequest ( $req ) {  \n    #Skip requests missing required fields  \n    ForEach ($val in @(status\", uri\",\"agent\",\"ip\"))  \n        { If ($val -notin $req.PSobject.Properties.Name) { Return $True} }  \n    #We only care about requests where the server returned success (codes 200-299)  \n    If ($req.status - lt 200 -or $req.scstatus -gt 299)  \n        { Return $True }  \n    Return $False  \n} \n \nanalyzeLogs agent  \nanalyzeLogs ip  \n \n \nPYTHON14 SCRIPT  TO ANALYZE APAC HE LOGS  \nUSAGE  ./LogCheck.py  <path to Apache  log file> \nCMD  import sys  \nimport os.path  \nimport csv  \n# Script will generate a list of URLs  from Apache web access log that have least unique IP address or unique user -agents  \n# Written for Python 3  \n urlpercentage = 0.05 # Bottom Percentile of URLs  to display  \nweblogfileName = None  \napachelogsfields = ['ip', 'identd', 'frank', 'time_part0', 'time_part1', 'request', 'status', 'size', 'referer', 'user_agent' ] \ndef analyze_weblog(filename): # function output the url base d on low unique ip address and low unique user -agents  \n    uniqueurlcount = 0                    # count of unique URL in web log  \n    urls = []                             # list of unique URL, also index into lists of lists of unique ip address and user -agents \n    uniqueipcount = []                    # list of unique ip address count for URL  \n    uniqueuseragentscount = []            # list of unique use agents for URL  \n    iplist = []                           # list of list of ip  address per unique URL to keep track of unique URL \n    useragentlist = []                    # list of list of user -agents per unique URL to keep track of unique user -agents  \n     \n    print(\"The weblog file to analyze is %s\" % filename)  \n    with open(filename, mode='r') as csv_file:                    # read in web log as csv file  \n        csv_reader = csv.reader(csv_file, delimiter=' ')  \n        for row in csv_reader:  \n            if (row[0][0] != '#'):                     # handles simple case where file has comments start with #     \n               ipaddress = row[apachelogsfields.index('ip')]        # ip address      \n               request = row[apachelogsfields.index('request')]     # request (URL part of request)                 status = row[apachelogsfi elds.index('status')]       # user -agent  \n               user_agent = row[apachelogsfields.index('user_agent')]  \n               url = (request.partition(' ')[2]).partition(' ')[0]  # extract URL from request field  \n               if (status >= '200' and statu s <= '299'):            # only request with status of 200 - 299  \n                   if (url not in urls):                        # determine if URL is already been seen  \n                                            \n14 Python is a registered trademark of the Python Software Foundation  \n\n \n \nU/OO/134094- 20          PP-20-0901 21 APRIL 2020  \n 10 NSA & ASD: Detect and Prevent Web Shell Malware  \n                       uniqueurlcount += 1                      # if not increment uniq ue URL count  \n                       urls.append(url)                         # append new URL to the unique URL list  \n                       uniqueipcount.append(0)                  # append an element of zero for the unique ip count list  \n                       uniqueuseragentscount.append(0)          # append an element of zero for the unique user -agents count list  \n                       newiplist = []                           # new empty element list for ip address tracking per URL  \n                       iplist.append(newiplist)                 # append empty list to list of list of ip per URL \n                       newuseragentlist = []                    # new empty element list for user -agents tracking per URL  \n                       useragentlist.append(newuseragentlist)   # append empty list to list of user -agents per URL  \n                   if (user_agent not in useragentlist[urls.index(url)]):  # determine if user -agents is in the particular URL list  \n                       useragentlist[urls.index(url) ].append(user_agent)   # if not append to user -agents list for the URL list  \n                       temp = uniqueuseragentscount[urls.index(url)] + 1   # also increment unique user -agents count  \n                       uniqueuseragentscount[urls.index(url)] =  temp \n                   if (ipaddress not in iplist[urls.index(url)]):              # determine if ip address is in the particular URL list  \n                       iplist[urls.index(url)].append(ipaddress)               # if not append ip address to list for the particular URL list  \n                       temp = uniqueipcount[urls.index(url)] + 1               # also increment unique ip address count for that URL  \n                       uniqueipcount[urls.index(url)] = temp                        \n                \n        numberofurltodisplay = urlpercentage * uniqueurlcount       # Determine line that represent s percent ile desired  \n        intnumberofurltodisplay = int(numberofurltodisplay)  \n        if (numberofurltodisplay > intnumberofurltodis play):        # Round up  \n            intnumberofurltodisplay += 1 \n        tempuniqueuseragentscount = uniqueuseragentscount.copy()    # temp copy of uni que user -agents count to sort  \n        tempuniqueuseragentscount.sort()  \n        useragentcounttodisplay = tempuniqueuseragentscount[(intnumberofurltodisplay -1)] # determine count to display  \n        tempuniqueipcount = uniqueipcount.copy()                    # Create a temporary copy uni que ip address count to sort  \n        tempuniqueipcount.sort()  \n        ipcounttodisplay = tempuniqueipcount[(intnumberofurltodisplay -1)]                # determine the count to display  \n     \n        print( --------------------'URL with least user agents -----------------------')  \n        for count in range (0, (useragentcounttodis play + 1)):  # Incre ment unique user -agents  \n            index = 0 \n            for elementuseragentcount in uniqueuseragentscount:         # Increment thru unique user -agents count list  \n               if (elementuseragentcount == count):                     #    List URL where user -agents is equal to count  \n                   print(urls[index])  \n               index += 1   \n        print( --------------------'URL with least user agents -----------------------')  \n        for count in range (0, (ipc ounttodisplay +  1)):    # Incre ment count to count of unique ip  \n            index = 0 \n            for elementipcount in uniqueipcount:                        # Increment thru unique ip address count list  \n               if (elementipcount  == count):                            #    List URL where user -agents is equal to count  \n                   print(urls[index])  \n               index += 1            \n         \nif __name__ == '__main__':  \n   try:  \n       if len(sys.argv) == 2:                                              # Simple check if an ar gument is passed (assume weblog file)  \n           weblogfileName=sys.argv[1]  \n           print (\"Web log file to read is %s\" % weblogfileName)  \n           if(os.path.isfile(weblogfileName)):  \n                analyze_weblog(weblogfileName)              \n       else:  \n           print ('Usage: python3 %s <weblogfile>' % sys.argv[0])         # Print usage statement  \n   except Exception as e:  \n        print(\"You must provide a valid filename (path) of a web logfile\")  \n        raise  \n \n  \n\n \n \nU/OO/134094- 20          PP-20-0901 21 APRIL 2020  \n 11 NSA & ASD: Detect and Prevent Web Shell Malware  \nAppendix D: Network Signatures of Traffic for Common Web Shells  \nWeb shell traffic is often obfuscated or encrypted. If organizations have inspection into Transport Layer Security (TLS) \nencrypted sessions for their network, such as via reverse proxy or Web Application Firewall (WAF), then the signatures in the table below may be able to identify network traffic for some common web shells that have not been significantly \nmodified.  These fingerprints are subject to change as attackers  are likely to  alter encoding techniques to evade these \nsignatures . This table is not comprehensive and should be used only as part of a defense- in-depth strategy . \nSNORT RULES  TO DETECT COMMON UNMODI FIED WEB SHELL MALWA RE  \nRATIONALE  Attackers sometimes use un modified web shells which can be detected by network sensors  \nRULES  \n # Be sure to put a valid SID in before implementing and test the signature for performance.  \n \n# These signatures are targeted at the China Chopper web shell  \n# Source: https://www.fireeye. com/blog/threat -research/2013/08/breaking- down- the-china- chopper -web- shell-part-ii.html  \nalert tcp any any - > any any (msg: \"China Chopper with first Command Detected\"; flow:to_server,established; content: \n\"FromBase64String\"; content: \"z1\"; content:\"POST\"; nocase;http_method; reference:url,http://www.fireeye.com/blog/technical/botnet -activities -research/2013/08/breaking- down- the-china- chopper -\nweb- shell-part-i.html; sid: 90000101;)  \nalert tcp any any - > any any (msg: \"China Chopper with all Commands Detected\";  flow:to_server,established; content: \n\"FromBase64String\"; content: \"z\"; pcre: \"/Z \\d{1,3}/i\"; content:\"POST\"; nocase;http_method; \nreference:url,http://www.fireeye.com/blog/technical/botnet -activities -research/2013/08/breaking- down- the-china- chopper -\nweb- shell-part-i.html; sid: 90000102;)  \n # These signatures are targeted at the C99 web shell  \n# Source: https://github.com/jpalanco/alienvault -ossim/blob/master/snort -rules -default -\nopen/rules/2.9.2/emerging.rules/emerging- web_server.rules  \nalert tcp any any - > any a ny (msg:\"ET WEB_SERVER c99 Shell Backdoor Var Override URI\"; flow:to_server,established; \ncontent:\"c99shcook[\"; nocase; http_uri; fast_pattern:only; pcre:\"/[&?]c99shcook \\[/Ui\"; \nreference:url,thehackerblog.com/every- c99-php-shell-is-backdoored- aka-free-shells/; sid:2018601; rev:1; \nmetadata:created_at 2014_06_24, updated_at 2014_06_24;)  \nalert tcp any any - > any any (msg:\"ET WEB_SERVER c99 Shell Backdoor Var Override Cookie\"; \nflow:to_server,established; content:\"c99shcook\"; nocase; fast_pattern:only; pcre:\"/c99shcook/Ci\"; \nreference:url,thehackerblog.com/every- c99-php-shell-is-backdoored- aka-free-shells/; sid:2018602; rev:1; \nmetadata:created_at 2014_06_24, updated_at 2014_06_24;)  \nalert tcp any any - > any any (msg:\"ET WEB_SERVER c99 Shell Backdoor Var Override Cli ent Body\"; \nflow:to_server,established; content:\"c99shcook[\"; nocase; fast_pattern:only; http_client_body; \npcre:\"/(?:^|&)c99shcook \\[/Pi\"; reference:url,thehackerblog.com/every- c99-php-shell-is-backdoored- aka-free-shells/; \nsid:2018603; rev:1; metadata:created_at 2014_06_24, updated_at 2014_06_24;)  \n \n#These signatures are targeted at the R57 web shell  \n# Source: nsa.gov  \nalert tcp any any - > any any  (msg: \"R57 Web shell Detected\"; content: \"<title>r57 Shell Version \"; rev:1; sid: 90000201;)  \n \n#These signatures ar e targeted at the B374k web shell  \n# Source: nsa.gov  \nalert tcp any any - > any any  (msg: \"B374k Web shell Detected\"; content: \"<title>b374k \"; rev:1; sid: 90000301;)  \n \n#These signatures are targeted at the WSO web shell  \n# Source: nsa.gov  \nalert tcp any any - > any any  (msg: \"WSO Web shell Detected\"; content: \"onclick=\\ \"g('SelfRemove',null,'','','') \\\">Self \nremove</a> ] \"; rev:1;  sid: 90000401;)  \n# Source: https://rules.emergingthreatspro.com/9598411999529178/suricata- 2.0/rules/web_server.rules \nalert tcp any any - > any any (msg:\"ET WEB_SERVER WSO Web  Shell Activity POST structure 2\"; \nflow:established,to_server; content:\"POST\"; http_method; content:\" name=|22|c|22|\"; http_client_body; \ncontent:\"name=|22|p1|22|\"; http_client_body; fast_pattern; \npcre:\"/name=(?P<q>[ \\x22\\x27])a(?P=q)[^ \\r\\n]*\\r\\n[\\r\\n\\s]+(?:S(?:e(?:lfRemove|cInfo)|tringTools|afeMode|ql)|(?:Bruteforc|Co\nnsol)e|FilesMan|Network|Logout|Php)/Pi\"; sid:2016354; rev:2; metadata:created_at 2013_02_05, updated_at \n2013_02_05;)  \n  \n  \n\n \n \nU/OO/134094- 20          PP-20-0901 21 APRIL 2020  \n 12 NSA & ASD: Detect and Prevent Web Shell Malware  \nAppendix E: Identifying Unexpected  Network Flows  \nThe following Snort rule can aid administrators in identifying unexpected network flows. Identifying unexpected network \nflows requires that administrators maintain an accurate understanding of the expected network architecture. The rule \nbelow is unlikely to be effective without tailoring it for a specific network.  \nSNORT RULE TO IDENTIFY UNE XPECTED WEB SERVERS  \nUSAGE  Replace  XXX.XXX.XXX.XXX/XX  with a target subnet (e.g., 192.168.1.0/24 ) and add the rule to Snort  \nSCRIPT  alert tcp  XXX.XXX.XXX .XXX/XX  [443,80] -> any any (msg:  \"potential unexpected web server \"; sid:4000921)  \n \n  \n\n \n \nU/OO/134094- 20          PP-20-0901 21 APRIL 2020  \n 13 NSA & ASD: Detect and Prevent Web Shell Malware  \nAppendix F: Identifying Abnormal Process Invocations in Sysmon Data  \nMicrosoft Sysmon is a logging tool that enhances logging performed on Windows systems. Among other things, \nSysmon logs information about how each process is created. The information is valuable for identifying anomalous \nbehavior, such as in the case of malicious web shells. Sysmon can be obtained from Microsoft at \nhttps://docs.microsoft.com/en -us/sysinternals/downloads/sysmon  and must be installed on a system in order to begin \nlogging. Ideally, Sysmon and other Windows logging should be mirrored to a central Security Information and Event \nManagement (SIEM) server where it can be aggregated and queried.  \nThe query below will simply report which executables were launched by an IIS  web server. In many cases, a web \napplication will cause IIS  to launch a pr ocess for entirely benign functionality. However, there are several executables \ncommonly used by attackers for reconnaissance purposes which are unlikely to be used by a normal web application. \nSome of these executables are listed in the table below. Administrators are encouraged to review the results of the \nPowerShell query below and verify that the web application in question is intended to use the identified executables.  \nPOWERSHELL SCRIPT TO IDENTIFY ANOMALOUS SYSMON ENT RIES FOR IIS  \nUSAGE  Run the fo llowing command from a PowerShell prompt with administrative access  \nSCRIPT  Get-WinEvent -FilterHashtable @{logname=\"Microsoft -Windows -Sysmon/Operational\";id=1;} |  \nWhere {$_.message - like \"*ParentImage: C: \\Windows \\System32\\ inetsrv \\w3wp.exe*\"} |  \n%{ $_.properties[4]} |  \nSort-Object -Property value -Unique  \n \nWindows environment executables frequently used by attackers and rarely launched by benign IIS apps \n arp.exe  hostname.exe  ntdutil.exe  schtasks.exe  \nat.exe  ipconfig.exe  pathping.exe  systeminfo.exe  \nbitsadmin.exe  nbtstat.exe  ping.exe  tasklist.exe  \ncertutil.exe  net.exe  powershell.exe  tracert.exe  \ncmd.exe  net1.exe  qprocess.exe  ver.exe  \ndsget.exe  netdom.exe  query.exe  vssadmin.exe  \ndsquery.exe  netsh.exe  qwinsta.exe  wevtutil.exe  \nfind.exe  netstat.exe  reg.exe  whoami.exe  \nfindstr.exe  nltest.exe  rundll32.exe  wmic.exe  \nfsutil.exe  nslookup.exe  sc.exe  wusa.exe  \n \n  \n\n \n \nU/OO/134094- 20          PP-20-0901 21 APRIL 2020  \n 14 NSA & ASD: Detect and Prevent Web Shell Malware  \nAppendix G: Identifying Abnormal Process Invocations with Auditd \nAuditd is the userspace component of the Linux Auditing System. Auditd can provide users with insight into process \ncreation logs. The information is valuable for identifying anomalous behavior, such as in the case of malicious web shells. \nAuditd is available in default repositories for many Linux distributions and mus t be installed and configured to log relevant \nweb server process data. Ideally, auditd and other Linux logging should be mirrored to a central Security Information and \nEvent Management (SIEM) server where it can be aggregated and queried.  \nThe query below will simply report which applications were launched by an Apache web server. In many cases, a web \napplication will cause Apache to launch a process for entirely benign functionality. However, there are several \napplications commonly used by attackers for reconnaissance purposes which are unlikely to be used by a normal web \napplication. Some of these executables are listed in the table below. Administrators are encouraged to review the results \nand verify that the web application in question is intended to use the identified applications.  \nConfiguring Auditd  \n1. Determine the web server uid:  \nAfter installing auditd (for example using apt - y install auditd), determine the uid of web server using:  \n     apachectl -S  \nThis will return apache details including the user id in a line such as:  \n     User: name=\"www -data\" id=33  \nHere the uid is 33  \n2. Add the following auditd rules ( /etc/audit/rules.d/audit.rules ) replacing XX with the uid identified above:  \n-a always,exit - F arch=b32 -F uid= XX -S execve - k apacheexecve  \n-a always,exit -F arch=b64 -F uid= XX -S execve - k apacheexecve  \n3. Restart auditd:  \nservice auditd restart  \nReview Auditd Log \n1. Applications launched by Apache can be identified with:  \n     cat /var/log/auditd/audit.* | grep \"apacheexecve\"  \nThis will return the path to the launched application (see bolded path in the example output below)  \n \ntype=SYSCALL msg=audit(1581519503.841:47): arch=c000003e syscall=59 success=yes exit=0 \na0=563e412cbbd8 a1=563e412cbb60 a2=563e412cbb78 a3=7f065d5e5810 items=2 ppid=15483 pid=15484 \nauid=4294967295 uid=33 gid=33 euid=33 suid=33 fsuid=33 egid=33 sgid=33 fsgid=33 tty=(none) \nses=4294967295 comm=\"cat\" exe=\" /bin/cat \" key=\"apacheexecve\"  \n2. Results can be analyzed to determine if unusual applications are launched (see table below)  \n3. Detailed infor mation, including call arguments, can be obtained using:  \n    cat /var/log/auditd/audit.* | grep \"msg=audit(1581519503.841:47)\"  \nReplace the value of msg=audit with the value returned in step 1 above  \nLinux environment applications frequently used by attac kers and rarely launched by benign Apache applications  \ncat ifconfig  ls route  \ncrontab  ip netstat  uname  \nhostname  iptables  pwd whoami  \n \n  \n\n \n \nU/OO/134094- 20          PP-20-0901 21 APRIL 2020  \n 15 NSA & ASD: Detect and Prevent Web Shell Malware  \nAppendix H: Commonly Exploited Web Application Vulnerabilities  \nThe list below shows some web application vulnerabilities that are commonly exploited to install web shell malware. This \nlist is not intended to be exhaustive, but it provides insight on some frequently exploited cases. Organizations are encouraged to patc h both internet -facing and internal web applications rapidly to counter the risks from n- day \nvulnerabilities.   \nVulnerability Identifier  Affected Application   \nReported  \nCVE- 2019 -0604  Microsoft SharePoint15 15 May 2019  [8] \nCVE- 2019 -19781  Citrix16 Gateway, Citrix Application Delivery Controller, and \nCitrix SD-WAN WANOP appliance  22 Jan 2020  [9] \nCVE- 2019 -3396  Atlassian Confluence17 Server  20 May 2019  [10] \nCVE- 2019 -3398  Atlassian Confluence Server and Atlassian Confluence Data \nCenter  26 Nov 2019  [11] \nCVE- 2019 -9978  WordPress18 Social Warfare Plugin  22 Apr 2019  [12] \nCVE-2019 -18935  \nCVE- 2017 -11317  \nCVE-2017 -11357  Progress Telerik19 UI 7 Feb 2019 [13] \nCVE- 2019 -11580  Atlassian Crowd and Crowd Data Center  15 July 2019  [14] \nCVE- 2020 -10189  Zoho ManageEngine20 Desktop Central  6 Mar 2020  [15] \nCVE- 2019 -8394  Zoho ManageEngine ServiceDesk Plus  18 Feb 2019  [16] \nCVE- 2020 -0688  Microsoft Exchange21 Server  10 Mar 2020  [17] \nCVE- 2018 -15961  Adobe ColdFusion22 8 Nov 2018  [18] \n \n  \n                                            \n15 SharePoint is a registered trademark of the Microsoft Corporation  \n16 Citrix is a registered trademark of Citrix Systems, Inc.  \n17 Atlassian and Confluence are registered trademarks of Atlassian Pty Ltd.  \n18 WordPress is a registered trademark of the WordPress Foundation  \n19 Progress and Telerik are registered trademarks of Progress Software EAD  \n20 Zoho and ManageEngine are registered trademarks of ZOHO Corporation  \n21 Exchange is a registered trademark of the Mic rosoft Corporation  \n22 Adobe and ColdFusion are registered trademarks of Adobe Systems Incorporated  \n\n \n \nU/OO/134094- 20          PP-20-0901 21 APRIL 2020  \n 16 NSA & ASD: Detect and Prevent Web Shell Malware  \nAppendix I: HIPS Rules for Blocking Changes to Web Accessible Directories  \nMcAfee HBSS allows specification of custom HIPS rules, which are then enforced by endpoint McAfee agents. These \nrules can be used to block file creation and file changes to web accessible directories effectively neutering the primary \ninfection vector for web shell malware. If necessary, these rules can be temporarily disabled during site updates or web \napplication patches. As with any new HIPS rule, administrators should begin enforcement at Level 1 (informational) in \norder to identify potential conflicts with existing applications. Enforcement should be raised to Level 4 (high) once impact \nassessment is deemed acceptable.  \nWINDOWS ENVIRONMENT  \nUSAGE  Replace C: \\\\inetpub \\\\wwwroot \\\\* with the target directory  path (i.e., the web directory)   \nRULE  Rule {   \nTag Blocking Changes to Web Directory (Windows)  \nClass Files  \nID -1  # this will select the next free ID number in the 4XXX series  \nLevel 1  \nfiles { Include C: \\\\inetpub\\ \\wwwroot \\\\* } \ndirectives files:rename files:permissions files:create files:write  \n} \n \nLINUX ENVIRONMENT  \nUSAGE  Replace /var/www/html/* with the target directory path (i.e., the web directory)  \nRULE  Rule {  \nTag Blocking Changes to Web Directory (Linux)  \nClass UNIX_file  \nID -1  # this will select the next free ID number in the 4XXX series  \nLevel 1  \nfiles { Include /var/www/html/* }  \ndirectives unixfile:symlink unixfile:create unixfile:mk dir unixfile:write  \n} \n \nTuning Signatures : \nSignatures should be tuned  according to the operating environment. If there are any exemptions  (e.g., web application \nuploads) an exception  can be created by clicking on the HIPS custom signature under Policy Catalog, Host Intrusion \nPrevention IPS/IPS rules and select ing the name of the IPS signature usually under My Default.  \nOnce on the signature page, the signature can be found by typing in the Search box key words including the name of the \nsignature.  Once the signature is displayed, the check box to the left of it  should be selected and the Exception Rule tab \ncan be clicked to add a Parameter for a specific file type (e.g., *.pdf) that should be allowed.  \n  \n\n \n \nU/OO/134094- 20          PP-20-0901 21 APRIL 2020  \n 17 NSA & ASD: Detect and Prevent Web Shell Malware  \n \nWorks Cited \n[1] Microsoft Detection and Response Team  (2020), Ghost in the shell: Investigating web shell attacks . [Online] Available at: \nhttps://microsoft.com/security/blog/2020/02/04/ghost -in-the-shell-investigating- web-shell-attacks/  [Accessed Apr. 6, 2020]   \n[2] Rascagneres, P. and Svajcer, V.  (2019). China Chopper still active 9 years later . [Online] Available at: \nhttps://blog.talosintelligence.com/2019/08/china- chopper -still-active -9-years -later.html  [Accessed Apr. 6 , 2020]  \n[3] CISA (2017), Alert TA15- 314A. [Online] Available at: https://www.us -cert.gov/ncas/alerts/TA15- 314A  [Accessed Apr. 6, 2020]  \n[4] CISA (2018 ), Alert AA18- 284A . [Online] Available at: https://www .us-cert.gov/ncas/alerts/AA18- 284A  [Accessed Apr. 6, 2020]  \n[5] ACSC (2015), Web Shells   Threat Awareness and Guidance. [Online] Available at  https://cyber.gov.au/sites/defaul t/files/2019-\n03/ACSC_Web_Shells.pdf  [Accessed Apr. 6, 2020]   \n[6] Dumont, R. (2017), MITRE ATT&CK Framework -  Timestomp. [Online] Available at  https://attack.mitre.org/techniques/T1099/  [Accessed Apr. \n6, 2020]  \n[7] NSA (2016), Segregate Networks and Functions. [Online] Available at  https://apps.nsa.gov/iaarchive/library/ia- guidance/security -\ntips/segregate- networks -and-functions.cfm  [Accessed Apr. 6, 2020]  \n[8] TrendMicro (2019), Security Alert: China Chopper Malware targeting vulnerable SharePoint servers. [Online] Available at  \nhttps://success.trendmicro.com/solution/000131747  [Accessed Apr. 6, 2020]  \n[9] Ballenthin et al. (2020), FireEye and Citrix Tool Scans for Indicators of Compromise Related to CVE -2019- 19781. [Online] Available at  \nhttps://www.fireye.com/blog/product s-and-services/2020/01/fireeye- and-citrix-tool-scans -for-iocs-related- to-vulnerability.html  [Accessed Apr. 6, \n2020]  \n[10] Capuano, E.  (2019), Analysis of Exploitation: CVE -2019- 3396. [Online] Available at  https://blog.reconinfosec.com/analysis -of-exploitation- of-\ncve-2019- 3396/  [Accessed Apr. 6, 2020]  \n[11] Joshi, A. (2019), CVE -2019- 3398: Atlassian Confluence Download Attac hments Remote Code Execution. [Online] Available at  \nhttps://blogs.juniper.net/en- us/threat -research/cve- 2019- 3398- atlassian- confluence- download- attachments -remote -code- execution  [Accessed \nApr. 6, 2020]  \n[12] Deng, Zhang, and Gao. (2019), Exploits in the Wild for WordPress Social Warfare Plugin CVE -2019- 9978. [Online] Available at  \nhttps://unit42.paloaltonetworks.com/exploits -in-the-wild-for-wordpress -social -warfare -plugin- cve-2019- 9978  [Accessed Apr. 6, 2020]  \n[13] Wulftange, M. (2019), Telerik Revisited. [Online] Available at  https://codewhitesec.blogspot.com/2019/02/telerik -revisited.html  [Accessed Apr. \n6, 2020]  \n[14] Narang, S. (2019), CVE -2019- 11580: Proof -of-Concept for Critical Atlassian Crowd Remote Code Execution Vulnerability Now Available. \n[Online] Available at  https://tenable.com/blog/cve- 2019- 11580- proof -of-concept -for-critical-atlassian- crowd -remote- code- execution  [Accessed \nApr. 6, 2020]  \n[15] ManageEngine (2020), Identification and mitigation of remote code execution vulnerability CVE -2020- 10189. [Online] Available at  \nhttps://manageengine.com/products/desktop- central/rce- vulnerability -cve-2020- 10189.html  [Accessed Apr. 6, 2020]  \n[16] CISA (2019), Bulletin SB19- 056. [Online] Available at  https://us -cert.gov/ncas/bulletins/SB19- 056 [Accessed Apr. 6, 2020]  \n[17] CISA (2020), Unpatched Microsoft Exchange Servers Vulnerable to CVE -2020- 0688. [Online] Available at  https://us -cert.gov/ncas/current -\nactivity/2020/03/10/unpatched- microsoft -exchange- servers -vulnerable- cve-2020- 0688  [Accessed Apr. 6, 2020]  \n[18] Volexity Threat Research (2018), Active Exploitation of Newly Patched ColdFusion Vulnerability. [Online] Available at  \nhttps://volexity.com/blog/2018/11/08/active- exploitation- of-newly -patched- coldfu sion- vulnerability -cve-2018- 15961/  [Accessed Apr. 6, 2020]  \nDisclaimer of Endorsement  \nThe information and opinions contained in this document are provided \"as is\" and without any warranties or guarantees. Reference herein to any specific \ncommercial products , process, or service by trade name, trademark, manufacturer, or otherwise, does not constitute or imply its endorsement, \nrecommendation, or favoring by the United States Government, and this guidance shall not be used for advertising or product endorsement purposes.  \nContact  \nNSA Client Requirements / General Cybersecurity Inquiries: Cy bersecurity Requirements Center , 410- 854-4200,  Cybersecurity_Requests@nsa.gov   \nNSA Media I nquiries / Press Desk: 443- 634-0721, MediaRelations@nsa.gov   \nASD Australian Cyber Security Centre / General Cybersecurity or Media Enquiries: asd.assist@defence.gov.au  or visit www.cyber.gov.au.  \n\n",
  "cves": [],
  "techniques": [
    "T1099"
  ],
  "advisory": "cybersecurity-alerts",
  "title": "csi-detect-and-prevent-web-shell-malware-20200422",
  "source": "nsa",
  "id": "92a385e92ad2e38709dc2b08f1c09af98f6fbc4a392cf3bb0b099418ac1fa783"
}