{
  "markdown": " \n \n \n \nNational Security Agency  \nCybersecurity  Technical Report  \n \n \n \n \n \nDoD Microelectronics:  \nThird -Party IP Review Process for  \nLevel of Assurance 1  \n \n \n \nDecember  2022 \n \nU/OO/ 230115 -22 \nPP-22-1870  \nVersion 1.0 \n \n \n \n \n  \n  \n\n\n \n \nU/OO/ 230115 -22 | PP-22-1870  | DEC  2022 Ver. 1.0  ii \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA1  \n \n For additional information, guidance or assistance with this document, \nplease contact the Joint Federated Assurance Center (JFAC) at \nhttps://jfac.navy.mil . \n  \n\n\n \n \nU/OO/ 230115 -22 | PP-22-1870  | DEC  2022 Ver. 1.0  iii \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA1  \nNotices and history   \nDocument change history  \nDate  Version  Description  \nDecember  2022  1.0 Initial Publication  \n   \nDisclaimer of warranties and endorsement  \nThe information and opinions containe d in this document are provided \"as is\" and without any warranties \nor guarantees. Reference herein to any specific commercial products, process, or service by trade name, \ntrademark, manufacturer, or otherwise, does not constitute or imply its endorsement, recommendation, or \nfavoring by the Unite d States Government, and this guidance shall not be used for advertising or product \nendorsement purposes.  \nPublication information  \nAuthor(s)   \nNational Security Agency  \nCybersecurity Directorate  \nJoint Federated Assurance Center  \nContact information  \nJoint Federated Assurance Center: https://jfac.navy.mil   \nCybersecurity Report Feedback / General Cybersecurity Inquiries: CybersecurityReports@nsa.gov   \nDefense Indust rial Base Inquiries and Cybersecurity Services: DIB_Defense@cyber.nsa.gov   \nMedia inquiries / Press Desk: Media Relations, 443 -634-0721, MediaRelations@nsa.gov   \nPurpose  \nThis document was developed in furtherance of NSAs cybersecurity missions . This  includ es its \nresponsibilities to identify and disseminate threats to National Security Systems, Department of Defense \ninformation systems, and the Defense Industrial Base, and to develop and issue cybersecurity \nspecifications and mitigations. This information may be shared broadly to reach all appropriate \nstakeholders.   \n\n \n \nU/OO/ 230115 -22 | PP-22-1870  | DEC  2022 Ver. 1.0  iv \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA1  \nExecutive summary  \nThis report outlines a methodology of design review sufficient to validate third -party \nintellectual property (3PIP) is appropriate to incorporate into a Level of Assurance 1 \n(LoA1) system. In this context , 3PIP refers to functions whose development are not  \nunder the control of the designer.  Use of the phrase intellectual property, IP , or 3PIP in \noutlining this methodology of design review does not refer to property rights , such as, \nfor example, copyrights, patents, or trade secrets. It is the responsibili ty of the party \nseeking review and/or the reviewer to ensure that any rights needed to perform the \nreview in accordance with the methodology outlined are obtained.  \nThis process is intended to mitigate the Threat  Description  #5 Adversary compromises  \nthird-party soft IP, as described in the FPGA Best Practices - Threat Catalog. The \nprocess attempts to minimize the required level of effort while providing sufficient \nmitigation for use of 3PIP at LoA 1. The review process, when correctly executed, \nestablishes an independent organization that an adversary would have to compromise \nor deceive for an attack to succeed. This directly increases the level of access  required \nto carry out an attack. The process assumes that technology and level of effort are not \nsubstantial barriers to an adversary in modifying the 3PIP design.  \nAdditionally, at LoA 1 only inherently targetable and high -utility attacks are of concern. \nAttacks that provide pre -positioning as part of a more complex attack to be executed in \nthe future are out of scope. This restriction  bounds the steps of review that are required . \nHowever, should issues outside the scope of highly targetable attacks be found  during \nthis review process, they must be reported as appropriate1 and the intellectual property \n(IP) should not be approved  for use in an LoA1 system . \nThis review process can be applied in two distinct ways. One is to approve a 3PIP in \ngeneral. The second  is to approve a 3PIP for use in a specific role within a specific \nsystem. For instance, this process can be used to review a memory controller IP for use \nin any application. In some circumstances, it may be less expensive to review that same \nIP, but only for use in a single application. In this case, information about the system \naround the IP might be used to avoid reviewing certain components in detail.  \n                                                \n1 Appropriate action will depend on the specific discovery. Coding errors that appear innocent may require cybersecurity report ing, and in \ngeneral should be reported to the IP vendor. Apparently intentional backdoors should be reported to the Federal Bureau  of Investigations (FBI) or \nthe National Intellectual Property Rights Coordination Center. JFAC is available for consultation should the correct action b e unclear.  \n\n \n \nU/OO/ 230115 -22 | PP-22-1870  | DEC  2022 Ver. 1.0  v \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA1  \nThe JFAC FPGA Level of Assurance 1 Best Practices  appendix  C specifies this flow \nas one option to as sure 3PIP. In most cases that will correspond to review within a \ncertain system. The other mitigation offered is to provide the 3PIP to the JFAC. When \nJFAC elects to review the 3PIP, it will be a review for any system context.  \nThis review process is summar ized below : \n Assemble review prerequisites  \n Establish suitability of the 3PIP for review  \n Partition  the design  \n Perform manual code review  \n Perform test-driven code review  \n Document  and sign the review package  \n \n  \n\n \n \nU/OO/ 230115 -22 | PP-22-1870  | DEC  2022 Ver. 1.0  vi \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA1  \nContents   \n \nDoD Microelectronics: Third -Party IP Review Process for Level of Assurance \n1 ................................ ................................ ................................ ................................ ........................ i \nExecutive summary  ................................ ................................ ................................ ......................  iv \nContents  ................................ ................................ ................................ ................................ ...... vi \n1. Introduction  ................................ ................................ ................................ ............................  1 \n2. Assemble review prerequisites  ................................ ................................ ...........................  1 \n2.1 Reviewer documentation  ................................ ................................ ................................ ................................ . 1 \n2.2 Record of review  ................................ ................................ ................................ ................................ .................  2 \n3. Estab lish suitability of the 3PIP for review  ................................ ................................ ........  3 \n4. Partition the design  ................................ ................................ ................................ ...............  4 \n4.1 Threats of interest  ................................ ................................ ................................ ................................ ..............  4 \n4.2 Controlled effects  ................................ ................................ ................................ ................................ ................  5 \n4.3 Pe rsistent effects  ................................ ................................ ................................ ................................ ................  7 \n4.4 Defining a functional area  ................................ ................................ ................................ ...............................  8 \n4.5 Criteria for functional areas of interest  ................................ ................................ ................................ ....... 8 \n5. Perform manual code review  ................................ ................................ ...............................  9 \n6. Perform test -driven code review  ................................ ................................ .......................  11 \n7. Document and sign the review package  ................................ ................................ ..........  11 \n8. Pre-compiler and other machine -generated code  ................................ ...........................  12 \n9. Summary  ................................ ................................ ................................ ..............................  13 \nAppendix A: Standardized terminology  ................................ ................................ ...................  15 \n \n \n\n \n \nU/OO/ 230115 -22 | PP-22-1870  | DEC  2022 Ver. 1.0  1 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA1  \n1. Introduction  \nThis report outlines a methodology of design review sufficient to validate third -party \nintellectual property (3PIP) is appropriate to incorporate into a Level of Assurance 1 \n(LoA1) system.  In this context, 3PIP refers to function s whose development are not \nunder the control of the designer.  Use of the phrase intellectual property, IP , or 3PIP in \noutlining this methodology of design review does not refer to property rights , such as, \nfor example, copyrights, patents, or trade secr ets. It is the responsibility of the party \nseeking review and/or the reviewer to ensure that any rights needed to perform the \nreview in accordance with the methodology outlined are obtained.  \n2. Assemble review p rerequisites  \nFor a 3PIP  review to be valid, the review itself must be thorough and auditable. \nProcedural elements related to the following must be in place for the duration of the \nreview : \n A clear record of who is conducting the review, and  \n The review process itself.  \n2.1 Reviewer documentation  \nA clear record of the individuals conducting the review must be maintained. This record \nmust justify why the individuals are well suited to the task. The information required is \nsimilar to the information collected by a company during the hiring process, s uch as \nidentification information and a resume. The following details must be included : \n Security suitability   For LoA 1 threats, the individuals doing the review must be \nU.S. persons. The organization performing the review must maintain \ndocumentation proving that status.  \n Qualifications  Each reviewer must have a suitable degree and experience \nwith the design language or circuit type they review. For instance, a reviewer of \nVerilog code must have an understanding of digital design typically acquired wi th \na degree in computer or electrical engineering and experience writing Verilog. \nLikewise, a reviewer of Very High Speed Integrated Circuit Hardware Description \nLanguage  (VHDL ) must similarly have experience with VHDL. Furthermore, if the \n3PIP incorporate s hard IP blocks, specific to the Field Programmable Gate Array \n\n \n \nU/OO/ 230115 -22 | PP-22-1870  | DEC  2022 Ver. 1.0  2 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA1  \n(FPGA ) platform, the reviewer must have experience with platforms from that \nvendor2. \n Independence   No reviewer should have worked on the development of the \n3PIP. No reviewer may be a current employee of, or contractor to, the company \nthat designed or sells the 3PIP.  \n2.2 Record of r eview   \nThe review process itself must be auditable such that each decision is traceable to at \nleast two specific reviewers. In addition, time spent conducting the re view must be \nrecorded for each decision. The time expected to complete the review will vary greatly \ndepending on block complexity. The specific information to gather and document is \nsimilar to the information collected automatically by many  code review too ls. \nSpecifically, for each artifact reviewed , the following must be maintained:  \n Identity of the reviewer   The name, or other ide ntifier, of the individual who \nperformed the review.  \n Times of review   A record describing the day or days on which the review took \nplace, as well as the duration of the review . \n Specific material reviewed   A record of the specific design files reviewed to \nmake the determination.  \n Tests run during the review   A rec ord of any functional tests run by the \nreviewers, including test  benches, scripts, and other inputs necessary to replicate \nthem.  \n Summary   A record explaining the decision made, and the facts that led to that \ndetermination.  \nIn addition , this documentation must record how the IP was received, how it was stored , \nand how it was distributed to the reviewers. A process should be put in place to ensure \nthat all reviewers receive the same versions of each file.  \nThis record of review should be  stored in the revision control system , but does not need \nto be distributed as part of the final signed review package.  \n                                                \n2 For example, if a 3PIP makes explicit use of a multiply accumulate block in an Intel FPGA the reviewer must have experience with Intel FPGAs, or previous \nrelated Altera FPGAs.  \n\n \n \nU/OO/ 230115 -22 | PP-22-1870  | DEC  2022 Ver. 1.0  3 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA1  \n3. Establish suitability  of the 3PIP for review  \nIP can only be reviewed if it is documented and developed using practices that cannot \nbe exploited to hide malicious changes. Tha t means well -documented hardware \ndescription language ( HDL) with clean names and reasonable design. Human and \nsimulation review of the IP are not effective outside of those conditions. This section \nintroduces a set of criteria that determines whether such practices are employed.  \nThis step determines if a review of the 3PIP is feasible in a way that will generate \nsufficient assurance. A single individual can conduct this process by reviewing the 3PIP \nand asking these guiding questions:  \n Is the 3PIP distribute d in an encrypted form that prevents the reviewers from \nviewing it?  Is it distributed as a netlist3? \n If yes, the 3PIP is not suitable for review. It may not be recommended for  \nthis flow.  \n Viewing the unencrypted code, or pre -synthesis code, is only sufficient \nwith additional steps to ensure equivalence of the distributed and reviewed \nfiles. If this is needed, contact JFAC for guidance . \n Is the 3PIP obfuscated4?  \n If yes, the 3PIP is not suita ble for review. It may not be approved with this \nflow. JFAC can be contacted if the 3PIP is essential; however, the process \nto approve such a 3PIP will be expensive, time consuming, and likely to \nreturn a negative result.  \n Is the 3PIP clearly organized, wit h distinct modules that are limited in scope with \ninputs/outputs (I/O) limited to those needed to achieve their scope?  \n If no, the 3PIP is not suitable for review. It may not be approved with this \nflow. \n                                                \n3 Whether a 3PIP is distributed as a netlist is not determined by the file format, but rather the contents. For example, a Veri log file is considered a netlist \ndistributi on when a substantial amount of it is implemented with primitive or gate -level elements. This could either be primitives drawn from a library, such as a \nvendor specific primitive library, or primitives that implement individual gates, such as the and and  or elements in Verilog.  \n4 3PIP for which module names, signal names, or any other names, have been replaced with non -meaningful alphanumeric codes or random words is considered \nto be obfuscated. Similarly, HDL that makes repeated use of complex Boolean operators, where it could have used complex functional commands, such as \nif/then blocks, is considered obfuscated.  \n\n \n \nU/OO/ 230115 -22 | PP-22-1870  | DEC  2022 Ver. 1.0  4 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA1  \n Is the 3PIP well documented? This should include comme nts describing the \npurpose of modules; clear, human -readable names; and top -level documentation \ndescribing the interface.  \n If no, the 3PIP is not suitable for review. It may not be approved with this \nflow. \n Does the 3PIP feature parameterized code and pre -compiler directives where \nparameters are used  to explicitly generate new HDL5 that is then added to the \nsystem? Is sof tware used to write HDL based on designer input? This includes \nexplicit code that writes code, uses terms like ifdef in Verilog, and uses the term \ngenerate in VHDL . This includes parameterized IPs that can be configured via  \nGUI or script.  \n If yes, the 3PIP is still suitable for review. Be aware that the review \nprocess will be longer.  \n Does the 3PIP have a substantial number of tests distributed with it?  \n If no, the 3PIP is still suitable for review . Be aware that the review process \nwill be longer.  \n4. Partition the design   \nOnce review suitability is established, the team reviews the overall design of the 3PIP \nand makes a plan to split the 3PIP into one or more functional units that can be \nreviewed and tested independently. This number can vary greatly depending on the \nscale of the 3PIP being reviewed. For some simple 3PIP modules, there may be no \nclear partitioning of the code into meaningf ul, functional modules. For others, there may \nbe a clear top -level design that quickly yields multiple functional modules.  \n4.1 Threats of interest  \nBecause this process is spe cifically intended to catch LoA 1 threats, the review team \nbegins by revisiting the  definition of these threats as it relates to their function.  \nTargetability   Inherently  targetable and high utility  threat \noperations are executed in a way that provides straightforward means \n                                                \n5 Generator code that exists solely to add additional logging for debug should not be counted as generator code for these purpo ses. Generator  code added to \nmake a simulation work must be reviewed carefully in the process below, but should not count as generator code for this purpo se. \n\n \n \nU/OO/ 230115 -22 | PP-22-1870  | DEC  2022 Ver. 1.0  5 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA1  \nto understand and predict the effect of an attack and provide a \nmechanism to control or time the attack. For example, an adversary \nwith the ability to introduce new code into a system design can \nimplement a broad number of malicious functions. A denial of service \nattack falls in this category if and only if it is pos sible for the adversary \nto control when it takes effect after the device is fielded. However, a \nsimple reduction in reliability not tied to any trigger, which therefore \ncannot be controlled or timed in a controlled way, does not fall in this \ncategory.  \nThe focus of this review is to look for malicious modifications to 3PIP. Because 3PIP \ncan have many origins , as well as many distribution methods, an adversary has many \noptions. In particular, an adversary can modify one copy of 3PIP destined for a \nparticula r target. This means that an in -depth 3PIP review is required.  \nThe JFAC best practice guides  do not specify what compromises are acceptable ; only \nthe scope of search that is required. If any flaw, intentional or otherwise, is identified , it \nshould be repor ted and may render a 3PIP unsuitable for use.  \nNote : While  these guidelines considerably limit the scope of attacks that reviewers must \nlook fo r, they do not limit the scope of attacks that must be reported if discovered.  \nHowever, there are some limits to the level of review required that enable us to \ndesignate specific sub -portions of a 3PIP design as of interest for review. These \nattacks are  divide d into two classes: controlled effects and persistent effects.  \n4.2 Controlled effects  \nA controlled effect is just that: a specific control mechanism, coupled to a specific effect. \nIn more detail, such an attack contains both of the following : \n A means of command and control   The attack must be externally controlled \nthrough some pre -existing or additiona l control mechanism. Within LoA 1, assume \nthat such an effect must not occur during acceptance testing or typical use of the \ndevice. To achieve this, a trigger must either be sufficiently long and unstructured \nor represent an out -of-spec behavio r in a defined protocol.  \n\n \n \nU/OO/ 230115 -22 | PP-22-1870  | DEC  2022 Ver. 1.0  6 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA1  \n As a rule of thumb, unstructured triggers of interest must contain, at a \nminimum, 32 bits of unstructured data. Logic to recognize such a \nsequence could be implemented many ways, but any implementation \nwould contain at least 5 bits  of state, as well as substantial additional logic \ncomprised of combinatorial and state elements.  \n Structured triggers of interest must occur within the context of a specific \ndefined protocol, and cannot simply represent any out -of-spec behavior. \nFor instan ce, an incorrect cyclic redundancy c heck (CRC ) field is overly \nbroad, as many systems will encounter incorre ct CRC  fields. However, a \nspecific bit error pattern within a CRC field could be sufficient.  \n A useful effect   Within LoA 1, useful effects can include specific actions on the \npart of the system, denial of service, leak of data, and so on. The presence of \ndenial of service in this list means that most any functional area in a 3PIP design \nwill meet this requirement.  \nExample that meets the criteria : A modification to a sub -block of a 3PIP Ethernet \nfilter waits for a specific magic packet before deactivating the Ethernet entirely \nuntil the system is reset.   \nThis attack is controllable:  the adversary knows the protocols pa rsed by the core at \ndesign tim e and they can understand its role in a specific system. It is use ful: it would \nallow an adversary to deactivate an important function of the device on command. This \nis an example of a controlled effect that would be of interest.  \nSecond e xample that meets the criteria: A modification to a programmable filter \n(e.g., a finite i mpul se response filter) is added that waits for a specific input \npattern, after which it causes the filter to output only 0 . \nThis attack is controllable if the adversary understands the  role of the filter in the system \nto process external data. For example, in a sensor application where data is \npreprocessed by a filter before being passed on to other parts of the system. While an \nadversary cannot control the exact values of the inputs in  many such applications, they \ncan control patterns in those values that can be detected by their additional code. In a \nsensor application such as this, this modification could give an adversary the means to \ndeactiv ate a sensor, which is a denial  of service  attack that meets the threshold.  \n\n \n \nU/OO/ 230115 -22 | PP-22-1870  | DEC  2022 Ver. 1.0  7 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA1  \nExample that fails to meet the criteria: A modification to a microcontroller core \ndeactivates memory protection when a specific sequence of instructions is run . \nThis attack does not meet the criteria. If an addition such a s this was discovered, it must \nbe reported and would represent a negative finding. However, the possibility is \ninsufficient to justify searching for such an attack. The effect described is only useful if \nan adversary is also able to execute  code on the tar get device. O ther cybersecurity \npractices should protect against this. As a result, this is a pre -positioning attack  that \nwould be addressed by LoA 2 protections, rather than a n effect that is  directly useful  to \nthe adversary.  \n4.3 Persistent effects  \nIn contrast to a controlled effect, a persistent effect lacks an external activation \nmechanism. Instead, it is always or regularly present. Persistent attacks against FPGA \n3PIP are constrained principally by testing performed during development and ordinary \nuse of the device. To be useful , they must evade detection during functional testing \nconducted by the designer and cannot immediately interfere with use of the system. \nThis is entirely feasible depending on the test methodology chosen by the system \ndesigner s and the 3PIP block itself.  \nFor this review, the reviewers may assume that two categories of tests  are run on the \n3PIP. The first is the set of any tests or exemplar use cases distributed with the 3PIP \nitself. The second is a system -level functional test performed to validate that a complete \nsystem using the 3PIP successfully achieves a s ystem -level outcome. When reviewing \na 3PIP block for a specific program, rather than for general use, the team may also take \nas an input any additional tests developed for  the block itself. However, the team should  \nnot assume that the 3PIP developer ran additional tests not shared with the review \nteam.  \nExample that meets the criteria: A multipoint bus 3PIP provides direct access \nfrom the Joint Test Action Group (JTAG) port of the FPGA to the bus.   \nThis attack is specifically useful, giving the adversary access to arbitrary data on the \nbus, which could easily include critical or classified data. It would be expect ed to be \nquickly discovered if placed into a widely distributed  piece of 3PIP as soon as a user \ntried to use the JTAG port themselves. However, the attack being mitigated by this \napproach is specifically looking at the possibility that a vendor receives a special version \n\n \n \nU/OO/ 230115 -22 | PP-22-1870  | DEC  2022 Ver. 1.0  8 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA1  \ntailored just for them. An adversary in this sc enario may have identified that JTAG is \nunused in a given system. This attack also could be detected by extensive testing and \nevaluation of the fina l system. As stated above, reviewers should assume that no test \nwould have involved JTAG if the system was n ot implementing JTAG intentionally.  \nExample that fails to meet the criteria: A modification to a PCI -E engine causes it \nto corrupt 0.001% of packets, degrading system performance . \nThis does not meet the criteria. Such a modification has no controlled effec t, but instead \ndegrades the performance at unknown times that are not necessarily useful to the \nadversary. As a resul t, this is not a concern at LoA 1. \n4.4 Defining a functional a rea \nThe remainder of this report discusses how to evaluate each functional area. To do this , \nthe review team must determine the functional area boundaries they want to use. A \nfunctional area is a set of circuit designs that have a specific documented purpose. \nEach functional area should  be reviewed and tested independently. The e valuators can \nset the exact scale of these functional areas. However, as is specified below, depending \non the design practices employed , the process may necessitate that functional areas be \ncombined into larger functional areas for evaluation, or generate considerably more \nwork bouncing between regions.  \nThe review team should make a first attempt at defining functional areas based on the \nfile structure, module documentation, and past experience. In general, small functional \nareas will be easier to evaluate.  Design practices that limit the connections to a \nfunctional area to those needed to perform its function will enable small functional \nareas. In practice, design best practices encourage limiting connectivity between \nmodules and thus this process should be  straightforward.  \n4.5 Criteria for functional areas  of interest  \nBased on the effe cts described at  LoA1, the following questions should be used to \ndetermine whether a functional area is inherently of interest and what level of review is \nrequired. For exampl e, review for controlled effects is much simpler tha n review for \npersistent effects:  \n Does the functional area have I/O s that an adversary will be able to influence? \nWhen evaluating a 3PIP for a specific system, system knowledge can be used. \n\n \n \nU/OO/ 230115 -22 | PP-22-1870  | DEC  2022 Ver. 1.0  9 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA1  \nWhen evaluating a 3PIP block in general, almost all I/O to the 3PIP block meet \nthis criteria .6 Additionally, inputs that are indirectl y connected to top -level I/O \nthrough other modules must be reviewed.  \n If so, it is a functional area of interest for controlled effects. All avenues \nfrom external stimuli that may affec t the device during normal use  should \nbe noted for investigation.  \n Does t he functional area have internal behaviors that serve a well -defined role in \nthe system, but could fail in wa ys that would not be apparent during  normal use ?7 \n If so, it is a functional area of interest for persistent effects. The specific \ndifficult to tes t internal behaviors should be noted for investigation.  \n Does the functional area have a well -constrained set of I/Os that limit it to \ninformation relevant to its intended function?  \n If not, it needs to be re -evaluated in combination with the other function al \nareas connected to it as a single , larger functional area.  \n5. Perform m anual code r eview  \nEach region requires two kinds of reviews: manual and test -driven. These two reviews \nmay influence each other and do not need to be completed in a particular sequence.  A \nmanual review is a traditional code review where knowledgeable designers read the \ncode to evaluate it for correctness.  \nBefore beginning manual code review , automatic tools should be used to generate lines \nof interest within the code. Both a lint tool and the synthesis process must be run. A lint \ntool can be run with settings derived from the apparent code standard used in \ndevelopment of the source. Any lines identified by the lint code should be marked for \nextra review during the review process. Additi onally, the 3PIP should be synthesized \nwith an appropriate FPGA synthesis tool. Any warnings generated should be marked for \nextra review.  \nEach functional area of interest must be reviewed independently. For functional areas \nthat are inspected for a control led effect, this review should focus on looking at the data \n                                                \n6 In certain cases it is possible to rule out external connections and influence when clear best practices would  prevent this behavior. Specifically, in the case of \nJTAG and other test infrastructure, the final write -up should simply note that these must not be exposed remotely to an adversary. They do not need to be \nreviewed as remote connections, as any system whe re an adversary could access them is compromised by definition.  \n7 An example of such a region would be encryption on an internal communications bus. If the bus is responsible for encryption a t one end, and decryption on the \nother end, no normal user of the  bus would necessarily notice an error.  \n\n \n \nU/OO/ 230115 -22 | PP-22-1870  | DEC  2022 Ver. 1.0  10 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA1  \npath for unexpected behaviors or unexpected connections from the functional area to \nother functional areas. For persistent effects, the specific hard -to-test function should be \nevaluated in depth for correctness.  \nFor assessing a controllable effect, the following factors must be evaluated : \n Where does the information from the I/O propagate?  \n If it propagates largely intact or is transformed in a simple way to other \nfunctional modules, then they must  be added as functional areas of \ninterest.  \n What logic handles the I/O? The logic within the functional area that processes \nthe I/O must be evaluated for signs of a trigger : \n If the logic is simple , then accounting for the state elements can be \nsufficient to  remove it from consideration. This is possible when many bits \nof previous input are not stored natively within the functional area. In these \ncases, that storage, or a finite state machine  (FSM) , would need to be \nadded in order to add a trigger.  \n If the logic is complex, it will require additional investigation : \n One phase of the investigation should focus on reviewing the code \nfor an unexpected, but explicit trigger. That is to say, specific lines \nof HDL or specific groups of gates, waiting for a specific co mmand.  \n The second phase of the investigation should focus on more subtle \neffects. In particular, this should look at each FSM  state not \ncovered by a test, and validate that it has a well -documented and \nmeaningful purpose. Special attention is needed if mul tiple \ninteracting FSMs  are present. In those cases, additional in -depth \nconsideration must be paid to the interactions between FSMs . \nParticular attention must be given to out -of-spec behavior in known \nprotocols.  \nFor investigating a persistent effect, the p rocess is more complex. Pote ntial persistent \neffects at LoA 1 are rare. However, appropriate experts must determine whether each \nsuch block is properly implemented. They must focus on the correctness and suitability \nof the implementation, with the entire im plementation being carefully reviewed. Review \nof persistent effects is more dependent on the test-driven code review.  \n\n \n \nU/OO/ 230115 -22 | PP-22-1870  | DEC  2022 Ver. 1.0  11 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA1  \n6. Perform t est-driven code r eview  \nAs part of the review process, simulation tests must be used. The simulation testing can \nbe performed at the top 3PIP level, the functional module level, or a mix of the two. With \nline-by-line code review, the test  benches that were provided with the 3PIP may be \nused as part of these tests.  JFAC will provide g uidance for blocks too large to efficiently \nsimula te. \nFor regions that are being investigated for a controlled effect, sufficient tests must be \ndeveloped such that all major desired functions are executed. In some cases, this may \ncorrespond to a requirements document or to the functions listed in a specif ication. In \naddition to that, code coverage8 must be recorded and any lines not included in the final \ncode coverage must be reviewed in detail, with a specific note identifying why they are \nnot of interest in the context of their functional module.  \nFor reg ions that are being investigated for a persistent effect, sufficient tests must be \ndeveloped such that all major desired functions are executed. In addition to this, \nconstrained random testing must be performed on the 3PIP. Code coverage must be \nrecorded. In some cases , such as redundant serial safety checks, it may be impossible \nto achieve 100% code coverage. For each line not covered in the final code coverage, a \nspecific note must be recorded  identifying why a test  bench did not reach that line and \nwhy i t is acceptable in the context of the larger system.  \nIn both cases, formal methods that prove equivalency between another reference \ndesign that has been vetted through this process and a 3PIP or functional module can \nsubstitute for the use of testing as de scribed above. Similarly reviewed software blocks \ncan also be used as a reference in this formal verification process.  \n7. Document and sign the review p ackage  \nA review package should summarize the results of the review and minimally provide the \nfollowing  items: \n The name of the vendor who sells the 3PIP ; \n The name of the 3PIP ; \n Any version information of the 3PIP ; \n                                                \n8 For the purposes of this specific document, code coverage is measured by statement and branch coverage, not signal or gate.  \n\n \n \nU/OO/ 230115 -22 | PP-22-1870  | DEC  2022 Ver. 1.0  12 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA1  \n The hash of the 3PIP package delivered by the vendor9; \n A list of any use restrictions that were ident ified in the review of the 3PIP ; \n The hash of a summary document detailing the review process, including all \nauditable information specified in the prerequisites section of this document ; \n The auditing organization that maintains  this summary document ; \n The portion of the summary document above not contai ning personally \nidentifiable information ; \n The hash of all test benches and scripts used in the review process ; \n If allowed by  the 3PIP vendor, the collection of the test benches and scripts used \nin the review process ; \n A summary hash of all the listed elemen ts above, including hashes of packages \nreviewed but not the package itself ; and  \n A cryptographic signature of the above information, tied to the reviewing \norganization.  \nThis set of information is a tradeoff designed to produce a distributable report not \nconstrained by agreements  that can be validated back to the specific files. The hashes \nin this report must be validated against a 3PIP for it to be used, and the report must be \nmaintained as a design artifact for future audits. For instance, explicitly includ e a hash \nof the 3PIP, but not the 3PIP itself, so that other users can determine if they have the \nsame files without needing to include the files themselves in the document.  \nShould an issue be identified during the review process , it should be reported to the \norganization requesting the r eview.  In the case of  U.S. Department of Defense \norganizations, any identified issue must be shared with the JFAC.  \n8. Pre-compiler and other machine -generated c ode \nMachine -generated code refers to any code that takes a set of parameters and uses \nthem to construct or pre -process HDL before a synthesis process. This can be custom \nC code that generates Verilog. This can be compiler directives such as ifdef in Verilog \nor generate in VHDL.  \nReview of a 3PIP block containing machi ne-generated code can be substantially more \nchallenging, and, depending on the complexity of the pre -compiler directives or scripts, \n                                                \n9 This review will be valid solely for 3PIP packages that match this hash.  \n\n \n \nU/OO/ 230115 -22 | PP-22-1870  | DEC  2022 Ver. 1.0  13 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA1  \nmay not be possible. Instead, it may require review with specific settings in place.  \nContact JFAC for additional guidance  for handling machine -generated code.  \nWhen reviewing 3PIP for use in a specific system, it is acceptable to run the pre -\ncompiler with the desired parameters and evaluate the result. In this case, the \ngenerated HDL must comply with the same standards as specified for any 3PIP block. \nIn addition, the review package must note those precise settings as a use restriction on \nthe 3PIP.  \n9. Summary  \nFor review of a 3PIP in general, the following guidelines govern the review:  \n Assemble review p rerequisites:  \n In cases where software is used to write HDL, members of the review \nteam that review the generator code must have experience in both the \nlanguage that the generator is written in and the language that is \ngenerated. For instance, to review C code that generates VHDL , the \nreviewer must have expertise in both.  \n Establish suitability of the 3PIP for review:  \n Are the purposes of all uses of pre -compiler directives or scripts clear and \nwell documented?  \n If not, the code is not suitable for review.  \n Are the parameters of the I P well documented with clear constraints?  \n If not, the code is not suitable to review.  \n Is the function of the 3PIP limited to a specific function, rather than an \narbitrary programmable set of functions10? \n If so, the code is suitable to review.  \n Partition the design : \n Partitions must be made with specific regard to the boundaries of the pre -\ncompiler directives. The I/O between partitions should not dynamically \nchange based on parameters outside of width changes.  \n Perform manual  code r eview : \n Manual code review mus t be performed with the pre -compiler directives in \nplace. The generator itself must be reviewed.  \n                                                \n10 For example, a parameter specifying the length of a filter is not problematic. A parameter indicating whether a CPU has a JTAG port is not  problematic. A \nparameter that specifies a complex equation or programmatic behavior to implement is problematic.  \n\n \n \nU/OO/ 230115 -22 | PP-22-1870  | DEC  2022 Ver. 1.0  14 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA1  \n Perform test-driven code r eview : \n Simulations must be done on a representative sample of \nparameterizations.  \n All conditional code blocks within ifdef, generat e, or similar statements \nmust be included in at least one of the samples simulated, though \nindividual lines within a block may be reviewed as in the normal flow.  \n Document  and sign the review package : \n List the specific details of all parameter sets used in  the evaluation.  \n Include this  data in the final summary hash.  \n  \n\n \n \nU/OO/ 230115 -22 | PP-22-1870  | DEC  2022 Ver. 1.0  15 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA1  \nAppendix  A: Standardized terminology  \nThe following terms are used in the Joint Federated Assurance Center Field \nProgrammable Gate Array Best Practices documents. These terms are  modified from \nDefense Acquisition University definitions to support common understanding.  \nApplication design   The collection of schematics, constraints, hardware description \nlanguage (HDL) , and other implementation files developed to generate an FPGA \nconfiguration file for use on one or many FPGA platforms.  \nApplication domain   This is the area of technology of the system itself, or a directly \nassociated area of technology. For instance, the system technology domain of a radar \nsystem implemented using FP GAs would be \"radar\" or \"electronic warfare.\"  \nConfiguration file  The set of all data produced by the application design team and \nloaded into an FPGA to personalize it. Referred to by some designers as a bitstream, \nthe configuration file includes that i nformation, as well as additional configuration \nsettings and firmware, which some designers may not consider part of their bitstream.  \nControllable effect   Program -specific , triggerable function allowing the adversary to \nattack a specific target.  \nDevice/ FPGA device   A specific physical instantiation of an FPGA.  \nExternal facility   An unclassified facility that is out of the control of the program or \ncontractor.  \nField  programmable gate array (FPGA)   In this context  FPGA includes the full range \nof devices  containing substantial reprogrammable digital logic. This includes devices \nmarketed as FPGAs, complex programmable logic device s (CPLD), system -on-a-chip \n(SoC) FPGAs, as well as devices marketed as SoCs and containing reprogrammable \ndigital logic capable of representing arbitrary functions. In addition, some FPGAs \nincorporate analog/mixed signal elements alongside substantial amounts of \nreprogrammable logic.  \nFPGA platform   An FPGA platform refers to a specific device type or family of devices \nfrom a vendo r.  \n\n \n \nU/OO/ 230115 -22 | PP-22-1870  | DEC  2022 Ver. 1.0  16 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA1  \nHard IP   Hard IP is a hardware design captured as a physical layout, intended to be \nintegrated into a hardware design in the layout process. Hard IP is most typically \ndistributed as Graphic Design System II (GDSII). In some cases, Hard IP is provided by \na fabrication company and the user of the IP does not have access to the full layout, but \nsimply a size and the information needed to connect to it. Hard IP may be distributed \nwith simulation hardware description language (HDL) and other soft components , but is \ndefined by the fact that the portion that ends up in the final hardware was defined by a \nphysical layout by the IP vendor.  \nLevel of assurance (LoA)   A Level of Assurance is an established guideline that \ndetails the appropriate mitigations necessa ry for the implementation given the impact to \nnational security associated with subversion of a specific system, without the need for \nsystem -by-system custom evaluation.  \nPhysical unclonable function (PUF)   This function provides a random string of bits of  \na predetermined length . In the context of FPGAs, the randomness of the bitstring is \nbased upon  variations in the silicon of the device due to manufacturing. These bitstrings \ncan be used for device IDs or keys.   \nPlatform design   The platform design is the set of design information that specifies \nthe FPGA platform, including physical layouts, code, etc.  \nSoft IP   Soft IP is a hardware design captured in hardware description language \n(HDL), intended to be integrated into a comple te hardware design through a synthesis \nprocess. Soft IP can be distributed in a number of ways, as functional HDL or a netlist \nspecified in HDL, encrypted or unencrypted.  \nSystem   An aggregation of system elements and enabling system elements to achieve \na given purpose or provide a needed capability.  \nSystem design  System design is the set of information that defines the \nmanufacturing, behavior, and programming of a system. It may include board designs, \nfirmware, software, FPGA configuration files, etc.  \nTarget  A target refers to a specific deployed instance of a given system, or a specific \nset of systems with a common design and function.  \n\n \n \nU/OO/ 230115 -22 | PP-22-1870  | DEC  2022 Ver. 1.0  17 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA1  \nTargetability  The degree to which an attack may have an effect that only shows up in \ncircumstances the adversary cho oses. An attack that is poorly targetable would be more \nlikely to be discovered accidentally, have unintended consequences, or be found in \nstandard testing.  \nThird -party intellectual property (3PIP)   Functions whose development are not \nunder the control of  the designer.  Use of the phrase intellectual property, IP , or 3PIP in \noutlining this methodology of design review does not refer to property rights , such as, \nfor example, copyrights, patents, or trade secrets. It is the responsibility of the party \nseeki ng review and/or the reviewer to ensure that any rights needed to perform the \nreview in accordance with the methodology outlined are obtained.  \nThreat category  A threat category refers to a part of the supply chain with a specific \nattack surface and set o f common vulnerabilities against which many specific attacks \nmay be possible.  \nUtility  The utility of an attack is the degree to which an effect has value to an \nadversarial operation. Higher utility effects may subvert a system or provide major \ndenial of service effects. Lower utility attacks might degrade a capability to a limited \nextent.  \nVulnerability  A flaw in a software, firmware, hardware, or service component \nresulting from a weakness that can be exploited, causing a negative impact to the \nconfide ntiality, integrity, or availability of an impacted component or components.  \n\n",
  "cves": [],
  "techniques": [],
  "advisory": "cybersecurity-alerts",
  "title": "ctr_dod_microelectronics-third_party_ip_review_process_for_loa1",
  "source": "nsa",
  "id": "29227c575e0577b51febcf9ae6fc01bd499062254281df45d5810fd5661639d2"
}