{
  "markdown": "Engaging with Artificial \nIntelligence (AI)\n\nEngaging with Artificial Intelligence (AI)2\n\n\nEngaging with Artificial Intelligence (AI)3Contents\nIntroduction   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\nWhat is AI?   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\nChallenges when engaging with AI   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\nMitigation considerations for organisations  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 9\nFurther Reading   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n\nEngaging with Artificial Intelligence (AI)4\nIntroduction\nThe purpose of this publication is to provide organisations with guidance on how to use AI \nsystems securely. The paper summarises some important threats related to AI systems and \nprompts organisations to consider steps they can take to engage with AI while managing risk. It \nprovides mitigations to assist both organisations that use self-hosted and third-party hosted AI \nsystems. \nThis publication was developed by the Australian Signals Directorates Australian Cyber Security \nCentre (ASDs ACSC) in collaboration with the following international partners:\n United States (US) Cybersecurity and Infrastructure Security Agency (CISA), the \nFederal Bureau of Investigation (FBI) and the National Security Agency (NSA)\n United Kingdom (UK) National Cyber Security Centre (NCSC-UK)\n Canadian Centre for Cyber Security (CCCS)\n New Zealand National Cyber Security Centre (NCSC-NZ) and CERT NZ\n Germany Federal Office for Information Security (BSI)\n Israel National Cyber Directorate (INCD)\n Japan National Center of Incident Readiness and Strategy for Cybersecurity (NISC) and \nthe Secretariat of Science, T echnology and Innovation Policy, Cabinet Office\n Norway National Cyber Security Centre (NCSC-NO)\n Singapore Cyber Security Agency (CSA)\n Sweden National Cybersecurity Center\nThe guidance within this publication is focused on using AI systems securely rather than \ndeveloping secure AI systems. The authoring agencies encourage developers of AI systems to refer \nto the joint Guidelines for Secure AI System Development .\n\nEngaging with Artificial Intelligence (AI)5\nWhat is AI?\nAI is the theory and development of computer systems able to perform tasks normally requiring \nhuman intelligence, such as visual perception, speech recognition, decision-making and \ntranslation between languages. Modern AI is usually built using machine learning algorithms. \nAI has several sub-fields of importance that include but are not limited to:\n Machine learning describes software components (models) that allow computers \nto recognise and bring context to patterns in data without the rules having to be \nexplicitly programmed by a human. Machine learning applications can generate \npredictions, recommendations, or decisions based on statistical reasoning.\n Natural language processing analyses and derives information from human language sources, \nincluding text, image, video and audio data. Natural language processing applications are \ncommonly used for language classification and interpretation. Many natural language processing \napplications not only process natural language but also generate content that mimics it. \n Generative AI refers to systems that use data models to generate new examples of content \nsuch as text, images, audio, code and other data modalities. Generative AI applications \nare typically trained on large amounts of real-world data and can approximate human \ngenerated content from prompts, even prompts that are limited or non-specific. \nAI systems are among the fastest growing applications globally. AI drives internet \nsearching, satellite navigation, and recommendation systems. AI is also increasingly \nused to handle activities traditionally undertaken by humans such as sorting large data \nsets, automating routine tasks, creative endeavours and augmenting business activities \nsuch as customer engagement, logistics, medical diagnosis, and cyber security.\nOrganisations from all sectors are exploring opportunities to improve their \noperations with AI. While AI has the potential to increase efficiency and lower costs, \nit can also intentionally or inadvertently cause harm. For this reason, government, \nacademia and industry have a role to play in managing the risks associated with this \ntechnology, including through research, regulation, policy and governance.\n\nEngaging with Artificial Intelligence (AI)6Challenges when \nengaging with AI\nLike all digital systems, AI presents both opportunities and threats. T o take advantage of the \nbenefits of AI securely, all stakeholders involved with these systems (e.g. programmers, end users, \nsenior executives, analysts, marketers) should take some time to understand what threats apply to \nthem and how those threats can be mitigated. \nSome common AI related threats are outlined below. These threats are not presented to \ndissuade AI use, but rather to assist all AI stakeholders to engage with AI securely. Cyber security \nmitigations that can be used to secure against these AI threats are covered in a later section of \nthis publication. For more information on AI-specific threats, adversary tactics and how they can \nbe risk-managed visit MITRE ATLAS  and the US National Institute of Standards and T echnologys \n(NIST) AI Risk Management Framework .\n1 . Data Poisoning of an AI Model\nNIST outlines that the data-driven approach of machine learning introduces security and privacy \nchallenges that are different from other systems (see NISTs Adversarial Machine Learning: A \nT axonomy and T erminology of Attacks and Mitigations ). These challenges include the potential for \nmanipulation of training data and exploitation of model vulnerabilities (also known as adversarial \nAI), which can adversely affect the performance of machine learning tools and systems.\nOne method of adversarial manipulation is data poisoning. Data poisoning involves manipulating \nan AI models training data so that the model learns incorrect patterns and may misclassify data \nor produce inaccurate, biased or malicious outputs. Any organisational function that relies on the \nintegrity of the AI systems outputs could be negatively impacted by data poisoning. An AI models \ntraining data could be manipulated by inserting new data or modifying existing data; or the \ntraining data could be taken from a source that was poisoned to begin with. Data poisoning may \nalso occur in the models fine-tuning process. An AI model that receives feedback to determine if it \nhas correctly performed its function could be manipulated by poor quality or incorrect feedback.\nCase Study: Tay Chatbot Poisoning  \nIn 2016, Microsoft trialled a Twitter chatbot called T ay that leveraged machine learning. \nThe chatbot used its conversations with users to train itself and adapt its interactions, \nleaving it vulnerable to a data poisoning attack. Users tweeted abusive language at T ay \nwith the intention of inserting abusive material into T ays training data therefore poisoning \nthe AI model when it was retrained. As a result, T ay began using abusive language towards \nother users.\n\nEngaging with Artificial Intelligence (AI)72 . Input manipulation attacks  Prompt injection and adversarial examples\nPrompt injection is an input manipulation attack that attempts to insert malicious instructions \nor hidden commands into an AI system.  Prompt injection can allow a malicious actor to \nhijack the AI models output and jailbreak the AI system. In doing so, the malicious actor \ncan evade content filters and other safeguards restricting the AI systems functionality.\nCase Study: DAN prompt injection  \nA widely reported example of prompt injection leading to jailbreaking an AI system is the \nDo Anything Now (DAN) prompt. Users of ChatGPT have discovered various ways to \nprompt ChatGPT to assume an identity named DAN that is not subject to the systems \nusual safety restrictions. OpenAIs efforts to address this case of prompt injection have \nbeen bypassed on several occasions by new iterations of the DAN prompt, highlighting the \nchallenges of enforcing safety restrictions on AI systems.\nAnother type of input manipulation attack is known as adversarial examples. In the context \nof AI, adversarial examples are the crafting of specialised inputs that, when given to an AI, \nintentionally cause it to produce incorrect outputs, such as misclassifications. Inputs can be \ncrafted to pass a confidence test, return an incorrect result or bypass a detection mechanism. \nNote that, in adversarial examples, inputs to the AI are manipulated while it is in use, rather \nthan when it is being trained. For example, consider a music sharing service that requires \nuser submitted music to pass an AI powered copyright check before it is published. In an \nadversarial example attack, a user might slightly speed up a copyright protected song so \nthat it passes the AI powered copyright check while remaining recognisable to listeners.\n3 . Generative AI hallucinations\nOutputs generated by an AI system may not always be accurate or factually correct. \nGenerative AI systems are known to hallucinate information that is not factually correct. \nOrganisational functions that rely on the accuracy of generative AI outputs could be \nnegatively impacted by hallucinations, unless appropriate mitigations are implemented.\nCase study: Southern District of New Y ork Legal Filing\nIn 2023, a district judge in the Southern District of New York reportedly \nfound that a legal brief submitted to him contained at least six hallucinated \ncases. The lawyer who submitted the brief attributed the hallucinated cases \nto research he undertook with ChatGPT, admitting in an affidavit that he \nwas unaware of the possibility that its content could be false.\n\nEngaging with Artificial Intelligence (AI)874 . Privacy and intellectual property concerns\nAI systems may also present a challenge to ensuring the security of sensitive data an \norganisation holds, including customers personal data and intellectual property. \nOrganisations should be cautious with what information they, and their personnel, provide \nto generative AI systems. Information given to these systems may be incorporated into the \nsystems training data and could inform outputs to prompts from non-organisational users.\nFor further information on applying privacy principles to generative AI technologies, \nvisit the Office of the Privacy Commissioner of Canadas Principles for responsible, \ntrustworthy and privacy-protective generative AI technologies .\n5 . Model stealing attack\nA model stealing attack involves a malicious actor providing inputs to an AI system \nand using the outputs to create an approximate replica of it. AI models can require a \nsignificant investment to create, and the prospect of model stealing is a serious intellectual \nproperty concern. For example, consider an insurance company that has developed an \nAI model to provide customers with insurance quotes. If a competitor were to query this \nmodel to the extent that it could create a replica of it, it could benefit from the investment \nthat went into creating the model, without sharing in its development costs. \nSimilarly, there are cases where prompts have led generative AI models to divulge \ntraining data. The exfiltration of training data can be a serious privacy concern for \nmodels that are trained on sensitive data, including data that contains personally \nidentifiable information. It is also a serious intellectual property concern for \norganisations that seek to maintain the confidentiality of their training data sets.\nCase study: ChatGPT memorised training data extraction\nIn November 2023, a team of researchers published the outcomes of their attempts to \nextract memorised training data from AI language models. One of the applications the \nresearchers experimented with was ChatGPT. In the case of ChatGPT, the researchers \nfound that prompting the model to repeat a word forever led the model to divulge training \ndata at a rate much higher than when behaving as normal 1. The extracted training data \nincluded personally identifiable information.\n1 Nasr, M., Carlini, N., Hayase, J., Jagielski, M., Cooper, A.F., Ippolito, D., Choquette-\nChoo, C.A., Wallace, E., Tramr, F. and Lee, K., 2023. Scalable extraction of training \ndata from (production) language models. arXiv preprint arXiv:2311.17035 .\n\nEngaging with Artificial Intelligence (AI)9\nMitigation \nconsiderations for \norganisations\nAI technologies are distinctive in their speed of innovation and scope of impact. As a result, \nit is important that organisations that use, or are considering using, AI systems, consider \ntheir cyber security implications. This includes evaluating the AI systems benefits and risks \nwithin the organisations context. The questions below are intended to prompt organisations \nto consider how they can use AI systems securely. They include a number of cyber security \nmitigations that may be relevant to their use of both their self-hosted AI systems and third-party \nAI systems. As an emerging technology, there are limited regulations to ensure AI systems are \nsecure.  In the absence of a robust regulatory framework, it is important organisations carefully \nconsider the risks associated with any AI system they are considering using. As AI is evolving \nquickly, the below mitigation considerations may need to be revisited on an ongoing basis. \nHas your organisation implemented the cyber security frameworks relevant  \nto its jurisdiction?\nYour organisations AI systems would benefit from many of the same cyber security mitigations \nthat you have implemented to protect your organisations other systems. Start by ensuring \nthat you have implemented the cyber security mitigations recommended in the framework \nthat is relevant to your jurisdiction. The Further Reading section below includes links \nto several cyber security frameworks developed by the authors of this publication. \n\nEngaging with Artificial Intelligence (AI)10How will the system affect your organisations privacy and data protection obligations?\nConsider how the AI system collects, processes and stores data, and how this \nmay impact your organisations privacy and data protection obligations. \n AI systems are often hosted in the cloud and may send data between different regions. Ensure that \nany AI system your organisation uses can meet your data residency or sovereignty obligations.\n If using a third-party AI system, understand if your organisations inputs will be used to \nretrain the AI systems model. Consider using private versions of the system, if available. \n If using a third-party AI system, ensure your organisation is aware of how your data will \nbe managed in the event your commercial agreement with the third-party ends. This \ninformation is typically outlined in the vendors privacy policy or terms of service.  \n If your AI system handles private data, consider if there are privacy \nenhancing technologies you can employ to protect that data.\nDoes your organisation enforce multi-factor authentication?\nRequire phishing-resistant multi-factor authentication, for example, FIDO2 security \nkeys, to access your organisations AI systems, including any repositories that \nhold training data. Multi-factor authentication protects against unauthorised \naccess to your organisations systems and resources. Unauthorised access could \nfacilitate several attacks including data poisoning and model theft. \nHow will your organisation manage privileged access to the AI system?\nGrant privileges based on the need-to-know principle and the principle of least \nprivilege. For example, limit the number of accounts with access to the AIs development \nand production environments and limit access to repositories that hold the AI \nmodels training data. Require that privileged accounts are routinely revalidated \nand disabled after a set period of inactivity. Restricting privileged access to the \nsystem mitigates several threats, including data poisoning and model theft.\nHow will your organisation manage backups of the AI system?\nMaintain backups of your AI model and training data. Backups will assist your \norganisation to recover if your AI system is affected by an incident. For example, if your \norganisation is affected by a data poisoning attack, maintaining backups would allow \nit to restore an unaffected copy of its training data and retrain its model. It should be \nnoted that backing up your AI model and its training data can be resource intensive.\nCan your organisation implement a trial of the AI system?\nTrialling an AI system can be an effective way to understand how the system can integrate \nwith your organisations cyber security systems and tools, for example, firewalls, gateways, \nextended detection and response tools, logging and monitoring systems. A trial can also \nhelp your organisation to test the limits and constraints of the system in a low-stakes \nenvironment. Before implementing a trial, consider its scope and success criteria. \n\nEngaging with Artificial Intelligence (AI)11Is the AI system secure-by-design, including its supply chain?\nSeek to use vendors that are transparent about how they have developed and \ntested their AI systems. Consider if the AI system has applied the guidelines \nrecommended in the NCSC-UKs Guidelines for secure AI system development . \nThe supply chains of AI systems can be complex and, as a result, are likely to carry inherent \nrisks. Conducting a supply chain evaluation can help you to identify and manage these risks. \nIf your organisation is involved in training the AI system it uses, consider the supply \nchain of foundational training data and fine-tuning data as well, to aid in preventing \ndata poisoning. The security of the data and model parameters is critical.\nDoes your organisation understand the limits and constraints of the AI system?\nAI systems can be incredibly complex. While it is often not practical, or possible, to understand \nthe intricacies of how AI systems work, it is still helpful to understand their general limits and \nconstraints. For example, is the AI system prone to hallucinations? If the system is involved in \ndata classification, what is its rate of false positives and false negatives? Understanding the \nsystems limits and constraints will assist your organisation to account for them in its processes.\nDoes your organisation have suitably qualified staff to ensure the AI system is set-up, \nmaintained and used securely?\nEnsure that your organisation is adequately resourced to securely set-up, maintain and use  \nthe AI system. \nConsider which staff would be interacting with the AI system, what these staff would be required \nto know to interact with the system securely and how this knowledge can be developed. \nStaff that use the system should be trained on what data can and cannot be input to the system, \nfor example, personally identifiable information or the organisations intellectual property. Staff \nshould also be trained on the extent to which the systems outputs can be relied upon and any \norganisational processes for output validation. \nDoes your organisation conduct health checks of your AI system?\nConduct periodic health checks of AI systems to detect data drift and ensure the system is working \nefficiently and as intended. Data drift describes when the data an AI system encounters in the real \nworld differs from the data that system was trained on. Data drift can lead to a degradation in the \nAI systems performance. It typically occurs over time, as the environment the AI system operates \nin changes. Periodically updating the systems training data with data received during normal \nusage can mitigate data drift. For more information on secure operation and maintenance of your \norganisations AI system visit the joint publication Guidelines for Secure AI System Development .\n\nEngaging with Artificial Intelligence (AI) 12Does your organisation enforce logging and monitoring?\nConsider how your organisation would detect anomalies or malicious activity associated with  \nthe AI system. \n Log and monitor outputs from the AI system to detect any change in behaviour \nor performance that may indicate a compromise or data drift.\n Log and monitor inputs to the AI system to ensure compliance obligations are met \nand to aid investigation and remediation efforts in the event of an incident.\n Log and monitor the network and endpoints that host your AI system \nto detect attempts to access, modify or copy system data.\n Log and monitor logins to repositories that hold training data, the AI systems \ndevelopment and production environments and backups. Consider how any logging \nand monitoring tools your organisation employs may integrate with your AI system.\n Log and monitor for high frequency, repetitive prompts. These can \nbe a sign of automated prompt injection attacks.  \n Establish a baseline of the AI systems activity to assist your organisation \nto determine when logged events are anomalous.  \nWhat will your organisation do if something goes wrong with the AI system?\nConsider how your organisation may be impacted if an incident or error affects the AI \nsystem so that you can implement proportionate mitigations and contingencies.\nIf you are using a third-party AI system, familiarise yourself with any up-time or availability \ncommitments the vendor has made. Ensure that vendor and customer responsibilities \nregarding incident management are clearly defined in the service contract.\nEnsure that your organisations incident response plan accounts for issues arising from, \nor to, its AI systems and consider how business continuity can be achieved in the event \nof a serious incident. Your incident response plan should also clearly define the roles and \nresponsibilities that are critical to addressing any incident that affects the AI system.\n\nEngaging with Artificial Intelligence (AI)13Further Reading\nASDs Cyber Supply Chain Risk Management\nGuidance published by ASD to help \norganisations manage risk in their  \ncyber supply chain.\nASDs Essential Eight\nASD has developed prioritised mitigation \nstrategies, in the form of the Strategies to \nMitigate Cyber Security Incidents , to help \norganisations protect themselves against \nvarious cyber threats. The most effective of \nthese mitigation strategies are the Essential \nEight. The Essential Eight has been designed \nto protect organisations internet-connected \ninformation technology networks.\nASDs Ethical AI framework\nASD has developed a framework that \nincorporates a set of ethical principles \nwhich govern how AI is used at ASD.\nASDs Information Security Manual\nASD produces theInformation Security \nManual. The purpose of the Information \nSecurity Manual is to outline a cyber security \nframework that an organisation can apply, \nusing their risk management framework, \nto protect their systems and data from \ncyber threats. The Information Security \nManual is intended for Chief Information \nSecurity Officers, Chief Information \nOfficers, cyber security professionals and \ninformation technology managers.\nBSIs AI Cloud Service Compliance \nCriteria Catalogue (AIC4)\nBSIs AI Cloud Compliance Criteria \nCatalogue provides AI-specific criteria, \nwhich enable evaluation of the security \nof an AI service across its lifecycle.BSIs Large Language Models: Opportunities \nand Risks for Industry and Authorities\nDocument produced by BSI for companies, \nauthorities and developers who want to learn \nmore about the opportunities and risks of \ndeveloping, deploying and/or using LLMs.\nCCCS Principles for responsible, \ntrustworthy and privacy-protective \ngenerative AI technologies\nThe Office of the Privacy Commissioner \nof Canada has published guidance \nto help organisations developing, \nproviding or using generative AI to apply \nkey Canadian privacy principles.\nCERT NZs T op online security \ntips for your business\nGuidance that includes cyber security \nmitigation strategies as well as why they \nmatter and how to implement them.\nHiroshima AI Process Comprehensive \nPolicy Framework\nThis is the first international framework that \nincludes guiding principles and a code of \nconduct aimed at promoting the safe, secure \nand trustworthy development of advanced AI \nsystems. The policy framework was successfully \nagreed upon at the G7 Digital & T ech Ministers \nMeeting in December 2023 and was endorsed \nby the G7 Leaders in the same month.\nThe Hiroshima AI Process was launched in \nMay 2023, following the Leaders direction \nat the G7 Hiroshima Summit in Japan.\nMITRE ATLAS\nMITRE ATLAS (Adversarial Threat \nLandscape for Artificial-Intelligence \nSystems) is a globally accessible, living \nknowledge base of adversary tactics and \ntechniques based on real-world attack \nobservations and realistic demonstrations \nfrom AI red teams and security groups.\n\nEngaging with Artificial Intelligence (AI)14NIST Adversarial Machine Learning: \nA Taxonomy and Terminology \nof Attacks and Mitigations\nThis NIST report on AI develops a taxonomy \nof attacks and mitigations and defines \nterminology in the field of adversarial machine \nlearning. T aken together, the taxonomy \nand terminology are meant to inform other \nstandards and future practice guides for \nassessing and managing the security of AI \nsystems by establishing a common language \nfor understanding the rapidly developing \nadversarial machine learning landscape.\nNIST AI Risk Management Framework\nIn collaboration with the private and public \nsectors, NIST has developed a framework \nto better manage risks to individuals, \norganizations, and society associated \nwith AI. Itis intended for voluntary use \nand to improve the ability to incorporate \ntrustworthiness considerations into the \ndesign, development, use, and evaluation \nof AI products, services, and systems.\nNIST Cybersecurity Framework\nThe NIST Cybersecurity Framework \nconsists of standards, guidelines and best \npractices to manage cyber security risk. It \nis comprised of three parts: the Framework \nCore, the Framework Implementation \nTiers, and the Framework Profiles. Each \nframework component reinforces the \nconnection between business/mission \ndrivers and cyber security activities.\nNCSC NZ Cyber Security Framework \nNCSC NZs cyber security framework sets \nout how NCSC NZ thinks, talks about, and \norganises its cyber security efforts. Its five \nfunctions and twenty-five security objectives \nrepresent the breadth of work needed to \nsecure an organisation in New Zealand.\nNCSC-NZ Interim Generative AI \nguidance for the public service\nThe New Zealand Government has published \ninterim guidance on the use of generative \nAI in the public service. The guidance \nincludes 10 dos for trustworthy use of \ngenerative AI in the public service.  NCSC-UK 10 Steps to Cyber Security\nThe NCSC-UKs 10 steps to cyber security \nprovides a summary of the NCSC-UKs advice \nfor medium to large organisations. It aims to \nhelp organisations manage their cyber security \nrisks by breaking down the task of protecting \nthe organisation into 10 components. \nNCSC-UK Guidelines for Secure \nAI System Development\nGuidelines co-sealed by Australia, Canada, \nNew Zealand, the United Kingdom, the United \nStates and a number of international partners \nfor providers of any systems that use artificial \nintelligence (AI), whether those systems have \nbeen created from scratch or built on top \nof tools and services provided by others.\nNCSC-UK Principles for the \nsecurity of machine learning\nThese principles aim to be wide reaching \nand applicable to anyone developing, \ndeploying or operating a system with a \nmachine learning (ML) component. They are \nnot a comprehensive assurance framework \nto grade a system or workflow, and do not \nprovide a checklist. Instead, they provide \ncontext and structure to help scientists, \nengineers, decision makers and risk owners \nmake educated decisions about system \ndesign and development processes, helping \nto assess the specific threats to a system.\nOWASP Machine Learning Security T op 10\nThe OWASP Machine Learning Security \nT op 10 project delivers an overview \nof the top 10 security issues relating \nto machine learning systems.\nUS Department of Defense 2023 \nData, Analytics, and Artificial \nIntelligence Adoption Strategy\nThis strategys approach embraces the need \nfor speed, agility, learning, and responsibility. \nPursuing this agile approach and focusing \nactivities on the goals outlined in this strategy \nwill allow the US Department of Defense \nto adopt data, analytics, and AI-enabled \ncapabilities at the pace and scale required \nto build enduring decision advantage.\n\nEngaging with Artificial Intelligence (AI)15\nDisclaimer\nThe material in this guide is of a general nature and should not be regarded \nas legal advice or relied on for assistance in any particular circumstance or \nemergency situation. In any important matter, you should seek appropriate \nindependent professional advice in relation to your own circumstances.\nThe Commonwealth accepts no responsibility or liability for any damage, loss or \nexpense incurred as a result of the reliance on information contained in this guide.\nCopyright  .\n Commonwealth of Australia 2023 .\nWith the exception of the Coat of Arms and where otherwise stated, all material \npresented in this publication is provided under a Creative Commons Attribution  \n4.0 International licence ( www.creativecommons.org/licenses ).\nFor the avoidance of doubt, this means this licence only applies to material  \nas set out in this document.\nThe details of the relevant licence conditions are available on the Creative \nCommons website as is the full legal code for the CC BY 4.0 licence \n(www.creativecommons.org/licenses ).\nUse of the Coat of Arms  .\nThe terms under which the Coat of Arms can be used are detailed  \non the Department of the Prime Minister and Cabinet website  \n(www.pmc.gov.au/government/commonwealth-coat-arms ).\nFor more information, or to report a cyber security incident, contact us:\ncyber.gov.au   |  1300 CYBER1 (1300 292 371) .\n\n",
  "cves": [],
  "techniques": [],
  "advisory": "cybersecurity-alerts",
  "title": "csi-engaging-with-artificial-intelligence",
  "source": "nsa",
  "id": "633bfd8378384fa80cc13ad4f81c4ffa6258bf1f0072773803dc5ff08f8b9274"
}