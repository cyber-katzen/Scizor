{
  "markdown": " \n \n \nNational Security Agency  \nCybersecurity  Technical Report  \n \n \n \n \n \nDoD Microelectronics:  \nField Programmable Gate Array \nLevel of Assurance 3 Best Practices  \n \n \n \n \nJune  2023 \n \nU/OO/ 170671 -23 \nPP-23-1734  \nVersion 1.0 \n \n \n \n \n  \n\n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   ii \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \n \n This document was created through collabora tion with each of the \nJFAC labs: National Security Agency (NSA), Air Force Research \nLab (AFRL) RYDT, Naval Surface Warfare Center (NSWC) Crane, \nand Army Development Command (DEVCOM)/AVMC.  \nFor additional information, guidance, or assistance with this \ndocume nt, please contact the Joint Federated Assurance Center \n(JFAC) at JFAC_HWA@radium.ncsc.mil . \n  \n  \n\n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   iii \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nNotices and history   \nDocument change history  \nDate  Version  Description  \nJUN 2023 1.0 Initial Publication  \n   \nDisclaimer of warranties and endorsement  \nThe information and opinions containe d in this document are provided \"as is\" and without any warranties \nor guarantees. Reference herein to any specific commercial products, process, or service by trade name, \ntrademark, manufacturer, or otherwise, does not constitute or imply its endorsement, recommendation, or \nfavoring by the United States Government, and this guidance shall not be used for advertising or product \nendorsement purposes.  \nPublication information  \nAuth or(s)   \nNational Security Agency  \nCybersecurity Directorate  \nJoint Federated Assurance Center  \nContact information  \nNSA Joint Federated Assurance Center : JFAC_HWA@radium.ncsc.mil  \nCybersecurity Report Feedback /  General Cybersecurity Inquiries: CybersecurityReports@nsa.gov   \nDefense Industrial Base Inquiries and Cybersecurity Services: DIB_Defense@cyber.nsa.gov   \nMedia inquiries / Press Desk: Media Relations, 443-634-0721, MediaRelations@nsa.gov   \nPurpose  \nThis document was developed in furtherance of NSAs cybersecurity missions . This  includ es its \nresponsibilities to ident ify and disseminate threats to National Security Systems, Department of Defense \ninformation systems, and the Defense Industrial Base, and to develop and issue cybersecurity \nspecifications and mitigations. This information may be shared broadly to reach all  appropriate \nstakeholders.   \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   iv \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nExecutive summary  \nIn support of securing Field Programmable Gate Array (FPGA)  based systems from \nadversary influence during the manufacturing process, this report outlines  the \ncategories of relevant threats and the best practice s for mitigating them at Level of \nAssurance 3 (LoA3 ). LoA3  captures the threats that are technically feasible but have \nhigh cost to implement , in addition to all LoA1  and LoA2  threats.  This level is defined as \ncausing extremely grave  harm  to U.S.  personnel , property, or interests if the systems \nfail. At this level, these threats have the following characteristics:  \n Access   Exploit  multiple points of difficult access  in different areas of the \ncustom microelectronic components ( CMC ) supply chain.  \n Technology   Feasible  threats for which existing research indicates the \nlikelihood that technology could be developed with an investment that would be \nfeasible for a known adversary .  \n Investment   A nation -state  scale directed priority  requiring resources from \nmany sp ecialties and organizations across a wide scope to facilitate an attack.   \n Value of effect   Fully or partially degrading a sys tem or feature.  \n Targetability   Affect only a subset of systems . \nOrganized by threat, this report provides  multiple technical miti gations to choose from to \nmitigate each threat  and to allow the program the best fit for their program needs. The \nfollowing table identifies the ten threat descriptions (TD) addressed by this guidance.  \n# Threat description (TD)  \nTD 1  Adversary utilizes a k nown FPGA platform vulnerability  \nTD 2  Adversary inserts malicious counterfeit  \nTD 3  Adversary compromises application design cycle  \nTD 4  Adversary compromises system assembly, keying, or provisioning  \nTD 5  Adversary compromises third -party soft intellectu al property (IP)  \nTD 6  Adversary swaps configuration file on target  \nTD 7  Adversary substitutes modified FPGA software design suite  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   v \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \n# Threat description (TD)  \nTD 8  Adversary modifies FPGA platform family at design  \nTD 9  Adversary compromises single -board computing system (SBCS)  \nTD 10 Adversary modifies vendor FPGA software design suite during \ndevelopment  \nEach subsection in this report contains mitigations described in detail to enable clear \nimplementation. Secondary documents are referenced in cases where the suggested \nmitigation is highly detailed, specific to individual FPGA platforms, or subject to frequent \nchange.  In some cases, one hundred percent threat mitigation is not possible.  The \nprovided guidance adds additional layers of protections to increase the difficulty of \nmalici ous action.  Additionally, the risks posed by the threat are explained. Appendix D: \nChecklists and data /documentation  requirements contains a quick reference list of \nthreats and associated mitigations.  \nOnce the program has mitigated these threats, they have  achieved an assurance level \nof LoA3 . \n  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   vi \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nContents  \nDoD Microelectronics: Field Programmable Gate Array Level of Assurance 3 \nBest Practices  ................................ ................................ ................................ ............................. i \nExecutive summary  ................................ ................................ ................................ ......................  iv \nContents  ................................ ................................ ................................ ................................ ...... vi \n1 Overview of Level of Assurance 3 threats and mitigations  ................................ ..................  1 \n1.1 Complementary standards and guidance  ................................ ................................ ................................ . 4 \n1.2 Exclusions  ................................ ................................ ................................ ................................ .............................  5 \n1.3 Document use  ................................ ................................ ................................ ................................ ......................  6 \n1.4 General comments on mitigations  ................................ ................................ ................................ ...............  7 \n2 Threat descriptions (TD)  ................................ ................................ ................................ ...........  7 \nTD 1: Adve rsary utilizes a known FPGA platform vulnerability  ................................ ..............  7 \nTD 1 mitigations  ................................ ................................ ................................ ................................ ..........................  8 \nTD 1 mitigation descriptions  ................................ ................................ ................................ ................................ .. 8 \nUse caution when selecting tools or platforms  ................................ ................................ ..........................  8 \nUse cleared personnel  ................................ ................................ ................................ ................................ ........ 8 \nResearch vulnerab ilities  ................................ ................................ ................................ ................................ ...... 8 \nUse revision control/version management  ................................ ................................ ................................ .. 9 \nEnforce auditability  ................................ ................................ ................................ ................................ .............  10 \nEnforce the approved design process  ................................ ................................ ................................ ........ 10 \nTD 2: Adversary inserts malicious counterfeit  ................................ ................................ ....... 11 \nTD 2 mitigations  ................................ ................................ ................................ ................................ ........................  13 \nTD 2 mitigation descriptions  ................................ ................................ ................................ ................................  13 \nPurchase from DoD authorized vendors and distributors  ................................ ................................ ... 13 \nConsult GIDEP  ................................ ................................ ................................ ................................ .....................  13 \nFollow storage and shipping guidance  ................................ ................................ ................................ ....... 14 \nVerify the FPGA cryptographically secure identifier  ................................ ................................ ..............  14 \nPerform physical inspection/analysis  ................................ ................................ ................................ ..........  17 \nCleared insider  ................................ ................................ ................................ ................................ .....................  20 \nTD 3 : Adversary compromises application design cycle  ................................ ......................  21 \nTD 3 mitigations  ................................ ................................ ................................ ................................ ........................  22 \nUse Secret level cleared personnel  ................................ ................................ ................................ .............  23 \nTrack critical data in a revision control system  ................................ ................................ ........................  23 \nEnforce auditability  ................................ ................................ ................................ ................................ .............  23 \nUse re vision control/version management  ................................ ................................ ................................  24 \nTD 3.1 Mitigating the introduction of a compromised design into the application  ..........................  24 \nIsolate and store the application design  ................................ ................................ ................................ ..... 25 \nPerform reproducible build  ................................ ................................ ................................ ..............................  25 \nTD 3.2 Mitigating the modification of test benches or plans to  reduce coverage or hide Trojan \ncode  ................................ ................................ ................................ ................................ ................................ ...............  26 \nExecute a documented test plan  ................................ ................................ ................................ ...................  26 \nValidate and verify the test processes  ................................ ................................ ................................ ........ 27 \nMaintain test environment via configuration management  ................................ ................................ . 27 \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   vii \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nTD 3.3 Mitigating the introduction of a Trojan into the application design during \ndevelopment  ................................ ................................ ................................ ................................ ...............................  27 \nMaintain bi -directional links to approved requirements  ................................ ................................ ........ 27 \nEnforce peer review  ................................ ................................ ................................ ................................ ...........  28 \nExecute a documented test plan  ................................ ................................ ................................ ...................  28 \nImplement, validate, and verify test processes  ................................ ................................ .......................  29 \nSelect a for mal proof process  ................................ ................................ ................................ ......................  29 \nTD 3.4 Mitigating the introduction of compromised tooling or software into the environment  .. 30 \nValidate cr yptographic hashes  ................................ ................................ ................................ .......................  30 \nResearch vulnerabilities  ................................ ................................ ................................ ................................ .... 30 \nValidate tools  ................................ ................................ ................................ ................................ .........................  31 \nTD 3.5 Mitigating intrusion into the internal network  ................................ ................................ ..................  32 \nAssign roles  ................................ ................................ ................................ ................................ ...........................  33 \nControl and monitor access  ................................ ................................ ................................ ............................  33 \nResearch vulnerabilities  ................................ ................................ ................................ ................................ .... 33 \nUse a secret or classified network  ................................ ................................ ................................ ................  34 \nTD 3.6 Mitigating r isk from a compromised employee  ................................ ................................ ..............  34 \nEnforce auditability  ................................ ................................ ................................ ................................ .............  34 \nEnforce the approved design process  ................................ ................................ ................................ ........ 35 \nReview critical design activities  ................................ ................................ ................................ .....................  35 \nUse cleared personnel  ................................ ................................ ................................ ................................ ...... 35 \nTD 3.7 Mitigating risk associate d with the compromise of device identifiers  ................................ ... 35 \nStore device identifiers  ................................ ................................ ................................ ................................ ...... 36 \nLimit access to device identifier information  ................................ ................................ .............................  36 \nTD 4: Adversary compromises system assembly, keying, or provisioning  ........................  36 \nTD 4 mitigations  ................................ ................................ ................................ ................................ ........................  37 \nTD 4 mitigation descriptions  ................................ ................................ ................................ ................................  38 \nPurchase from DoD authorized vendors and distributors  ................................ ................................ ... 38 \nFollow storage and shipping guidance  ................................ ................................ ................................ ....... 38 \nProvide keys and configuration data  ................................ ................................ ................................ ...........  39 \nClear memory devices  ................................ ................................ ................................ ................................ ....... 39 \nProvision private keys  ................................ ................................ ................................ ................................ ........ 39 \nProtect the configuration data package  ................................ ................................ ................................ ...... 39 \nPerform verification activities  ................................ ................................ ................................ ..........................  39 \nAuthenticate the FPGA device  ................................ ................................ ................................ .......................  40 \nTD 5: Adversary compromises third -party soft IP  ................................ ................................ .. 41 \nTD 5 mitigations  ................................ ................................ ................................ ................................ ........................  41 \nTD 5 mitigation descriptions  ................................ ................................ ................................ ................................  42 \nPurchase from DoD authorized vendors and distributors  ................................ ................................ ... 42 \nOnly accept IP that is unobfuscated  ................................ ................................ ................................ ............  42 \nEnsure IP deliverable packages are digitally signed ................................ ................................ .............  42 \nValidate the cryptographic hash  ................................ ................................ ................................ ....................  42 \nStore IP in a revision control repository ................................ ................................ ................................ ...... 42 \nExamine IP for mali cious functions  ................................ ................................ ................................ ..............  43 \nTD 6: Adversary swaps configuration file on target  ................................ ...............................  43 \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   viii \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nTD 6 mitigations  ................................ ................................ ................................ ................................ ........................  44 \nTD 6 mitigation descriptions  ................................ ................................ ................................ ................................  45 \nIncorporate cryptographic authentication  ................................ ................................ ................................ .. 45 \nAuthenticate configurat ion data each time the data is loaded  ................................ ..........................  45 \nPrevent direct read back ................................ ................................ ................................ ................................ ... 45 \nUse a CNSS/NIST approved algorithm and key length  ................................ ................................ ....... 45 \nUse DoD evaluated authentication mechanisms  ................................ ................................ ....................  45 \nDisable test access pins  ................................ ................................ ................................ ................................ ... 46 \nEnsure authentication for modifications  ................................ ................................ ................................ ..... 46 \nUse a FIPS 140 -2 compliant, Level 2 HSM  ................................ ................................ ..............................  48 \nTD 7: Adversary substitutes modified FPGA software design suite  ................................ .... 48 \nTD 7 mitigations  ................................ ................................ ................................ ................................ ........................  49 \nTD 7 mitigation descriptions  ................................ ................................ ................................ ................................  49 \nPurchase from DoD authorized vendors and distributors  ................................ ................................ ... 49 \nPrevent automatic tool updates  ................................ ................................ ................................ .....................  49 \nUse a trusted compu ting environment  ................................ ................................ ................................ ........ 49 \nUse cleared personnel  ................................ ................................ ................................ ................................ ...... 50 \nValidate the cryptographic hash  ................................ ................................ ................................ ....................  50 \nValidate the tool output  ................................ ................................ ................................ ................................ ..... 50 \nTD 8: Adversary modifies FPGA platform family at design  ................................ ...................  51 \nTD 8 mitigations  ................................ ................................ ................................ ................................ ........................  52 \nTD 8 mitigation description  ................................ ................................ ................................ ................................ ... 52 \nEngage JFAC  ................................ ................................ ................................ ................................ ........................  52 \nTD 9: Adversary comprom ises single -board computing system (SBCS)  ...........................  53 \nTD 9 mitigations  ................................ ................................ ................................ ................................ ........................  53 \nTD 9 mitigation descriptions  ................................ ................................ ................................ ................................  54 \nEngage a DoD vendor to build the SBCS  ................................ ................................ ................................ . 54 \nVerification and authentication  ................................ ................................ ................................ .......................  54 \nAuthenticate the FP GA devices ................................ ................................ ................................ .....................  54 \nVerify the SBCS configuration process  ................................ ................................ ................................ ...... 54 \nTest non -volatile memory  ................................ ................................ ................................ ................................ . 55 \nDocument the steps  ................................ ................................ ................................ ................................ ...........  55 \nTD 10: Adversary modifies vendor FPGA software design suite during development  ..... 55 \nTD 10  mitigations  ................................ ................................ ................................ ................................ .....................  56 \nTD 10 mitigation descriptions  ................................ ................................ ................................ ..............................  56 \nPerform logical equivalency checking  ................................ ................................ ................................ ......... 56 \n3 Summary  ................................ ................................ ................................ ................................ ... 57 \nAppendix A: Standardized terminology  ................................ ................................ ...................  58 \nAppendix B: IP Reuse Guidance  ................................ ................................ ...............................  61 \nReuse conditions  ................................ ................................ ................................ ................................ ......................  61 \nReuse scenarios  ................................ ................................ ................................ ................................ .......................  62 \nAppendix C: JFAC FPGA reporting template  ................................ ................................ ..........  65 \nAppendix D: Mitigations and data/documentation requirements  ................................ .........  69 \nChecklist for TD 1: Adversary utilizes a known FPGA platform vu lnerability  ................................ ... 69 \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   ix \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nChecklist for TD 2: Adversary inserts malicious counterfeit  ................................ ................................ .... 71 \nChecklist for TD 3: Adversary compromises applicat ion design cycle  ................................ ...............  74 \nChecklist for TD 4: Adversary compromises system assembly, keying, or provisioning  ............  83 \nChecklist for TD 5: Ad versary compromises third -party soft IP  ................................ .............................  85 \nEnsure IP deliverable packages are digitally signed ................................ ................................ .............  86 \nChecklist for TD 6: Adversary swaps configuration file on target  ................................ .........................  87 \nChecklist for TD 7: Adversary substitutes modified FPGA software design suite ..........................  88 \nChecklist  for TD 8: Adversary modifies FPGA platform family at design  ................................ ..........  90 \nChecklist for TD 9: Adversary compromises single -board computing system (SBCS)  ...............  90 \nChecklist for TD 10: Adversary modifies vendor FPGA software design suite during \ndevelopment  ................................ ................................ ................................ ................................ ...............................  92 \n \nTable s \nTable 1: LoA3 threats  ................................ ................................ ................................ ................................ .................  3 \nTable 2: List of AS6171 slash sheets ................................ ................................ ................................ ...............  17 \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   1 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \n1 Overview of Level of Assurance 3 threats and \nmitigations  \nThis document provides JFACs recommended hardware assurance strategies \nfor Fi eld Programmable Gate Array (FPGA) devices.  The guidance outlined by this \ndocument provides hardware assurance to systems requiring Level of Assurance 3 \n(LoA3 ). Additionally, it provides the requisite strategies and details for implementing \neach threat mit igation. Secondary documents are referenced in cases where the \nsuggested mitigation is highly detailed, specific to individual FPGA platforms, or subject \nto frequent change.  \nThis guidance is meant to stand on its own and not require the participation of JF AC in \nthe development process of a programs product, unless required by a specific \nmitigation. However, JFAC does remain at the ready to aid programs who seek to better \nunderstand this guidance, to incorporate a program specific mitigation or are seeking \nalternatives to the guidance contained herein. For further information or support, please  \nvisit the JFAC portal at https://jfac.navy.mil.  \nIn addition, to threats and mitigations identified at LoA1 and LoA2 , LoA3  requires \nmitigations against FPGA assurance threats that have the following characteristics:  \nAccess  Multiple points of difficult access  in different areas of the custom \nmicroelectronic components  (CMC ) supply chain.  \n This could include multiple people working on different elements of the CMC or \ngovernment design teams.  \n This could include multiple people performing different functions in the fabrication \nprocess.   \n This could include single or multiple cleared insiders working on the same or \ndifferent parts of the supply chain.  \nFor a mitigation based o n access to be effective, it needs to make it considerably more \ndifficult to carry out the attack. Examples include necessitating multiple points of difficult \naccess via many cleared people  in conjunction with attacking multiple areas of the \nsupply chain s uch that actors will need coordination and communication amongst the \ngroup.  \n \nLoA3  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   2 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nTechnology   Technologically feasible  threats for which existing research indicates \nthe likelihood that technology could be developed with an investment that would be \nfeasible for  a known adversary. However, these threats may not be associated with \nexisting and/or known tools and may not have associated reporting indicating adversary \nactivity. Moreover, while all the threats are validated to be possible, it may be that there \nis no known or ongoing investment in the capability.   \nFor a mitigation based on technological complexity to be effective, it must increase the \nlevel of technology needed to carry out the attack to that which is beyond what is \nrecognized as technically feasible a nd practical. This includes areas for which there is \nno known research . \nInvestment  A nation -state  scale directed priority refers to a substantive program \nconducted by a nation state that coordinates resources from many specialties and \norganizations acros s a wide scope to facilitate an attack.   \nFor a mitigation based on investment of resources to be effective, it must force the \nattacker to expend greater resources that would be daunting even for  a nation -state . \nValue of Effect  Degrade system performance  are those effects that reduce  the \nbehavior  of a system without fully disabling any specific feature or reliably having a \nspecific  planned effect. Note, that the term degradation may be used in some domains \nin a different way. For instance, a communications  link might be degraded in a way \nthat prevents all communication. Such an attack would fall under disabling a capability \nfor the purposes of this evaluation. In addition,  this LoA must consider  all higher value \neffects described in LoA1  and LoA2 .  \nFor a mitigation based upon value of effect  to the adversary  to be effective  in LoA3 , it \nmust eliminate or substantially reduce the value to the attacker.  \nTargetability  Blind attacks  refers to attacks that impact large numbers of parts,  \nwhole device families, or users in a way that has a significant likelihood of discovery  \nwithout effort , but only  to impact a specifically targeted part. Blind attacks are those \nwhere i t is hard to predict the interaction between what adversaries do and the intended \nconsequence  of the attack. This could include attacks that are performed against far \nmore targets  than expected, or with an intelligent agent that acts without an outside \ntrigger or  without foreknowledge of the attack outcome that would inform the adversary \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   3 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nof its exec ution. These attacks can also include activation times that cannot be \ncontrolled once fielded; that is to say, a pre -determined time at which devices will fail.  \nFor a mitigation based on targetability to be effective, it must remove the ability of the \nadversary to affect target ed systems and force the adversary  to rely on general and \nblind attacks.  \nFor a program to achieve Level of Assurance 3, it must provide mitigations against \nthreats that possess  these characteristics. Of prime importance in LoA3  is the assumed \npresence of one or more compromised cleared insider s and allowance for attacks that \nare not targeted, but broadly applied to the entire supply chain. These new conditions \nrender classified facilities and cleared people ineffective as a sole mea ns of mitigation. \nAs such, many of the mitigations offered in this guide focus on nullifying this adversarial \nadvantage using dual or independent teams. LoA3  addresses threats that originate from \nan adversary whose intent is malicious , but unlike the previ ous LoA levels also includes \ncases where reliability is also compromised. These threats should be addressed by the \nreliability testing of a program. For programs with stringent or specific reliability \nrequirements, it is strongly recommended that the appro priate level of testing be \nconducted to ensure the proper operation of the product rather than relying on \nassurance mitigations.  \nThe following table lists the ten FPGA threats that are addressed by LoA3 . Each threat \nis explained and accompanied by examples  in more detail within the JFAC FPGA Best \nPractices  Threat Catalog . \nTable 1: LoA3  threats  \n# Threat description (TD)  \nTD 1  Adversary utilizes a known FPGA platform vulnerability  \nTD 2  Adversary inserts malicious counterfeit  \nTD 3  Adversary compromises application design cycle  \nTD 4  Adversary compromises system assembly, keying, or provisioning  \nTD 5  Adversary compromises third -party soft intellectual property (IP)  \nTD 6  Adversary swaps configuration file on target  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   4 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \n# Threat description (TD)  \nTD 7  Adversary su bstitutes modified FPGA software design suite  \nTD 8  Adversary modifies FPGA platform family at design  \nTD 9  Adversary compromises single -board computing system (SBCS)  \nTD 10  Adversary modifies vendor FPGA software design suite during \ndevelopment  \nEach threat listed here has corresponding mitigations . These mitigations are derived \nfrom various commercial/government standards and existing best practices . The use of \nthese standards/best practices should not preclude the use of any other standards or \nbest pract ices. In particular, DoD projects identified as National Security Systems (NSS) \nshould also utilize the appropriate guidance as required by the Committee on National \nSecurity Systems (CNSS) Policy 15 and other CNSS documents.  \n1.1 Complementary standards and guidance  \nMicroelectronic  quantifiable assurance (MQA) standards are intended to be \ncomplementary to other government and industry recognized risk management \npractices and standards. The following are standards for various mitigations:  \n CNSS Policy on the use of Commercial Solutions to Protect National Security \nSystems Polic y 7 \n CNSS  Cryptographic Key Protection  Policy 30  \n National Institute of Standards and Technology ( NIST ) Federal Information \nProcessing Standards (FIPS)  Publication 186 Digital Signature Standa rd \n NIST FIPS Publication 198 The Keyed -Hash Mes sage Authentication Code \n(HMAC)  \n NIST Special Publication (SP) 800-53 Security and Privacy Controls for Federal \nInforma tion Systems and Organizations  \n NIST SP 800-57 Recommendation for Key Man agement  \n The Departm ent of Defense Cybersecurity Maturity Model Certification (CMMC)  \n The Configuration Management section of NIST  SP 800-60 Systems Security \nEngineering: Considerations for a Multidisciplinary Approach in the Engineering  \nof Trustworthy Secure Systems  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   5 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \n NIST SP 800-171 Protecting Controlled Unclassified Information in Nonfedera l \nSystems and Organizations  \n NIST SP 800-172 Enhanced Security Requirements for Protecting Controlled \nUnclassified Information.  \n SAE International AS6171 Test Methods Standard; General Require ments, \nSuspect/Counterfeit, Electrical, Electro nic and Electromechanical Parts  \n Trusted Sys tems and Network (TSN) Analysis  \n Defense Acquisition Guidebook Chapter  Nine  Program Protection Plan  \n JFAC FPGA Best Practices Documents  contact JFAC for available d ocuments \nto support implementation practices for t he FPGA standards in this guide  \n \nProgram offices should review and adhere to the standards provided in each document, \nas applicable. The standards and guidance contained in this best practice guide do not \nsupersede any other DoD acquisition requirement or other DoD mandate.  Additionally, \nprograms are encouraged to apply  applicable  standards in addition to the standards \ndescribed in this document  \n1.2 Exclusions  \nThis FPGA Level of Assurance 3 Best P ractice  guide  does not address the following \nconcerns:  \n Non-malicious and profit driven reliability risks such as re -marked parts. \nPrograms are responsible for establishing and enforcing system reliability \nrequirements. This document will not include guidance on how to c onduct \nreliability testing.  However, compliance with  SAE International AS 6171 Test \nMethods Standard: General Requirements  Suspec t/Counterfeit, Electrical, \nElectronic and Electromechanical Parts  as recommended by this report is an \neffective detection mechan ism for these kinds of counterfeit parts.  \n Threats to the confidentiality of the application design. The program application \ncan be loaded apart from the manufacturing process and under the protection \nand oversight of the program. Confidentiality is preserv ed using existing \nengineering practices, bitstream encryption and other anti -tamper practices. For \nmore guidance in this area, see the DoDs Anti -tamper Executive Agent  \n(https://at.dod.mil ). \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   6 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \n1.3 Document use  \nThese FPGA assura nce best practices instruct programs on protecting manufacturing \nand provisioning processes from adversarial influence. Specifically, they apply to the \nmanufacturing, acquisition, programming and first attachment of the FPGA devices. The \nprogram must defin e its own protection methods as boards become integrated into \nsubcomponents, components, and then final systems.  \nFor LoA3  compliance, each program should perform each mitigation listed in the  TD \nMitigation section.  The  Mitigations Description  section provides details for each \nmitigation.  Underlined text  in a listing  indicates that there is a following section providing \nfull details for implementing the protection. In some cases, the full description contains \nmultiple technical options for mitigating th e threat  to be LoA3  compliant.  An asterisk * \nnext to any mitigation indicates that multiple technical options exist . In those case s, at \nleast one option must be implemented.  \nWhen mitigations for all the threats listed under LoA3  are completed, that devic e can be \nsaid to have achieved LoA3 . However, compliance with LoA3  can be impacted by \nchanges in several areas during the systems life.  \nThe Program Protection Plan (PPP) emphasizes the need to maintain and update \nprotection measures throughout lifecycle of a program. It is strongly recommended that \neach program identify events that would trigger a review of the PPP and the hardware \nassurance practices after fielding. These events should include but not be limited to:  \n Changes to the system  \n Changes to the s upplier of critical components including the FPGA devices  \n Changes to the FPGA design software (new releases, fixes, etc.),  \n Changes to the threat environment  \n Revelations of new vuln erabilities to the FPGA devices  \nThe PPP documents list resources with which the program can track the latest available \nintelligence on threats  and supply chain vulnerabilities. Changes in any of these areas \nshould trigger a review of the most up -to-date assurance mitigations against the \ntriggering event. If threats or vulnerabilit ies threaten the system, new mitigations should \nbe implemented to remain compliant to LoA3 . Absent any changes in these areas, the \ndevices should be considered to have achieved LoA3 . \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   7 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \n1.4 General comments on mitigations  \n Programs are encouraged to own as muc h of the assembly  process as possible \nand avoid third parties to the fullest extent possible.  \n Programs are encouraged to diversify their supply sources to minimize malicious \ntargeting.  \n Programs are encouraged to use cleared personnel and classified resourc es to \nthe fullest extent possible.  \n Programs are encouraged to use verification of all manufacturing steps to the \nfullest extent possible.  This applies to packaging and assembly.  \n2 Threat descriptions (TD)  \nTD 1: Adversary utilizes  a known FPGA platform v ulnerability  \nIn this threat, an  adversary utilizes a vulnerability in an FPGA platform or vendor \ndevelopment software package to initiate an attack. A t LoA3 , a vulnerability is  defined \nas a weakness known to the adversary in the design of a specific FPGA plat form or \nsoftware program that would allow the ability to use it for malicious purposes.  The \nvulnerability could be public ly or non -publicly known.  \nVulnerabilities could allow for the leakage of sensitive information or keys , compromise \nof security or tampe r detection functions , or unauthorized reconfiguration of the product. \nUnclassified and public  vulnerabilities are published in databases , such as the DISA \nVulnerability Management System (VMS), Common Vulnerabilities and Exposures \n(CVE) , and the Nation al Vulnerabilities Database (NVD), vendor advisories, errata \nbulletins, etc. Non -public vulnerabilities refer to ones that have been discovered  by the \nadversary's research or known by vendors but not exposed  to the public .  \nThis threat can be realized whe n a program does not perform  vulnerability research  or \nan insider hides  the fact of the vulnerability such that it may be used for nefarious \npurposes or by adding/modifying design features for use with  or for triggering  the \nvulnerability.  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   8 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nTD 1 mitigations  \n Use caution when select ing tools or platforms . When possible do not select tools \nor platforms  that are end -of-life or beta/initial releases . Also, e nsure previously \nidentified vulnerabilities in tools/platforms have been adequately addressed in \nnewer rele ases.  \n Use cleared  personnel  that possess at least a  Secret  level clearance . \n *Research vulnerabilities  affecting  tools/platforms . \n Use revision control/versio n management  that includes document/data control, \ndocument/data release, backups,  and archives, refr esh of backup media, \nretention of tools and software, test equipment and test environment . \n Enforce auditability  of the req uirements, architecture, design, code, test s, bugs, \nand fixes. At a minimum, audit data should include what decisions were made, by \nwhom, for what reason, and on what date.  \n Adopt, document,  and enforce  the approved design process  that is \norganizationally approved and with clear entry and exit criteria. Entry and exit \ncriteria incorporate peer reviews and technical reviews with management  \napproval to exit a phase.  \nTD 1 m itigation descriptions  \nUse caution when s electing tools or platforms  \nConsider the longevity of selected tools and FPGA platforms. Newly released devices \nmay not y et have a vulnerability history.  Programs  should proceed with  caution  when \nusing newly released devices  or tools . End-of-life devices may not have support to \nmitigate vulnerabilities once identified.  \nUse cleared personnel  \nUse personnel with at least a Secret  clearance to perform designated work.  Designated \nwork coul d include design reviews, peer reviews, vulnerability research, validation,  and \nverification activities, etc. \nResearch vulnerabilities  \nResearch the respective FPGA platform and software for existing vulnerabilities in \ndatabases such as:  \n Common Vulnerabili ties and Exposures (CVE)  https://cve.mitre.org   \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   9 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \n NIST National Vulnerabilities Database (NVD)  https://nvd.nist.gov   \n Government Industry Data Exchange Program (GIDEP)  \nhttps://www.gidep.org/products/products.htm   \n DISA Security Technical Implementation Guides (STIGs)  \nhttps://public.cyber.mil/stigs/   \n Search vendor advisories , errata, publications , and academic papers detailing \nvulnerabilities in the device in question.  \n Contact the vendor field application engineer for unreleased or pre -release \nvulnerability reports.  \nIf vulnerabilities are found in the FPGA device, choose one  of the following options:  \nOption 1 : Select a different FPGA platform device or software that does not have \npublished vulnerabilities and that meet s the program requirements . \nOption 2:  Use standard formal processes and procedures to work with the vendor to \nresolve the vulnerability. Once a fix is identified, only accept formal releases, do not \naccept custom beta fixes, custom patches, etc. for incorporation ; or \nOption 3 : The program can internally determine the vulnerability poses no significant \nrisk to thei r product . JFAC is available to provide assistance in assess ing the risk that \nthe vulnerability poses to the system and acquire recommended mitigations for a \nparticular vulnerability.   \nNote: If a vulnerability is identified, JFAC recommends report ing it to DISA and to \ncontact the vendor so they may correct it . \nUse revision control /version management  \nTo prevent vulnerable software from being loaded into the environment, it is important \nthat robust configuration management and revision control  systems are in place. All \nchanges to the system and /or any artifacts should be documented, approved , and \nauditable.  \nThese systems should  fulfill the following requirements:  \n Allow only authorized system administrators to make changes to the underlying \nrevision control to ol and underlying server.  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   10 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \n Use a backup system that syncs to the primary and is maintained by a separate \nadministrator. Each system should be managed by separate system \nadministrators . \n Enforce administrative restrictions ; restrict privileged ac cess to autho rized \npersonnel only; limit what users can do to the database; ensure all users are \nverified; encrypt database information both in transit and at rest; enforce secure \npasswords; enforce  role-based access control and privileges; and remove \nunused accounts.  \n Remove any components or functions that are not necessary (for example, \nremove all sample files and default passwords).  \n Ensure the system provides a complete and immutable , long-term change history \nof every file. The system must log every change made by in dividuals. This \nincludes changes such as creating and deleting files and editing content.  The \nhistory must identify the person who made  the change, what was changed, the \ndate of the change, and the purpose of the change.  \n Ensure the system stores a reliabl e copy of assets that are currently in \nproduction.  \n Ensure the system stores reliable copies of previous production versions of \nassets, allowing for the complete retrieval of those versions.  \n Ensure password best practices (password rotation, length, etc.) a re enforced. In \nlieu of a password, two -factor authentication can be utilized.  \n Ensure the final application synthesis  and bitstream  generation  configuration \nsettings are captured and stored.  \n All changes to the system and /or any artifacts should be document ed, approved, \nand auditable.  \nEnforce auditability  \nEnforce auditability of the req uirements, architecture, design, code, test s, bugs, and \nfixes. At a minimum, audit data includes what decisions were made, by whom, for what \nreason, and on what date.  System a udits and logs are required where applicable.  \nEnforce the approved design process  \nThe design process  should include the identification of all assurance critical activities  \nand highlight how each activity will be revi ewed. The design process should ensure the \ndesign is reviewed by multiple cleared individuals . The original designer should not be \nthe responsible party for performing the review . The cleared reviewers should assess \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   11 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nthe satisfaction of all requirements, ensure no extraneous design  elements , and review  \nall vulnerability activities, including  identification of vulnerabilities and the \nappropriateness of the mitigations.  Additionally, the  design process should contain clear \nentry and exit criteria  that incorporate peer reviews and technical reviews with \nmanagement approval required to exit a phase.  \nTD 2: Adversary inserts malicious counterfeit  \nLoA1  address es counterfeit parts made in an unauthorized fabrication facility and \ninserted into the supply chain . These parts mimic the behavior of the target device , but \nare manufactured in a process differing from the authorized one . Insertion of counterfeit \nparts can happen during any part of the device's lifecycle. This includes prior to \npurchase, in transit, while in storage by the program, during assembly , and at \ndistribution prior to fielding . \nIn addition to  the LoA1 threats,  LoA2 addresses  counterfeit parts made in an authorized \nfabrication facility through the malicious compromise of the manufacturing process. \nSuch an attack could happen during  any of th e following phases of the process :  \n Transfer of graphic \ndesign system 2 \n(GDSII ) mask data  \n Mask fabrication  \n Mask storage   Wafer manufacturing  \n Wafer testing  \n Wafer dicing and \npackaging   Package testing  \n Device \npersonalization  \nLoA2 also includes c ounterfeit part s created in an adversary facility purposely built to \nmimic the authorized device  manufacturing  process , as well as  the insertion of a \nmalicious function into the package of an authentic device.  This includes : \n insertion of a snooping die stacked in the pac kage,  \n introduction of a kill switch in the package , or \n alteration of the bond out to compromise some FPGA feature . \nLoA3 adds counterfeit parts fabricated in the authorized fab rication facility  using stolen \nauthorized GDSII under a different product name . It can also include  the introduction of \na reliability and performance degradation due to an attack on the manufacturing process \nor the remarking of used devices.  The modification of the original design to insert a \nmalicious function could be considered a counterfeit  part, but will be  addressed \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   12 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nseparately in TD 8: Adversary modifies FPGA platform family at design . The difference \nbetween this new threat in TD 2 in LoA3  and TD 8 is that , for this threat, there exists a \ngolden, unalte red (known good) representation of the design in the GDSII.  For TD  8, the \nmalicious function is baked into the design and cannot be exposed by any comparison \nto a golden model .  \nThe mitigations at LoA3  for this threat rely heavily on the physical inspectio n of the \nparts. This reliance requires the differentiation between counterfeits from the authorized \nfabrication facility  and counterfeits from an unauthorized fab rication facility  as different \ntypes of malicious  counterfeits. Physical inspections are more intensive for detection of \na counterfeit device from an authorized fab rication facility . \nCommercial (non -malicious) counterfeits , such as re -marked parts , may represent a \nreliability risk. Programs with specific reliability  requirements should plan for the  \nappropriate level of testing to verify that their design and components meet those goals . \nThis document will not provide the details for performing reliability testing as they differ \nfor each program.  The program should perform sampled reliability testing  against the \nstandards claimed by the manufacturer  or those needed by the program.  \nAdditionally, at LoA3  there is the assumption of the existence of  one or more  \ncompromised cleared insider s in the program and the presumption of an adversary \nachieving diffi cult points of access . The insider (s) may be used by the adversary to \nintroduce malicious features during  any portion of the product manufacturing cycle \nand/or compromise a portion  of the FPGA device verification process. Compromised \ncleared insiders may b e used to introduce counterfeit  parts into the program supply \nchain or to compromise the programs acceptance testing. Overlapping  checks are \ntherefore necessary for each threat commensurate to this level of assurance . \nJFAC relies on substantial physical device inspection  to address these threats because \nthe program has no positive control over the fabrication facility or its processes . Most of \nthe FPGA fabricatio n facilities are foreign owned and not controllable by the program or \nDoD. JFAC can identify nu merous technically feasible attacks for all fabrication \ncountermeasures considered.  Overlapping personnel and multi -party review in the \nverification process  along with c ryptographically protected IDs  and reliability testing of \nsampled devices  provides ad ditional assurance protections . \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   13 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nGuidelines for conducting physical inspection are provided by the SAE AS6171 \ncounterfeit detection standard. These guidelines  are organized into slash sheets. Each \nslash sheet is a description of a singular type of inspect ion process. For the purposes of \nthis document, the slash sheets may be divided into several purposes:  \n Slash sheet s 2-10: describe physical inspections able to identify devices that \nwere manufactured in an unauthorized fab.  \n Slash sheet s 11: describes physi cal inspections able to identify maliciously \naltered devices that were manufactured in an authorized fab.  \n Slash sheets 3, 4, 6,  and 10: describe physical inspections intended to uncover \nmalicious alterations made to the package internals of an authentic de vice. \nMore details regarding the physical inspection process are outlined below in the \nmitigations.  \nTD 2 mitigations  \n Purchase  from DoD authorized  vendors and  distributors . \n Consult GIDEP  and follow their guidance on counterfeit risk mitigation, including \nguidance on known counterfeit parts. The program should use this information to \ninform their physical analysis efforts.  \n Follow storage and shipping guidance  when storing  and transferring FPGA \ndevices between locations .  \n Verify  the FPGA  cryptographically sec ure identifier  (ID) against information sent \nby the vendor (not the authorized distributor).  \n Using  the latest approved  version of AS6171  with associated slash sheets  \nperform physical inspection/analysis  on a sampling of random devices to detect \ncounterfei t parts . \n Mitigate risk of a cleared insider  involved in the physical inspection process . \nTD 2 mitigation descriptions  \nPurchase from DoD  authorized vendors and distributors  \nEnsure devices are purchased from vendors and distri butors authorized by DoD.  \nConsu lt GIDEP   \nGIDEP provides technical data comp iled by government and industry  regarding \ncounterfeit hardware devices to be used for system design, development, production, \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   14 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nand logistics support processes . This information contains counterfeit risk mitigations  \nand physical analysis results.  \nFollow s torage and  shipping  guidance  \nThe program should document, maintain and enforce both device storage and shipping \nprocedures.  Minimally the plan should enforce the verification of all devices  upon \nreceipt.  Once verific ation has taken place production devices should be stored and \nmaintained in a restricted area separate from non -production devices (design, test, \netc.). Production devices should be continuously tracked to include arrival of the device \nby unique identifier , interaction anyone has with the device, and exit of the device from \ninventory.  The restricted area should enforce access control that limits access to only a \nminimum subset of people that require access to support direct job responsibilities and \nexcludes  all members of the design team.  The restricted area should have a clearly \ndefined perimeter, but physical barriers are not required. Personnel within the area are \nresponsible for challenging all persons who may lack appropriate access authority.  The \nrestricted area access should be audited to include data containing who entered/exited \nthe area, with a timestamp and reason for entry.  \nShipping should be controlled and managed.  JFAC recommends shipping material \nusing a commercial carrier that has been approve d by the CSA to transport Secret  \nshipments, although the material is not Secret . Commercial carriers may be used only \nwithin and between the 48 contiguous States and the District of Columbia or wholly \nwithin Alaska, Hawaii, Puerto Rico, or a U.S. possessio n or trust territory.  When \nshipping using a commercial carrier take efforts to afford additional protection against \npilferage, theft, and compromise as follows.  This includes using hardened containers \nunless specifically authorized otherwise and ensuring t he packages are sealed.  The \nseals should be numbered and the numbers indicated on all copies of the bill of lading \n(BL). When seals are used, the BL shall be annotated substantially as follows: DO NOT \nBREAK SEALS EXCEPT IN CASE OF EMERGENCY OR UPON PRIOR A UTHORITY \nOF THE CONSIGNOR OR CONSIGNEE. IF FOUND BROKEN OR IF BROKEN FOR \nEMERGENCY REASONS, APPLY CARRIER'S SEALS AS SOON AS POSSIBLE AND \nIMMEDIATELY NOTIFY BOTH THE CONSIGNOR AND THE CONSIGNEE.  \nVerify the FPGA c ryptographic ally secure  identifier  \nFor LoA3 , the program should u tilize an FPGA device that incorporates a  \ncryptographically protected ID that can be verified against information sent by the  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   15 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nvendor (not the authorized distributor). The use and verification of this type of device ID \nmitigates the counterfeit parts made in an existing, non -authorized fabrication facility \nsub-threat.  \nWhile  the specifics of each FPGA vendor and platform vary, many newer FPGA \nplatforms contain this type of anti -counterfeiting feature. When these features are \nsufficiently  secure, such mechanisms provide an extremely cost -effective method to \ndetect counterfeits both at acquisition and throughout the FPGA devices lifecycle in a \nsystem. The biggest two advantages of such techniques are the ability to validate a \ndevice remote ly and the ability to non -destructively re -evaluate a device at any time.  \nBy contrast to physical anti -counterfeiting techniques, properly implemented  \ncryptographic ally secure  identifiers do not require destructive analysis for verification. A \ntypical sche me could validate such a device simply by placing it in a socket. A design \ncan facilitate access to the identifier through local access , such as a board header , or \nremotely. Depending on the exact mitigations selected, this potentially saves two \ndistinct d estructive steps: one at acquisition of the devices and one after assembly of \nthe PCB.  \nFor device families that do not offer a cryptographically secure  ID, a soft physically \nunclonable function  (PUF) should be used by the program for device authentication  \nthroughout the manufacturing and lifecycle of the device. In this case, a soft PUF is \nconfigured into each device to produce a unique identifier. This ID is then stored in \nassociation with the physical package serial number. The PUF is then removed. It ca n \nbe reconfigured into the device at any time to retrieve the ID to validate the authenticity \nof the device at any later date. The PUF should be added and the PUF ID recorded \nimmediately after validating the device lot as authentic. The step can then be re peated \nafter the component has been out of the control of the program to verify the devices. \nThe program should not proceed to manufacture or field LoA3  devices without one of \nthese ID services. Additionally, the PUF code must be protected as critical data  in a \nSecret  level repository with strong access controls. The PUF signature  should be \nverified before each use with a hash value.  \nThis kind of validation is where details matter. At the same time, each FPGA vendor \noffers a unique approach, and each FPGA p latform offers a unique variation. In no case \nis a fully readable ID acceptable. Instead, these schemes all detail cases where the \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   16 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \ndevice possesses a specific private cryptographic key. The device ID in this scheme \ncan be cloned only if an adversary is abl e to get access to that private key. Regardless \nof the specific platform used , the public keys/identifiers of the devices being \nauthenticated must be delivered and maintained in a secure way. For delivery, the \nvendor must provide this information to the pr ogram using a  CNSS or NIST approved \nauthentication algorithm to transmit the data. Ex amples would be an ECC -signed e mail \nwith a verified certificate  or an https -based file distribution system using a verified \ncertificate. Once received, the integrity of th at list must be maintained by storing it as \ncritical data in a Secret  level repository with strong access controls.   \nRemote attestation is an additional advantage enabled by a cryptographically secure  ID. \nWhile remote attestation cannot be used during acqu isition and assembly  due to  the \npotential introduction of additional vulnerabilities , it can be used throughout the  rest of \nthe lifecycle of the device. This provides the possibility of a future where devices and \ntheir configurations can be validated and m onitored remotely. Capabilities for remote \nattestation of hardware, firmware, and software are currently being developed in the \ncybersecurity space as enterprise management tools. While their use is not yet fully \nwidespread in hardware development, inclusi on of these features is a potential growth \narea for the lifecycle hardware assurance of FPGA devices.  \nRemote attestation is a powerful and valuable technique and JFAC can consult on \nappropriate remote attestation schemes, potentially based on these same me chanisms. \nHowever, the initial counterfeit screening must be done locally, v alidatin g each specific \ndevice .  \nThis section describes at the highest level the specific criteria that is required for an \nappropriate device ID to support anti -counterfeiting.  \n Cryptographically protected IDs must utilize  a CNSS Policy compliant private \nasymmetric key for which no read function exists.  If CNSS is not a program \nrequirement , the program should use a CNSS or NIST approved asymmetric \nauthentication algorithm . \n The prove nance of the key must be understood in detail.  \nThe device must be able to authenticate a nonce using this key. Each devices ID must \nbe aut henticated by the public vendor -provided key through decryption of the nonce.  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   17 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nPerform physical  inspection/ analysis  \nPerform phys ical analysis on a sampling of random devices to detect counterfeit parts. \nThis analysis applies specific, industry standard counterfeit inspection techniques, \nincluding package analysis, x -ray of the part, and examination of the die with \ncompari sons against FPGA vendor provided golden samples. This physical analysis is \nintended to catch parts that have been remarked or contain counterfeit die. The details \nof what steps to conduct in the analysis and recommendations on how to execute them \nare cont ained in the commercial standard document, SAE AS6171. To reduce \npersonnel threats, t hese inspections should be carried out by cleared person nel at a \nSecret  level or higher.  \nAt LoA2  and LoA3 , there are additional  attacks introduced under the counterfeit t hreat:  \n Insertion of a malicious function into the package of an authentic device  \n Counterfeit parts made in an authorized fabrication facility  \nIt is due to these new threats that physical inspection is required in all cases. In LoA1, \ncryptographically secure IDs were sufficient to address the counterfeit threat. However, \nthis would not be sufficient at LoA2 or LoA3 since these IDs would not preclude the \ninsertion of a malicious function into the package of a device nor identify devices where \nmalicious feat ures were added to the die during manufacturing.  \nPhysical analysis is a sequence of device analysis steps, from least destructive to most \ndestructive, designed to ensure that the part in question is authentic. If a device fails a \ngiven step, it is not auth entic and there is no need to complete further steps. If all steps \nare completed and the device passes, it is likely authentic, with likelihood \ncommensurate with the amount of effort it would take to get a counterfeit device to pass \nthese tests, and the fa ct that the device in question is subject to LoA3 . Each AS6171 \ntest is detailed in a separate document called a slash sheet. Listed below are the slash \nsheets that comprise the standard.  \nTable 2: List of AS6171 slash s heets \nTest N umber  Description  \nAS6171  Test Methods Standard; General Requirements, \nSuspect/Counterfeit, Electrical, Electronic, and Electromechanical \nParts  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   18 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nTest N umber  Description  \nAS6171/1  Suspect/Counterfeit Test Evaluation Method  \nAS6171/2  Techniques for Suspect/Counterfeit EEE Parts De tection by \nExternal Visual Inspection, Remarking and Resurfacing, and \nSurface Texture Analysis Test Methods  \nAS6171/3  Techniques for Suspect/Counterfeit EEE Parts Detection by X -ray \nFluorescence Test Methods  \nAS6171/4  Techniques for Suspect/Counterfeit E EE Parts Detection by \nDelid/Decapsulation Physical Analysis Test Methods  \nAS6171/5  Techniques for Suspect/Counterfeit EEE Parts Detection by \nRadiological Test Methods  \nAS6171/6  Techniques for Suspect/Counterfeit EEE Parts Detection by \nAcoustic Microscopy  (AM) Test Methods  \nAS6171/7  Techniques for Suspect/Counterfeit EEE Parts Detection by \nElectrical Test Methods  \nAS6171/8  Techniques for Suspect/Counterfeit EEE Parts Detection by \nRaman Spectroscopy Test Methods  \nAS6171/9  Techniques for Suspect/Counterfe it EEE Parts Detection by \nFourier Transform Infrared Spectroscopy (FTIR) Test Methods  \nAS6171/10  Techniques for Suspect/Counterfeit EEE Parts Detection by \nThermogravimetric Analysis (TGA) Test Methods  \nAS6171/11  Techniques for Suspect/Counterfeit EEE Par ts Detection by \nDesign Recovery Test Methods  \nFor the purposes of LoA3, the program should follow the lot sampling guidelines found \nin the latest version of AS6171 and exercise the test s defined by slash sheets 1 -11. \nSheets 1 -10 should uncover a counterfe it fabricated in an unauthorized fab rication \nfacility  or a malicious package insert. Sheet 11 should uncover a counterfeit fabricated \nin the authorized fab rication facility . \nHere are the physical analysis steps that should be taken:  \n If the device family po ssesses cryptographically protected IDs:  \n Perform slash sheets 2 and 3 that incorporate visual inspection and 3D  x-\nray. This effort focuses on analyzing the parts for a malicious  additive  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   19 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \ninsert ed inside the package. The number of parts sampled should be \nguided by the sampling standard found in slash sheet 1.  \n If the device family does not possess cryptographically protected IDs:  \n Perform slash sheets 2 -10 for the purposes of detecting an in -package \nmalicious insert and a die manufactured in an unauthorized facility. \n Perform the steps outlined below as they relate to slash sheet 11 to \nidentify malicious functions added to the die during manufacture in an \nauthorized fa cility. This test may be limited to a single device.  \nSheet 11 is a set of instructions for perf orming a full delayering, imaging of die , and \ncomparison against the vendor provided GDSII or an exemplar device . This analysis \nexposes the FPGA die manufactured layers for comparison against a golden  model \nmade up of either vendor provided images/GDSII or  an exemplar part. An exemplar part \nis one that is obtained directly from the vendor and not from an authorized distributo r. \nFor LoA3 , the program must  perform the following reverse engineering comparison on a \nsingle part:  \nFull chip delayering   imaging an d comparison of all layers to the exemplar when the \nstate of the art capability allows it. This is the ideal option for detecting malicious \nchanges.  In the case of FPGA multi -chip modules (MCM),  all the dies should be \nexamined using this technique. Special  care should be taken to validate the internal \npackaging connections .  \nFull backside delayering   imaging and comparison of la yers active, poly, contact , and \nmetal 1 ( M1). This is the ideal option for detecting malicious changes when a state -of-\nthe-art lab  capability is not sufficient.  In the case of FPGA multi -chip modules (MCM),  all \nthe dies should be examined using this technique. Special care should be taken to \nvalidate the internal packaging connections .  \nForward the results of the examination to JFAC with information regarding the FPGA \ntype and lot. The results should include a description of the verification method and the \ncoordinates of the windows opened for evaluation. JFAC will compile this information \nover time to develop better insight into mali cious attacks on the manufacturing process.  \nContact  JFAC for guidance  when process geometries are beyond the state -of-the-art \nin reverse engineering.  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   20 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nReliability testing should be conducted on sampled parts using the same sampling \nguidelines provided by SA E AS6171 . This testing should ensure that the devices meet \nthe reliability requirements of the program . Do not assume compliance based upon \ndevice datasheet information.  \nCleared insider  \nTo mitigate  the risk of a cleared insider compromising the physical analysis  process , \nprograms  should:  \n Select sample parts  bound for physical inspection in ways that  specifically defeat \ninsider compromise . \n Create cryptographically protected IDs  post verification . \n *Verify  independent lab work  using o verlapping p ersonnel and multi -party review . \n Follow the mitigation guidance in TD 4: Adversary compromises system \nassembly, keying, or provisioning .  \nSelect sampl e parts  \nThe selection of parts to be physically sampled must be handled in such a w ay that a \ncompromised cleared insider could not just select good parts to be sampled. Possible \noptions include the following:  \n Multiple independent parties handle part selection before shipping, and they \nshould physically verify that the parts selected make  it all the way to the physical \ninspection processes.  \n An independent party verifies sampling before shipping, and multiple parties \nverify upon receipt that the right parts were received.  \n Use a non -human random selection automated process for sampling.  \n All physical verification and sampling work should be conducted by personnel \nholding clearances of at least the Secret  level and carried out in facilities cleared \nto at least the Secret  level.  \nCreat e cryptographically protected IDs  post verification  \nFollowing physical verification above, JFAC recommends  using soft PUFs to protect  the \nauthentic parts from being swapped out for modified devices during the subsequent \nprogram manufacturing process :  \n Load the soft PUF into the fabric and generate a unique ID for the  FPGA die.  \n Record the device serial number and PUF ID.  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   21 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \n Erase the soft PUF.  \nAt any time during the lifecycle of the FPGA device, the soft PUF can be reloaded  and \nthe unique ID  can be  extracted and compared against the expected value for \nconfirmation of the authenticity of the part.  \nVerif y independent lab work  \nThere is a need to check the lab performing the physical verification for compromised \nresults.  If a compromised program insider is working with a compromised lab to pass \ncounterfeit parts off as good , the compromised lab could throw away all the devices \nsubmitted for examination and simply create reports and photos of an exemplary \ndevice, o r they could do all the work but falsify the reports.  \nThis threat is not completely mitigated with the following steps, but these steps  increase \nthe difficulty of returning false reports : \n Insist on the return of sampled  materials and detailed reports after evaluation. \nThis serves as a check that the lab did the work and serves as an additional \nmeans to verify that samp ling guidelines were followed.  \n Require lock and key storage of all parts to be physically inspected  and whether \nthat inspection is done by the program or by  independent lab(s).  \nAdditionally, choose one  of the following : \nOption 1 : Insert known bad parts int o the samples to be physically verified. Track which \nparts those are using custom bad data and/or markings. If the independent lab does not \nreport those parts as bad, then either they, or who they are reporting bad parts to, or \nboth, may be compromised.  \nOption 2:  Use two labs, use an independent expert observer , or both . This creates a \ncheck against the lab being compromised.  \nOption 3:  Perform any p hysical inspections done by the program, rather than a n \nindependent lab, with two -person authentication, or d uplicate  them independently, or \nboth.  \nTD 3: Adversary compromises application design cycle  \nIn this threat , a compromised  insider has access to the design process and data related \nto an FPGA application development effort . This insider can use their access to modify \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   22 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \ndesign code  or design constraints, change FPGA configuration settings , or swap in a \ndistinct configuration file that is authenticated and built with the same tools and keys \nbeing used by the design team. The actor  is in a particularly advantageou s position \nbecause they can modify the product during any phase of the design process. This \nsame threat surface may also be attacked via remote network intrusion. An attacker \nwith network access may also be able to modify important design data in a way tha t \nintroduces a Trojan or other nefarious function s. \nAt LoA3 , it is assumed that  multiple cleared and uncleared  individuals may be the \nadversarial actor.  The uncleared people can have different positions within the supply \nchain.  The actors could be working independently or with each other. In this threat the \ncompromised insider has access to the design process and data related to an FPGA \napplication development effort.  \nTD 3 is comprised of several specific scenarios.  These scenarios describe the entire \nthreat at TD 3 and each of the mitigations for each scenario should be implemented.  \nThe specific scenarios are as follows:  \n Introduction of a compromised design into the application ,  \n Modification of test benches  or plans to reduce coverage or hide Trojan code ,  \n Introduction of a Trojan into the application design during development , \n Introduction of compromised tooling  or software into the environment ,  \n Intrusion into the internal n etwork ,  \n Compromised employee , \n Modification  of the revision control system to hid e malicious  code or test bench \nmodification s (associated mitigations are captured in the in all cases section \nbelow) ,  \n Introduction of modified configuration data after generation (associated \nmitigations are captured in th e in all cases section below),  \n Compromise of device i dentifiers . \nTD 3 mitigations  \nThe best practices presented here  do not constitute a standalone FPGA design flow , \nbut rather should be integrated into the existing design procedures. These assurance  \npractices incorporate industry accep ted design best practices with emphasis on \ndocumented and approved design, review , and test procedures.  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   23 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nThe following set of mitigations apply to all TD3 scenario s, in addition to mitigations \nidentified in the individual scenario sections.  \nIn all cases m itigations  \n Use Secret  level cleared  personnel . If the program has higher level clearance \nrequirements , the program s requirement should be followed.  \n Track critical data in a revision control  system . \n Enforce auditability  of the req uirements, architecture, de sign, code, test s, bugs, \nand fixes.  \n Use revision  control/version management  that meets the requirements described \nlater in this section.  \nDescriptions  \nUse Secret level c leared personnel  \nUse personnel with at least a Secret  level clearance to perform desig nated work  \nTrack critical data in a revision control  system  \nThe program should identify and document all data that is considered critical.  Each \ncritical data item should be stored and tracked in the revision control system.  Minimally, \nthe following documen ts, data artifacts , and tool configurations should be managed  in \nthe revision control system:  \n Third -party IP (3PIP)  \n Utilized libraries  \n Development files, code, software used for development, synthesis scripts, and \ntools  \n Test benches, test plans, test proce dures, and test reports  \n Tool configuration settings  \n Design documents  \nEnforce auditability  \nEnforce auditability of the req uirements, architecture, design, code, test s, bugs, and \nfixes. At a minimum, audit data should include what decisions were made, by w hom, for \nwhat reason, and on what date.  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   24 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nUse revision  control /version management  \nRevision control /version management systems should meet the following requirements:  \n Allow only authorized system administrators to make changes to the underlying \nrevision cont rol tool and underlying server.  \n Implement a backup system that mimics the primary system and is maintained by \na separate administrator. Separate system administrators should manage each \nsystem.  \n Enforce administrative restrictions; restrict  privileged acc ess to authorized \npersonnel only;  limit what users can do to the database; ensure all users are \nverified; encrypt database information both in transit and at rest; enforce secure \npasswords; enforce role -based access control and privileges; and remove \nunuse d accounts.  \n Remove any components or functions that are not needed; for example, remove \nall sample files and default passwords.  \n Ensure the system provides a complete and immutable long -term change history \nof every file. The system must log every change m ade by individuals. This \nincludes creation and deletion of files and content edits. The history must include \nthe person who made the change, what was changed, the date, and written \nnotes on the purpose of each change.  \n Ensure the system stores a reliable c opy of assets that are currently in \nproduction.  \n Ensure the system stores reliable copies of previous production versions of \nassets, allowing for the complete retrieval of those versions.  \n Enforce password best practices (password rotation, length, etc.). In lieu of a \npassword, two -factor authentication can be used.  \nTD 3.1 Mitigating  the introduction of a compromised design  into the \napplication  \nIn this scenario , the adversary is able to insert a Trojan into the design after the design \nhas been verified, but  before the design is loaded for final deployment. Strict controls on \nthe revision control system will help prevent the adversary from making unmonitored \nchanges.   \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   25 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nTo accomplish this task the adversary would have to compromise t he revision control \nsystem.  That compromise could allow the adversary to change  the verified configuration \nfiles, settings, hash , or other pertinent information.  To protect against this , the program \nshould store and isolat e the verified  configuration files, settings , and associated h ashes. \nBefore the design is loaded for final deployment , the program should  verify the hash to \nensure that  the verified version is the same as what they are going  to deploy . For extra \nassurance , the program has all the necessary data to reproduce the build  and can  verify \nthe stored version against the reproduced version.  \nMitigations  \n Physically isolate and store the application design  until it is delivered.  \n Perform reproducible build  of the application .  \nDescriptions  \nIsolat e and store the application design  \nTo protect the application design after verification but before deployment, the final \nconfiguration file and hash should be physically isolated and stored until it is delivered \nfor provisioning . Ensure the file can only be accessed via authentication of tw o distinct \nparties. No single individual should be able to access the file. The limited set of people \nwith access should have to follow access control procedures such that access is  \ncontrolled, monitored,  logged , and auditable.   \nPerform reproducible build  \nUse a reproducible build process to verify the integrity of the FPGA synthesis and build \nsoftware. The r eproducible build performs the synthesis process that takes  in human \nreadable HDL, and other human readable inputs, and consistently generates the same \nfinal configuration file (bitstream).  It is expected that this process will, in most cases, \nrequire the use of the same version of the Electronic Design Automation  (EDA) tools, \nand in some cases the same operating system version. This process will highligh t the \npossession of modified software where there is a mismatch. Contact the FPGA software \nvendors  or JFAC  for more information on how to perform reproducible builds.  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   26 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nTD 3.2 Mitigating the m odification of test benches  or plans to reduce \ncoverage or hide Tr ojan code  \nIn this threat, the adversary makes changes to the test bench to hide malicious code, \nreduce coverage or reduce functionality.  \nMitigations  \n Create and execute a documented test plan  that identifies the various test \nreviews that will take place, an alysis to be performed, type of testing to be \nperformed, and the methods used to accomplish the test.  \n Validate and verify test processes  which include design/test team separation, \npeer reviews, and use of automated tools where applicable . \n Maintain  test en vironment via configuration management  as a critical system.  \nDescriptions  \nExecute a documented test plan  \nThe program should  consider assurance when creating and maintain ing the test plan.  \nThe test plan and processes should at least:  \n Provide a  mechanism t o verify all  the requirements  captured in the FPGA \napplication specification .  \n Explicitly list code coverage metrics, the type of testing that will be performed, \nand acceptable testing guidelines.  Code coverage should state how much code \nis checked by the test bench, providing information about dead code in the \ndesign and holes in the test suites. Document the  decision to use/not use other \ntypes of testing , such as directed test, constrained random stimulus, and \nassertion.  \n Ensure code coverage includes sta tement coverage, branch coverage, Finite \nState Machine (FSM), condition, expression, and toggle coverage. Document \nany code that will not be covered and why. Ensure untested code is documented \nand reviewed through the review process. Use functional test s to verify the FPGA \ndoes what it is supposed to do. Any deviations must be documented and \napproved.  \n Specify the verification environment which describes the tools, the software, and \nthe equipment needed to perform the reviews, analysis, and tests. Each of th ese \nitems should be maintained under revision control.  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   27 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \n Document and analyze unexpected  behavior and final implementation \nconclusions.  \n Ensure all test discrepancies, bugs, etc. , are resolved via a change process.  \nValidate  and verify  the test  processes  \nThe program should take care to ensure test processes consider assurance needs.  This \nincludes design/test team separation, peer reviews, and use of automated tools where \napplicable . All test discrepancies, bugs, etc. , should be res olved via a change process  \nutilizing  a change management system.  The established processes should be \ndocumented, enforced , and audited.  \nMaintain  test environment  via configuration management  \nThe test environment  should be treated as a critical system and maintain ed similarly to \nthe p roduction environment.  \nTD 3.3 Mitigating the introduction of a Trojan in to the application \ndesign during develop ment  \nIn this scenario, malicious functionality is introduced into the application design during \nthe development phase.  \nMitigations  \n Maintain bi-directional link s to approved requirements . Tracing to design \ndecisions is permitted in support of derived requirements.  \n Enforce peer review  best practices.  \n Create and execute a documented test plan .  \n Implement, validate,  and verify test processes  which in clude design/test team \nseparation, peer reviews, and use of automated tools where applicable . \n Select a formal proof process  that can validate the equivalency of the  HDL and \nthe final configuration file. For more information on proof tools, contact JFAC . \nDescriptions  \nMaintain b i-directional link s to approved requirements  \nAll requirements should be documented and traced.  Functionality that is not associated \nwith a requirement should not be allowed.   \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   28 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nEnforce peer review  \nEstablish and enforce peer review processes  with the following  characteristics :  \n The author and the reviewer must be different people.  \n Ensure the design process has time allocated for code reviews.  \n Code review should be done in parallel with development, reviewing small \nchunks at a time.  \n Anyone reviewing the code should already be familiar with the approved  \narchitecture.  \n All black box portions of the design must be identified, justified,  and approved.  \n All scripts that produce design artifacts (HDL, Netlist, etc.) must be reviewed and \napproved. Ensure there are no unexpected paths, filenames , or suppressed \noutputs.  \n Ensure the code reviews, at a minimum, verify:  \n The code does what it is intended to do.  \n The code can be traced to requirements.  \n The code is not needlessly complex.  \n Coding st andards are being utilized.  \n No extraneous code exists: the developer is not implementing unapproved \nitems that may have future utility.  \n The code has appropriate unit tests.  \n Tests are well designed.  \n The developer used clear names for everything.  \n Commen ts are clear and useful and mostly explain why instead of \nwhat.  \nExecute a documented test plan  \nThe program should consider assurance when creating and maintain ing the test plan.  \nThe test plan and processes should at least:  \n Provide a  mechanism to veri fy all requirements  captured in the FPGA application \nspecification .  \n Explicitly list code coverage metrics, the type of testing that will be performed, \nand acceptable testing guidelines.  Code coverage should state how much code \nis checked by the test bench , providing information about dead code in the \ndesign and holes in the test suites. Document the  decision to use/not use other \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   29 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \ntypes of testing such as directed test, constrained random stimulus, and \nassertion.  \n Specify the verification environment which d escribes the tools, the software, and \nthe equipment needed to perform the reviews, analysis, and tests. Each of these \nitems should be maintained under revision control.  \n Document and analyze unexpected  behavior and final implementation \nconclusions.  \n Ensure code coverage includes statement coverage, branch coverage, FSM, \ncondition, expression, and toggle coverage. Document any code that will not be \ncovered and why. Ensure untested code is documented and reviewed through \nthe review process. Use functional test s to verify the FPGA does what it is \nsupposed to do. Any deviations must be documented and approved.  \n Ensure all test discrepancies, bugs, etc. , are resolved via a change process.  \nImplement, validate,  and verify test processes  \nThe program should take care to ensure test processes consider assurance needs.  This \nincludes design/test team separation, peer reviews, and use of automated tools where \napplicable.  All test discrepancies, bugs, etc. , should be resolved via a change process \nutilizing  a change manageme nt system.  The established processes should be \ndocumented, enforced and audited  \nSelect a formal proof process  \nUse logical equivalency checking to the greatest extent possible.  Equivalency checking \nis used to prove the tools did not modify the logic or co nfiguration settings.  To do this \nthe final bitstream  is compared to the originating application HDL  to demonstrate they \nare logically equivalent with no extraneous logic in the final format. This approach \nconfirms Trojans were not inserted during the imple mentation steps.  This check also \nconfirms configuration settings ar e maintained and not altered. Configuration settings \nare those parameters included in the configuration file that affect the behavior of the \nFPGA device itself , but are not a part of the pr ogram application. Examples would \ninclude tamper settings, Joint Test Action  Group  (JTAG ) settings , and key storage.  \nThere are technical challenges associated with performing logical equivalency checking \n(LEC) on FPGA data . Contact JFAC for information on  emerging industry tools that can \nassist in identifying configuration data in the FPGA formats  or automate the creation of \nhints files.  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   30 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nTD 3.4 Mitigating the i ntroduction of compromised tooling  or software \ninto the e nvironment  \nIn this scenario, the adversar y introduces compromised tooling or software into the \nenvironment. This can be accomplished by an insider or through network intrusion.  \nMitigations  \n Validate cryptographic hashes  against hash es signed by the vendor.  \n *Research vulnerabilities  affecting  tools /platforms us ing commercial and JFAC \nprovided resources. If vulnerabilities are found, use an alternate or newer version \nthat does not have the vulnerability. Alternatively, perform a risk assessment and \ncoordinate findings with JFAC.  \n *Validate tools . \nValidate cryptographic hashes  \nAll parts of the software delivery should be authenticated by comparing the \ncryptographic hash of all received software against the hash signed by the vendor. This \nincludes  install macros and other support functions. Only accept  certificates validated \nby reputable third parties. Only accept publicly released software and document the \nsource of the hash signature and the hash itself.  \nResearch vulnerabilities  \nSoftware and tooling vulnerabilities can be exploited for nefarious purp oses. The \nprogram should actively monitor for vulnerabilities and perform risk assessment for any \nsoftware or tools selected.  Platforms and tool vulnerabilities can be found in databases \nsuch as:  \n Common Vulnerabilities and Exposures (CVE)  https://cve.mitre.org   \n National Vulnerabilities Database (NVD)  https://nvd.nist.gov   \n Government Industry Data Exchange Program (GIDEP)  \nhttps://www.gidep.org/products/products.htm   \n DISA Security Technical Implementation Guides (STIGs)  \nhttps://public.cyber.mil/stigs/   \n Searches for vendor advisories, publications and academic papers detail ing \nvulnerabilities in the device in question.  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   31 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \n Contact the vendor technical representative for unreleased or pre -release \nvulnerability reports.  \nIf vulnerabilities are found in the software of tools   \nIf vulnerabilities  are found in the software or tools , choose one  of the following options:  \nOption 1: Select a different  tool or software that does not have published vulnerabilities \nand meets the program requirements . \nOption 2: Use standard formal processes and procedures to work with the vendor to \nresolve the vulnerability.  Once a fix is identified, accept  only formal releases  and do not \naccept custom beta fixes, custom patches, etc. , for incorporation ; or \nOption 3 : Internally determine the vulnerability poses no significant risk to the pro gram .  \nNote:  If a vul nerability is identified, it is recommended to report it to the Government \nIndustry Data Exchange Program (GIDEP) and to contact the vendor so they may \ncorrect it . \nValidate tools  \nValidate  that the tool delivers the expected output by selecting from one of the options \nbelow:  \nOption 1: Select a formal proof process  that can validate the equivalency  of the HDL \nand final configuration file.  \nOption 2: Use a reproducible build process  to generate any  deployable configuration \nfiles, AND  acquire EDA tools from at  least two different distributors . \nUse a f ormal proof process  \nUse logical equivalency checking  (LEC)  to the greatest extent possible.  LEC is used to \nprove the tools did not modify the logic or configuration settings.  To do this , the final \nbitstream  is compared to the originating application HDL  to demonstrate they are \nlogically equivalent with no extraneous logic in the final format. This approach confirms \nTrojans were not inserted during the implementation steps.  This check also confirms \nconfiguration se ttings ar e maintained and not altered. Configuration settings are those \nparameters included in the configuration file that affect the behavior of the FPGA device \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   32 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nitself, but are not a part of the program application. Examples would include tamper \nsettings,  JTAG settings , and key storage.  \nThere are technical challenges associated with performing LEC on FPGA data . Contact \nJFAC for information on  emerging industry tools that can assist in identifying \nconfiguration data in the FPGA formats  or automate the creat ion of hints files.  \nUse a r eproducible build process  \nA reproducible build process is a methodology to verify the integrity of the FPGA \nsynthesis and build software. A reproducible build performs the synthesis process \ntaking in  human readable HDL, and other  human readable inputs, and consistently \ngenerates the same final configuration file (bitstream) .  \nAcquire EDA tools from at least two different distributors  \nAt LoA 3, reproducible builds should be performed using in dependently acquired \nsoftware and install ed independently on two distinct computers. It is expected that this \nprocess will, in most cases, require the use of the same version of the EDA tools, and in \nsome cases the same operating system version. This process will highlight the \npossession of modif ied software where there is a mismatch. Contact the FPGA software \nvendors for more information on how to perform reproducible builds.  \nTD 3.5 Mitigating  intrusion into the internal network  \nIn this scenario, an adversary gains access to the internal network.  With this access, the \nadversary can employ multiple  methods to achieve nefarious goals , such as making \nmodifications to tools, swap ping files, etc.  \nMitigations  \n Assign roles . \n Control and monitor access , including physical  and logical  restrictions.  \n Periodi cally research vulnerabilities  using commercial and JFAC provided \ninformation. If vulnerabilities are found, use an alternate or newer version that \ndoes not have the vulnerability. Alternatively, perform a risk assessment and \ncoordinate findings with JFAC.  \n Use a secret or classified network  to protect from remote attack .  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   33 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nDescriptions  \nAssign roles  \nEmployees should be assigned a specified  role with associated accesses and privileges \nbased on the role. At a minimum, these roles should include design, test, n etwork \nadministration and system administration. Roles should also be defined and \ndocumented with no overlap . For example, the test engineer should not be the same \nperson who wrote the requirements to be tested . Users should not have multiple roles . \nNote : In many real -world flows, designers and testers will require elevated privileges . \nSome of these elevated privileges may be shared with system administrators. Some \nmay have names (\"local admin,\" \"root,\" etc.) that imply system administration. For \nexample, a  member of the design team working on a software hardware interface may \nrequire local administrati ve privileges to install and debug their work. A member of the \ntest team for an FPGA -based device connected to an IP network might require the \nability to conf igure multiple network devices in the test environment, as well as to \nconnect a computer in promiscuous mode to that same test environment. Those \naccesses represent a part of the design or test role. However, these must be based on \nthe needs of the design or test process.  \nElevated privileges on computers should be granted  only as needed, and kept local to \nspecific computers. Elevated privileges should never include administrative access to \nrevision control servers, software installation, or other corporate infrastructure.  \nElevated privileges on networks should be limited to distinct test networks, properly \nisolated from the design environment and the corporate network.  \nControl and monitor access  \nEmployees should only have access to areas, equipment, data, a nd information \nnecessary to meet the requirements of their assigned job. Entry/access to appropriate \nareas should be recorded, monitored, and logged for auditability.  \nResearch vulnerabilities  \nSoftware and tooling vulnerabilities can be exploited for nefar ious purposes. The \nprogram should actively monitor for vulnerabilities and perform risk assessment for any \nsoftware or tools selected. Platforms and tool vulnerabilities can be found in databases, \nsuch as:  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   34 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \n Common Vulnerabilities and Exposures (CVE)  https://cve.mitre.org   \n National Vulnerabilities Database (NVD)  https://nvd.nist.gov   \n Government Industry Data Exchange Program (GIDEP)  \nhttps://www.gidep.org/products/products.htm  \n DISA Security Technical Implementation Guides (STIGs)  \nhttps://public.cyber.mil/stigs/  \n Searches for vendor advisories, publications, and academic papers detailing \nvulnerabilities in the device in qu estion.  \nUse a secret or classified network  \nPrograms should select a network classified at the Defense Security Cooperation \nAgency (DSCA ) Secret  level or above.  \nTD 3.6 Mitigating risk from a c ompromised employee  \nThis scenario  involves the compromise of an employee with access to the design, tools, \nor network being used for design or test.  \nMitigations  \n Enforce auditability  of the req uirements, architecture, design, code, test s, bugs, \nand fixes.  \n Enforce the approved design process .  \n Identify, document , and review critical design  activities . These items should be \nreviewed by a cleared individual that is different than the o riginal designer.  \n Use cleared personnel  in an environment certified to handle classified material at \nthe Secret  level or higher by DSCA. Th is also include s design centers certified \nfor Trust Category I by DMEA.  \nNote:  For this threat , independent is defined as \"not the originator .\" The reviewer can \nbe on the same team if necessary.  \nDescriptions  \nEnforce auditability  \nEnforce auditability of the  requirements, architecture, design, code, test s, bugs, and \nfixes. At a minimum, audit data includes what decisions were made, by whom, for what \nreason, and on what date.   \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   35 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nEnforce the  approved design process  \nThe design should include the identification of all assurance critical activities  and \nhighlight how each will be revie wed. The design process should ensure the design is \nreviewed by multiple cleared individuals . The original designer should not be the \nresponsible party for performing the review . The cle ared reviewers should assess the \nsatisfaction of all requirements, ensure no extraneous design, and assess  all \nvulnerability activities, including  identification of vulnerabilities and the appropriateness \nof the mitigations.  The design process should conta in clear entry and exit criteria. Entry \nand exit criteria  should  incorporate peer reviews and technical reviews with \nmanagement approval to exit a phase.  \nReview critical design activities  \nEnsure all critical activities are identified, documented, and the entire design is reviewed \nby multiple cleared individuals other than the original designer. Reviewers should \nassess all critical activities . Specific considerations include:   \n Design source files in conjunction with behavioral simulations   \n Design synthesis i n conjunction with functional verification   \n Design i mplementation in conjunction with static timing analysis   \n Bitstream generation with reproducible build results   \n Programming in conjunction with in -circuit verification  \nEnsure that the review teams do not include the original designers and each reviewer \nshould hold a U .S. Secret  security clearance.  \nUse cleared personnel  \nUse personnel with at least a Secret  level clearance to perform designated work . \nTD 3.7 Mitigating risk associated with the c ompromise of device \nidentifiers  \nIt is imperative to protect the device IDs , ensuring adversaries are not able to utilize this \ninformation to track  devices , swap counterfeits into the stores , or manipulate device \ncontrols.  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   36 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nMitigations  \n Store device identifiers  in a prot ected area utilizing access control . This should \ninclude physical or logical separation , and could be a safe, a classified network, \nor a sensitive compartmented information facility ( SCIF ). \n Limit access to device identifier information  to those that need i t for completion of \njob responsibilities.  \nDescriptions  \nStore device identifiers  \nStore device s in a protected area utilizing access control . This should include physical \nor logical separation , and could be a safe, a classified network, or a SCIF.  \nLimit acce ss to device identifier information  \nLimit device identifier information  to those that need it for completion of job \nresponsibilities.  \nTD 4: Adversary compromises system assembly , keying , or \nprovisioning  \nIn this threat , an adversary has carried out an attac k on the system during printed circuit \nboard ( PCB) assembly, key injection , or flash provisioning. This attack could include the \nassembly house acquiring counterfeit parts on behalf of the end customer, swapping out  \nauthentic  FPGA parts for counterfeit one s, stealing  or compromising configuration data, \nor stealing or modifying keys. Multiple par ties can be involved during the  system \nassembly phase. The following areas of the supply chain are included in this threat:  \n Shipping devices to the PCB assembly faci lity. \n Transmitting keys, configuration data and FPGA part numbers to the assembly \nfacility . \n Injecting keys into the FPGA devices . \n Provisioning the configuration storage devices . \n Attaching the FPGA devices to the PCB . \n Testing PCBs. \n Shipping the PCBs to the next manufacturing stage.  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   37 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nOf particular concern in this attack is the assumed existence of one or more  cleared \ninsider s working maliciously in some portion of this manufacturing process. At LoA3 , \nthis insider could be working alone or in partnership with a n external party to influence \nthe outcome . Additionally, in LoA3 , attacks can also result in reliability or performance \ndegradation. The following mitigations are built to address these  premises:  \n All assembly work requires after -the-fact validation  by the program validation team  \nin a cleared facility.  \n The assembly work should be conducted in a facility minimally classified as \nSecret . The post -fab validation should be done by a verification team  with S ecret \nclearance and  independent of those who conducted th e assembly work. The \nduplication is necessary as cleared insiders working in conjunction can \ncompromise the device  and the validation process . The use of multiple cleared \nteams helps to reduce the risk of that scenario.  \nIt is recommended  that all mitigatio n steps be performed in a classified facility.  \nTD 4 mitigations  \nRegardless  of where the work is performed, the program should  implement the following \nlist of mitigations in the assembly, keying , and provisioning process:  \n Purchase from DoD authorized vendo rs and  distributors . The DoD program \nacquisition group can provide this information.  \n Follow storage and shipping  guidance  when storing or transferring FPGA devices \nbetween locations .  \n Provide keys  and configuration data  to the provisioning house in digital ly signed \npackages and with hashes . \n Prior to provisioning, clear memory devices  that store configuration data . \n Provision private keys  into the FPGA devices in a DSCA Classified Secret  or \nTrust Category I certified facility after the assembly process.  \n Prote ct the configuration data package  by sending  it separately to the  assembly \nhouse and the validation team .  \n Following assembl y and provisioning, perform verification activities  in a DSCA \nClassified Secret  or Trust Category I certified facility.  \n *Authenticat e the FPGA device  after being ou t of the control of the program . \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   38 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nTD 4 m itigation descriptions  \nPurchase from  DoD authorized vendors and  distributors  \nUse DoD authorized vendors for all purchases.  Authorized vendors can be located \nthrough the acquisition orga nization . \nFollow s torage and s hipping  guidance  \nAll devices  should be verified upon receipt.  Once verification has taken place , \nproduction devices should be stored and maintained in a restricted area separate from \nnon-produ ction devices (design, test, etc. ). Production devices should be continuously \ntracked to include arrival of the device by unique identifier, interaction anyone has with \nthe device, and exit of the device from inventory.  The restricted area should enforce \naccess control that limits access t o only a minimum subset of people that require \naccess to support direct job responsibilities and excludes all members of the design \nteam.  The restricted area should have a clearly defined perimeter, but physical barriers \nare not required. Personnel within the area are responsible for challenging all persons \nwho may lack appropriate access authority.  The restricted area access should be \naudited to include data containing who entered/exited the area  with a timestamp and \nreason for entry.  \nShipping  should be co ntrolled and managed.  JFAC recommends shipping material \nusing a commercial carrier that has been approved by the CSA to transport Secret  \nshipments, although the material is not Secret . Commercial carriers may be used only \nwithin and between the 48 contiguo us States and the District of Columbia or wholly \nwithin Alaska, Hawaii, Puerto Rico, or a U.S. possession or trust territory.  When \nshipping using a commercial carrier take efforts to afford additional protection against \npilferage, theft, and compromise as follows.  This includes using hardened containers \nunless specifically authorized otherwise and ensuring the packages are sealed.  The \nseals should be numbered and the numbers indicated on all copies of the bill of lading \n(BL). When seals are used, the BL sha ll be annotated substantially as follows: DO NOT \nBREAK SEALS EXCEPT IN CASE OF EMERGENCY OR UPON PRIOR AUTHORITY \nOF THE CONSIGNOR OR CONSIGNEE. IF FOUND BROKEN OR IF BROKEN FOR \nEMERGENCY REASONS, APPLY CARRIER'S SEALS AS SOON AS POSSIBLE AND \nIMMEDIATELY NO TIFY BOTH THE CONSIGNOR AND THE CONSIGNEE.  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   39 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nProvide k eys and configuration data  \nProvide keys and configuration data to the provisioning house in digitally signed \npackages and with hashes. JFAC recommends that these data packages be encrypted \nusing the Advan ced Encryption Standard ( AES) algorithm with a key of at least 256 -bit \nlength. The assembly house should utilize the  signature and hash  to verify the integrity \nof the contents . \nClear memory devices  \nPrior to provisioning, clear memory devices  that store con figuration data . This prevent s \nan adversary from storing malicious configuration data in non -used areas of the memory \ndevice . These memory devices could include a discrete PCB component like a Flash or \nthe on -chip FPGA non -volatile storage available on cer tain devices.  \nProvision private keys  \nProvision private keys  into the FPGA devices in a DSCA Classified Secret  or Trust \nCategory I certified facility after the assembly process.  \nProtect the c onfiguration data package  \nThe program should ensure there are proc esses and procedures in place to ensure that \nthe configuration data package is provided to the assembly house and the validation \nteam  in a manner  that cannot be corrupted by a single individual . The data should be \nprovided directly and independently to eac h destination . The assembly house should \nnot be used to pass the data to the test facility . Ensure there is a golden copy provided \nto each functional area ensuring the same data is transmitted.  \nPerform v erification activities  \nFollowing assembly and provisi oning, perform all  verification activities  in a DSCA \nClassified Secret  or Trust Category I certified facility.   \nAt LoA3 , there can be multiple compromised cleared insiders.  To mitigate this threat , a \nteam of people cleare d at the S ecret level and independe nt from the assembly and \nprovisioning team should be utilized to conduct the validation.  \nThose performing this validation must:  \n Verify  the PCB traces related to the FPGA device, the configuration memory \ndevices, and any other devices related to the authent ication of the configuration \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   40 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \ndata. If needed, t he program should rely on guidance from the JFAC PCB \nExecutive Agent to perform this verification.  \n Verify  the authenticity of the configuration data loaded on the FPGA memory \ndevice following provisioning and assembly. The verification can be executed by \na bit comparison or a hash. This verification must be performed by a team \nindependent of the assembly and provisioning process. The verification should \ncover the entire contents of the memory device and not jus t the addresses \ncontaining the configuration data . It is recommended to program the entire \nmemory space to disallow unused memory for nefarious purposes .  \n Verify  that the FPGA system can cryptographic ally authenticate all loaded \nconfiguration data as part of the system containing the FPGA  upon load . The \nauthentication methodology should verify both the source and contents.  \n Verify  that the proper post assembly keys have been loaded into the FPGA key \nstorage elements. This verification must be performed by a team independent of \nthe assembly and provisioning process. Some FPGA devices allow a hash of the \nkeys to be read out for confirmat ion. Additionally, the program should create test \nbitstreams  to verify that the devices can properly utilize the keys and can reject \nactions u sing wrong keys.  \n Verify  the authenticity of the FPGA d evice to rule out the introduction of a \ncounterfeit part during assembly.  \nAuthenticate the FPGA device  \nWhen the FPGA has been  out of positive control of the program it must be \nauthentica ted. The program should select one of the options below:  \nOption 1: Verify the device on the PCB is an authentic and authorized device by \nvalidating that each device has a unique cryptographic ID signed by the vendor. Each \ndevice must contain a unique priva te asymmetric key for which no read function exists, \nand validation must involve the device signing a nonce. A NIST approved asymmetric \nauthentication algorithm  must be used for this. The program should authenticate the \nFPGA devices utilizing this ID when they have been out of the positive control of the \nprogram.  \nOption 2: Verify the device on the PCB is an authentic and authorized device by \nperforming physical counterfeit inspection with destructive sampling  as described under \nPerform physical  inspection/ analysis . This is primarily an SAE International AS6171 \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   41 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nTest Methods Standard; General Requirements, Suspect/Counterfeit, Electrical, \nElectro nic and Electromechanical Parts based evaluation, with requirements to ob tain \nvendor information.  \nOption 3: Use a soft PUF. Verify the device on the PCB is an authentic and authorized \ndevice by utilizing a soft PUF to create unique IDs. The soft PUF is used to validate the \nintegrity of the devices when they are outside of the program's control. The program \nshould generate these IDs when FPGAs are in their control by loading the soft PUF into \nthe FPGA fabric, use it to generate a unique ID for the respective device , and then \ndelete the PUF. Following assembly, the program should  repeat this process and \nensure the ID matches, authenticating the device.  If the soft PUF will be used to \nauthenticate the device when it is outside the program control, it is recommended that \nthe following be done:  \n Prevent readout of the PUF output to th e FPGA s external pins . \n Utilize the PUF to encrypt a nonce that can transmit outside the device.  \n Utilize  a public key based on the PUF value to decrypt the nonce and \nauthenticate the device.  \nThis approach can be used to support remote attestation when nee ded. \nTD 5: Adversary compromises third-party soft IP  \nIn this threat,  an adversary compromises third -party soft IP intended for integration into \nthe configuration of the FPGA. The compromise can occur during the IPs development \ncycle, during its delivery , or while it is at rest at the programs design center. In all \nscenarios, the compromised IP contains a malicious function that was inserted during its \ndesign and can be triggered through some input to the FPGA, or when a specific \nscenario occurs. In all ca ses, it is important to remember the purpose of the Trojan is \nunknown, but probable impacts include  functional change,  performance, power , or \nreliability. The mitigations to these attacks focus on verifying integrity of the delivery of \nthe IP and reviews o f its HDL code.  \nSee Appen dix B: IP Reuse Guidance  for information describing parameters for reusing \ninternally created or previously evaluated IP.  \nTD 5  mitigations  \n Purchase from DoD authorized vendors and  distribu tors. \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   42 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \n Only a ccept IP that is  unobfuscated  and distributed as source code.   \n Ensure IP deliverable packages are digitally signed . \n Validate the  cryptographic hash  of the IP  against the hash signed by the vendor.  \n Store IP in a revision control  repository  imme diately upon receipt with the hashes \nused to authenticate the contents. Protection of the hash will allow for re -\nverification of the IP at a later date.  \n *Examine IP for malicious functions . \nTD 5 mitigation  descriptions  \nPurchase from DoD authorized vendors and distributors  \nUse DoD authorized vendors and distributors for all purchases.  Authorized vendors can \nbe identified  through the acquisition organization . \nOnly accept  IP that is unobfuscated   \nOnly accept IP that is unencrypted , unobfuscated, and distribute d as source  code.  IP \nmust be human readable for review.   \nEnsure IP deliverable packages are digitally signed  \nThe program should only accept digital signature certificates validated by reputable third \nparties. The program should be limited to publicly relea sed software  and not special or \ncustom distributions of the software . The program should maintain documentation of th e \nvendor provided signature and hash, and the actual software hash.  \nValidate the c ryptographic hash  \nEnsure that the cryptographic hash of t he IP  is validated against the hash signed by the \nvendor.  All parts of the software delivery should be authenticated in this manner \nincluding install macros and other support functions. The program should only accept \ncertificates validated by reputable t hird parties. The program should be limited to \npublicly released software. The program should maintain documentation of the source \nof the hash and the actual software hash.  \nStore IP in a revision control  repository  \nImmediately upon receipt , the IP with it s associated hash should be checked into  a \nversion control  repository . The hash of the IP should be verified at various stages to \nensure there have been no modifications . The hash should be stored separately from \nthe IP block and be made read -only to the d evelopment team.  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   43 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nExamine IP for malicious functions  \nTo examine  the IP for malicious functions,  choose one of the following options : \nOption 1: Have two cleared personnel review the IP, according to the JFAC guidance in \nThird -Party IP Review for Level of Ass urance 3. JFAC can provide this document upon \nrequest.  \nOption 2: Contact JFAC to determine if an IP review of the complete IP package has \nbeen previously  completed.  If JFAC has not performed an IP review , option 1 must be \nselected.  \nTD 6: Adversary swaps configuration file on target  \nIn this threat, an adversary  obtains access to the system during or after assembly and \ncan compromise the FPGA devices operation via a modification to the configuration \ndata.  \nFor assurance  purposes, these guidelines are not con cerned with the exposure of the \nconfiguration data or the confidentiality of the public keys, as they do not compromise \nthe authentication of the data. However, programs with security requirements may need \nto protect this information and can choose to impl ement additional protections.  \nTechnological mitigations exist publicly for this threat such as configuration data \nauthentication. Mitigations must involve authenticating the configuration file for both \nintegrity and provenance.  JFAC encourages programs to use device families that \nsupport configuration data authentication .  \nPrograms are discouraged from using devices that do not support configuration data \nauthentication . In this scenario, authentication practices apply to all configuration file \nloads , includ ing local  loads , remote updates, multi -boot scenarios, configuration  via \nsoftware , and configuration via protocol where  the configuration file is loaded into the \nFPGA. For devices that store th e data internally in non -volatile m emory (NVM), this \nrequiremen t only applies to the initial loading.  \nAs of October  2022 , all the major U.S. FPGA vendors provide built -in functionality to \nauthenticate configurat ion files either at load into  internal memory or at configuration  for \nat least one device family . The speci fics of this authentication vary great ly. The exact \ndetails of key management and storage vary from device to device. Some offer facilities \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   44 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nto store many authentication keys, some use fuses, and others use independently \npowered random access m emory (RAM). Further, there are public technique s to subvert \nthe authentication , which have complex implications for the security of built -in \nauthentication1. \nThe result is that the exact security of each method is not apparent without a detailed \nevaluation. This repor t communicates  the specific mechanisms that meet JFAC \nexpectations, as well as caveats for their use.  As a rule, the program must use CNSS or \nNIST approved asymmetric cryptographic algorithms at LoA3 . \nTo achieve LoA3 , all boot/configuration images must be authenticated with respect to \ntheir source and data integrity. That is, the device must validate that the file comes from \nan authorized provider and that the data has not been modified prior to loading. For \nLoA3 , the recommended method for authenticating t he data source is to use an \nasymmetric algorithm recommended by  CNSS or  NIST.  Asymmetric algorithms are \npreferred because they do not require the protection of a secret  key. For data integrity, \na hashing algorithm , such as secure hashing algorithm  (SHA) , is recommended. Many \nof the existing FPGA devices provide these functions for the user.  \nTD 6  mitigations  \nThese are the configuration file threat m itigations : \n Incorporate cryptographic  authentication  of all loaded configuration data as part \nof the system co ntaining the FPGA.  \n Design the system to authenticate configuration data each time the data is \nloaded  into the FPGA device.  \n Configure all production devices in a way that prevents direct read back  of the \nprivate keys through electrical means.  \n Use a  CNSS /NIST approved algorithm and key length . \n Use DoD evaluated authentication mechanisms . \n Disable test access pins  in fielded products.  \n *When the program utilizes  mechanisms that allow application updates , ensure \nauthentication for modifications  is supported  \n                                                \n1 The Unpatchable Silicon: A Full Break of the Bitstream Encryption of Xilinx 7 -Series FPGAs. Usenix Security 20. Maik Ender, Amir Moradi, Christof Paar.  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   45 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \n Generate and store all a uthentication keys on a program controlled, FIPS 140 -2 \ncompliant, Level 2 hardware security module ( HSM ) \nTD 6 m itigation descriptions  \nIncorporate  cryptographic authentication  \nThe program should  enforce cryptographic authentication  of the configuration file . In \naddition, the program should  maintain documentation including the authentication \nmethodology, its architecture , and its compliance with appropriate CNNS Policy if the \nproject is identified as a National Security System.  Otherwise , ensure compliance with \nappropriate NIST standards . \nAuthenticate configuration data each time the data is loaded  \nDesign the system to authenticate configuration data each time the data is loaded  into \nthe FPGA device.  \nPrevent direct read back  \nConfigure all  production devices in a way that prevents direct read back  of the private \nkeys through electrical means.  \nUse a CNSS /NIST approved algorithm and key length  \nIf the project is identified as an NSS, u se a CNSS Policy approved algorithm and key \nlength . Otherw ise use a NIST approved algorithm and key length , as described in the \nlatest approved version of FIPS 186 , Digital Signature Standard , or FIPS 198 , The \nKeyed -Hash Message Authentication Code (HMAC) . \nUse DoD evaluated authentication  mechanism s \nThe program c an either select an authentication mechanism with an existing evaluation \nor sponsor the evaluation itself. JFAC can perform evaluations and maintains best \npractices in using commercial technology for this purpose.  \nAt a minimum, any evaluation must:  \n Ensure  compliance with the current version of FIPS 186 , Digital Signature  \nStandard.  \n Authenticate all boot configuration data.  \n Confirm its ability to verify data integrity  using positive and negative testing . \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   46 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \n Confirm its ability to verify the authorized source  using positive and negative \ntesting .  \n Ensure authentication is applied to all configuration data regardless of how it is \nstored or delivered prior to or in parallel to configuration.  \n Verify the authentication mechanisms do not contain any known vulnerabilit ies. \n All keys must be generated and protected in accordance with FIPS 140 -2 Level \n22.  \n The use and operation of application test access is disabled in fielded products.  \nDisable te st access pins  \nAll modern FPGA family devices have hardware test interfaces t o support fabrication \ntesting of the device and testing of the user product.  These interfaces usually include \nJoint Test Action Group ( JTAG ) pins and dedicated test pins.  \nJTAG pins should be d isabled in fielded products. It is a common practice to disable \nthese access points prior to fielding the device. JFAC recommends  disabling this in non -\nvolatile fuses when available.  \nEnsure authentication for modifications  \nMany FPGA platforms contain mechanisms that allow the application to change  or \nupdate  itself. So me allow for true in -flight reprogramming, where some portion of the \nFPGA continues normal operation while another portion changes its behavior. Others \nallow for reprogramming via external storage.  Ensure that the built -in application change \ntechnique appl ies authentication to all the reconfiguration data.  \nThe names of these operations are system specific and include terms like dynamic \nreconfiguration,  partial reconfiguration,  in-application programming,  etc. In practice, \nmost FPGA device families  do not provide the same degree of authentication that the \nprimary programming mechanisms provide.  \nAuthenticating reconfiguration data in the application  itself  \nIn this case, the program  incorporates functions in the  application to perform \nauthentication on c onfiguration data when the FPGA device cannot. When utilizing this \noption, the program should pay attention to the following considerations.  \n                                                \n2 FIPS 140 -2 will be replaced at  a future date with FIPS 140 -3. \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   47 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nSystem -on-chip FPGAs (SoC FPGAs) incorporate central processing units  (CPU s) as a \ncomponent of a reconfigurable pla tform. The JFAC FPGA Best Practices do not seek to \nprovide software assurance to the application running in the CPUs of a SoC FPGA. \nHowever, the best practices listed here will provide the same degree of assurance to \nthe initial user code (sometimes called  a bootloader) executed by the CPU.  \nFrom there , it is possible for a designer to extend the same authenticity to the user code \nif their system requires it. In cases where the program uses an interface between the \nFPGA fabric and the SoC in order to have o ne function load the other, it is vital that no \npath exists from this interface to the input/output (I/O). It is up to the program to ensure \nthat only the application has access to it.  \nIn some platforms, security settings can be programmed into both non -volatile storage \nin the device itself and as a setting in the configuration file loaded into the device. \nSetting s should always be programmed in the non -volatile storage of the device. In \nthose cases where use of security settings within the configuration f ile is acceptable, it \nmust  be explicitly noted.  \nSome platforms provide support for remotely updating the boot or configuration data on \nthe FPGA device. This update is sent via a network, stored local ly on  the FPGA device, \nand then loaded into the device by  the application.  \nAn application designer using these operations should implement one of the following \ntwo options:  \nOption  1: Validate that the built -in application change technique being used  fully applies \nauthentication to all the reconfiguration data.  \nOption 2:  Perform authentication of th e reconfiguration data in the application itself. \nMany platforms support the ability to load different boot or configuration files from a \nlocal memory. This methodology involves the current application instructing the device \nto point to a new memory location for the boot/configuration information. In these cases, \nthe device maintains a pointer to the original data if there  is a load error with new file. It \nis necessary to ensure that all boot/configurations can be authe nticated with respect to \nits source and data integrity in the same manner as the base load. Many devices leave \nthis t ask to the application to perform.  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   48 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nUse a FIPS 140 -2 compliant, Level 2 HSM  \nGenerate and store all authentication keys on a program controll ed, FIPS 140 -2 \ncompl iant, Level 2 HSM with the HSM c onfigured to enforce role -based restrictions on \nthe use of the keys. Maintain an approved list of individuals who can access the keys.  \nIt is worth noting that  there are additional protections that can be applied to the FPGA \nconfiguration data when its fielded location is physically unguarded. These include : \n Configuration file encryption using a NIST or DoD approved algorithm.  \n The use of split dec ryption keys to make key theft more difficult. This involves \nstoring multiple keys throughout the system, concatenating them , and then using \nthe hash of the concatenation as the decryption key.  \n The use of PUFs for key generation or a combination of PUF output and stored \nkey. \n Utilize any additional key protection mec hanisms provided by the vendors.  \n Utilize good physical access protections for the PCB.  \nTD 7: Adversary substitutes modified FPGA software design \nsuite \nIn this threat , an adversary replaces the design suite an  application designer uses  with \none modified to subvert the application during synthesis, place and route , or \nconfiguration data generation. In this threat , the adversary would have access to a \nmodified version of commercial vendor software and would use the modified software \nto: \n Subvert the security fe atures of an FPGA during configuration data generation.  \n Insert a malicious function into the device during synthesis, place and route or \nconfiguration data generation.  \n Insert a data leak or backdoor into the synthesized device during synthesis , place \nand r oute, or configuration data generation.  \nThis subverted tool would then be entered into the programs design environment by a \nvendor insider, an adversary -in-the-middle  technique,  or through a network intrusion. \nThis threat does not  include the scenario whe re an FPGA vendor insider modifies the \nauthorized software  during development  for maliciou s purpose s, which  is covered by  TD \n10: Adversary modifies vendor FPGA software design suite during development .  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   49 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nTD 7  mitiga tions  \n Purchase from DoD authorized vendors and  distributors . Both DoD and vendors \nhave recommendations for the a ppropriate distributors of  products.  The DoD \nprogram acquisition group can provide this information.  \n Prevent automatic  tool updates  by using a n installation  and update process  that \ndoes not require I nternet connectivity.  \n Install and execute software using a trusted  comp uting environment  to protect \nfrom remote intrusions . \n Use cleared  personnel  with at least a Secret  level clearance . \n Validate  the cr yptographic  hash  of the software against  the hash signed by the \nvendor.  \n *Validate the tool output  has not modified the source design . \nTD 7 mitigation d escriptions  \nPurchase fr om DoD authorized vendors and  distributors  \nUse DoD authorized vendors for all purc hases.  Authorized vendors can be  identified  \nthrough the acquisition organization.  \nPrevent a utomatic tool updates  \nPrevent automatic tool updates  by using an installation and update process that do es \nnot require Internet connectivity.  \nUse a trusted computing  environment  \nPrograms should select one of the trusted computing environment  options below, to \nprotect from remote attack.   \nOption 1: A computer and network classified at the DSCA Secret  level or above.  \nOption 2:  A computer and network certified for use i n a Trust Category 1 facility as \ndefined by DMEA.  \nOption 3 : A network -isolated computer enclave with limited and controlled access  \nadhering to NIST and CMMC standards . This is a computer with the vendor software \ninstalled by a network administrator. This administrator should not be a designer \nworking on the application design.   \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   50 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nUse cleared personnel  \nUse personnel with at least a Secret  level clearance to perform designated work.  \nValidate the c ryptographic hash  \nEnsure the cryptographic hash  of the software deliverables  is validated against the hash \nsigned by the vendor. All parts of the software delivery should be authenticated in this \nmanner including install macros and other support functions. The program should only \naccept digital signature certificates  validated by reputable third parties. The program \nshould be limited to publicly released software  and not special or custom distributions of \nthe software . The program should maintain documentation of th e vendor provided \nsignature and hash, and the actual software hash.  \nValidate the tool output  \nValidate the  tool has not inserted any Trojan by choosing one of the following options : \nOption 1: Perform logical equivalency checking between the application HDL and the \nfinal configuration data. This effort should attempt t o verify that the final bitstream and \noriginating application HDL are logically equivalent with no extraneous logic in the final \nformat. This action will confirm that no Trojan s were inserted during the implementation \nsteps.  \nOption 2:  Use a reprod ucible build process to validate the software.   \nWhen using reproducible builds to validate software , enlist a third party to mirror the \nFPGAs synthesis, place and route, and configuration file generation. If the mirroring is \nexecuted properly and independ ently, the outputs can be compared to verify that the \nvendor software package is unmodified or modified in a way that does not affect the \napplication design. To ensure proper execution of this mitigation, the following must be \nobserved:  \n The software used t o mirror the programs synthesis effort must be procured in a \nmanner to make it independent from the procurement of the original version.  \n The reproducible build software should be loaded/installed by a different \nadministrator than the administrator that pe rformed the original install.  \n This mitigation requires independent duplicative activities since  the adversary \ncould have knowledge about the project and how it obtains, loads , and controls \nits tools.  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   51 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \n The mirrored effort should utilize the same version of t he software on the same \noperating system and version.  \n The application development teams software and the mirroring software should \npossess matching hashes and size values.  \n The mirrored effort must utilize the same HDL code, IP and synthesis scripts.  \n The mirrored effort must utilize the same vendor tool settings.  \n The output of the effort is an unencrypted, uncompressed configuration data file.  \nContact  the FPGA software vendor for more detailed guidance on creating reproducible \nbuilds. They have already per formed work in this area and can assist with documented \ninstructions.  \nBoth the development effort and the mirror effort should execute the FPGA \ndevelopment flow from synthesis to configuration file output and then perform the \nfollowing steps:  \n Throughout th e flow, output any intermediary files that can be used to compare \nresults at various stages. This can include primitive netlists, synthesized netlists, \nphysical netlists , and final configuration data files.  \n Compare the final configuration files for size a nd content. They should match in \nall respects except for header information that may include timestamps and other \nproperty information.  \n If the files are encrypted, take steps to ensure that any nonces, such as the \ninitialization vector, used by both effor ts are the same.  \nIf discrepancies are found in the comparison, the following steps should be followed:  \n Contact the software vendors for assistance.  \n Contact  JFAC for assistance in resolving the discrepancy.  \nIf a software version does not match what was expe cted, JFAC recommends  report ing it \nto the vendor for further analysis and correction.  \nTD 8: Adversary modifies FPGA platform family at design  \nIn this threat, an adversary inserts a malicious function or preplaces a vulnerability for \nlater use in an FPGA de vice during its hardware design phase. This attack involves a \nnetwork intrusion  or a compromised insider working for the vendor or one of its \nsubcontractors. While this attack lacks the ability to target an individual program, it can \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   52 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \npreposition a vulnerab ility for later use.  Evaluation of manufactured hardware for built -in \nmalicious functions or vulnerabilities is a very difficult, highly expensive , and near \nimpossible task.  As such,  no practical amount of evaluation can guarantee the absence \nof any design ed-in malicious function.   \nTD 8  mitigations  \n Engage JFAC  to evaluate the FPGA device family.  \nTD 8 mitigation description  \nEngage JFAC  \nJFAC recommends  the program engage JFAC to evaluate the chosen FPGA device \nfamily or to acquire information garnered from p revious evaluations. JFAC will then \ninstruct the  program on what steps to take to identify malicious code or weaknesses in \ntheir FPGA platform. Initially, the program may be asked to conduct a subset of the \nevaluation steps in partnership with JFAC. In par allel, JFAC may evaluat e the  FPGA \ndevice famil y for malicious behavior and operational weaknesses. In addition, JFAC has \nbeen evaluating commonly used FPGA device families proactively.  \nIn support of this mitigation, JFAC asks all programs seeking LoA compl iance at any \nlevel to provide JFAC with information regarding the FPGA devices they are using along \nwith a brief summary of the ir use. This information will be compiled to create a picture of \nwhich FPGAs are of greatest interest to DoD and which ones might  represent a \nvulnerability to multiple programs. This information will drive the decision -making behind \nwhich device families to proactively analyze for vulnerabilities.  \nJFAC communicates this information at a variety of classification levels. Please contact \nJFAC to obtain the appropriate email address  at https://jfac.navy.mil.  \nRefer to Appendix C: JFAC FPGA reporting template  for the information a program \nshould include in the email.  \nAs evaluations are completed,  JFAC will document the findings for programs to use in \ntheir vulnerabilit y research.  \nFinally, JFAC recommends  that program s utilize newer and more modern device \nfamilies when possible . The se families  possess more mature design architectures that \nencompas s vulnerability fixes and advanced assurance features.  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   53 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nTD 9: Adversary compromises single -board computing \nsystem (SBCS)  \nIn this threat, an adversary compromises a single -board computing system (SBCS) \npurchased by a program for use in a system. A n SBCS is a  commercial off -the-shelf \nproduct consisting of a PCB with FPGAs and computer processing resources. These \nboards are common throughout DoD systems as they are readily available in the \nmarketplace. Under this threat, the program does not have control of the  manufacturing \nprocess of the SBCS , forcing the prog ram to rely upon a verification heavy approach to \nmitigating attacks. In this light, programs should work with existing DoD providers to \nbuild custom SBCS devices in compliance with LoA3 guidelines .  \nOf primary concern in this scenario are threats to : \n Authenticity of the FPGA devices  \n PCB connections to the FPGA  \n The configuration methodology  \n Test interfaces  \nThe following mitigations only address  the hardware assurance concerns related to the \nmanufacturing and operation of the FPGA device and do not consider other \ncomponents of the SBCS.  \nTD 9  mitigations  \n Programs should engage a DoD vendor  to build the SBCS  devices under the \nLoA3  constraints. This includes the use of clear ed people and classified facilities , \nminimally at the Secret  level. \n All verification and authentication  steps in this section should be conducted by a \nteam of people independent from the manufacturing team.  \n Authenticate the FPGA devices .  \n Verify the  SBCS configuration process  and that the board-level connections \ncomply with the LoA3  mitigation requiremen ts.  \n Document the steps  taken to comply with these requirements. This includes \nhardware and software features.  \n Test nonvolatile memory  verifying there are  no conflicting prepopulated settings.  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   54 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nTD 9 mitigation descriptions  \nEngage a DoD vendor  to build the SBCS  \nPrograms should engage a DoD supplier to build the SBCS devices under the LoA3  \nconstraints. This includes the use of people cleared at least at the Secret  level working \nin Secret  cleared e nvironments .  \nVerification and authentication  \nAll verification and authentication steps in this section should be conducted by a team of \npeople independent  from the manufacturing team.  This team should obtain  and review \nthe SBCS schematics for functional c orrectness, vulnerabilities , and security concerns \nas they relate to the FPGA configuration process and security connections.  Verify  the \nPCB traces related to the FPGA device, the configuration memory devices, and any \nother devices related to the authentic ation of the configuration data. The program \nshould rely on guidance from the JFAC PCB Executive Agent to perform this \nverification. This evaluation should be performed on all devices.  \nAuthenticate the FPGA devices  \nIn this mitigation, the program should au thenticate the devices utilizing the \nrecommendations found under TD 2: Adversary inserts malicious counterfeit . Then , the \ndevices should be re -authenticated upon completion of th e SBCS manufacture utilizing \na cryptographically protected ID or through the use of a soft PUF.  \nVerify the SBCS configuration process  \nUtilize SBCSs whose configuration process and board level connections comply with \nthe LoA3  mitigation requirements for TD 6: Adversary swaps configuration file on target . \nThis includes , but is not limited to , requirements for:  \n NIST comp liant authentication algorithms   \n Differential p ower analysis  (DPA) resistant authentication  \n Protect ed key storage  \n Anti-tamper de tection and response   \n Being f ree of known vulnerabilities in the confi guration and security functions   \n All encryption and authentication keys lengths must be compliant with the \nrequi rements outlined NIST SP 800 -57  \n The ability  to disa ble FPGA test pins, such as JTAG   \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   55 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nIf the configuration file memory storage device contains SBCS vendor code, the \nprogram should review and evaluate that code for malicious function s. The proprietary \nSBCS support for configuration must be fully unde rstood and validated.  If the SBCS \nconfiguration process cannot be fully evaluated,  it should not be used at LoA3 . \nOnce the SBCSs configuration design and implementation are evaluated to be free of \nmalicious function s, the program should craft a set of tes ts and validation processes to \nverify that all the devices comply with the evaluation.  \nTest non-volatile memory  \nPoll the FPGA settings captured in non -volatile memory , such as fuses,  to determine if \nthe SBC S vendor has preprogrammed any settings in a manne r conflicting with these \nassurance guidelines or that conflict with user application needs.  \nDocument the steps  \nDocument all steps taken to demonstrate compliance with TD 9.  These steps and \nassociated data artifacts should be auditable.  \nTD 10: Adversary modifies vendor FPGA software design \nsuite during development  \nIn this threat , an adversary modifies the vendor design suite during its development to \nsubvert the DoD application during FPGA implementation. This subversion could \ninclude:  \n Inserting a malicious  function or vulnerability into the device during synthesis, \nplace and route , or configuration data generation.  \n Enabl ing the exfiltration of program application design data over a network \nconnection.  \nThis subverted tool would then be part of the authorized  software delivered by the \nvendor and its distributors. In this light, delivery protections such as encryption, package \nsigning, and hashes would have no mitigating value. Evaluating the vendor  software \nand certifying it as Trojan  free is a prohibitively intensive  and costly venture that is not \npractical at the program level.  \nAt present, the only approach to addressing this attack is to verify the results of the \nFPGA implementation steps. Rather than determine that the tool is Trojan  free, the \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   56 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \napproach is to verify that the tool suite did nothing malicious to the application design. \nLogical equivalence checking (LEC) is the tool used to perform this verification .  \nJFAC is currently investigating additional measures to detect and thwart compromised \nvendor to ols. Pending new advances, JFAC can assist programs with overcoming the \ndifficulties of performing LEC.  \nTD 10  mitigations  \n To prevent exfiltration of data from a malicious FPGA EDA tool, perform all \nFPGA design work on an isolated network  as recommended in the mitigations  for \nTD 3: Adversary compromises application design cycle . \n Perform logical  equivalency checking  between the application HDL and the final \nconfiguration data.  \nTD 10  mitigation d escriptions  \nPerform l ogical equivalency checking  \nTo the greatest  extent possible, LEC verifie s that the vendor tools did not modify the \nlogic or configuration settin gs. The goal is to verify  that the final bitstream and \noriginating application HDL are logically equivalent with n o extraneous logic in the final \nformat. This confirms that Trojans were not inserted during the implementation steps.  \nThe LEC also verifies that the configuration settings were maintained and not altered. \nConfiguration settings are those parameters include d in the configuration file that affect \nthe behavior of the FPGA device itself but are not a part of the program application. \nExamples include tamper settings, JTAG settings , and key storage.  \nThere are technical challenges associated with performing LEC on  FPGA data . First, \ndue to the proprietary nature of the configuration file format, including it i n the LEC effort \nis difficult. Contact JFAC for information on commercial tools that can assist with th is for \nseveral device families.  \nAdditionally , many FPGA synthesis optimizations make it difficult to perform LEC. For \nthis reason, the following are recommended:  \n Perform LEC after each implementation step to limit the amount of change that \nmust be accounted for by the tool. This includes synthesis, place and ro ute, and \nconfiguration data generation.  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   57 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \n Use hints files to assist in matching difficult -to-correlate logic in the compared \ndatabases.  Most LEC  tools accept these files.  \n Contact JFAC for information on  emerging industry tools that can assist in \nidentifying configuration data in the FPGA formats  or automate the creation of \nhints files.  \n3 Summary  \nThe mitigations in this report are intended to protect against adversarial  threats to  \nassurance on FPGA -based systems. Once a program incorporates the mitigations for  \nthese 10 threat descriptions,  it can consider its FPGAs to have achieved LoA3 .  \nIf a program has developed alternate solutions for mitigating these threats , it can \nconsult with JFAC to determine if the alternative mitigation s are sufficient .  \nFinally, if a program has questions regarding this report  or requires assistance, it should \ncontact  JFAC at https://jfac.navy.mil/  for assistance.  \n  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   58 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nAppendix  A: Standardized terminology  \nThe following term s are used in the Joint Federated Assurance Center Field \nProgrammable Gate Array Best Practices documents. These terms are modified from \nDefense Acquisition University definitions to support common understanding.   \nApplication design   The collection of sch ematics, constraints, hardware description \nlanguage (HDL), and other implementation files developed to generate an FPGA \nconfiguration file for use on one or many FPGA platforms.   \nApplication domain   This is the area of technology of the system itself, or a directly \nassociated area of technology. For instance, the system technology domain of a radar \nsystem implemented using FPGAs would be \"radar\" or \"electronic warfare.\"   \nConfiguration file   The set of all data produced by the application design team and \nloaded into an FPGA to personalize it. Referred to by some designers as a bitstream, \nthe configuration file includes that information, as well as additional configuration \nsettings and firmware, which some designers may not consider part of their bitstrea m.  \nControllable effect   Program -specific, triggerable function allowing the adversary to \nattack a specific target.   \nDevice/FPGA device   A specific physical instantiation of an FPGA.   \nExternal facility   An unclassified facility that is out of the contr ol of the program or \ncontractor.   \nField programmable gate array (FPGA)   In this context FPGA includes the full range \nof devices containing substantial reprogrammable digital logic. This includes devices \nmarketed as FPGAs, complex programmable logic device s (CPLD), system -on-a-chip \n(SoC) FPGAs, as well as devices marketed as SoCs and containing reprogrammable \ndigital logic capable of representing arbitrary functions. In addition, some FPGAs \nincorporate analog/mixed signal elements alongside substantial amou nts of \nreprogrammable logic.   \nFPGA platform   An FPGA platform refers to a specific device type or family of devices \nfrom a vendor.   \n \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   59 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nHard IP   Hard IP is a hardware design captured as a physical layout, intended to be \nintegrated into a hardware design in the layout process. Hard IP is most typically \ndistributed as Graphic Design System II (GDSII). In some cases, Hard IP is provided by \na fabrication company and the user of the IP does not have access to the full layout, but \nsimply a size and the information  needed to connect to it. Hard IP may be distributed \nwith simulation hardware description language (HDL) and other soft components, but  is \ndefined by the fact that the portion that ends up in the final hardware was defined by a \nphysical layout by the IP ve ndor.   \nLevel of assurance ( LoA)  A Level of Assurance is an established guideline that \ndetails the appropriate mitigations necessary for the implementation given the impact to \nnational security associated with subversion of a specific system, without the need for \nsystem -by-system custom evaluation.   \nPhysical unclonable function (PUF)   This function provides a random string of bits of \na predetermined length. In the context of FPGAs, the randomness of the bitstring is \nbased uponvariations in the silicon of the device due to manufacturing. These bitstrings \ncan be used for device IDs or keys.   \nPlatform design   The platform design is the set of design information that specifies \nthe FPGA platform, inc luding physical layouts, code, etc.   \nSoft IP   Soft IP is a hardware design captured in hardware description language \n(HDL), intended to be integrated into a complete hardware design through a synthesis \nprocess. Soft IP can be distributed in a number of  ways, as functional HDL or a netlist \nspecified in HDL, encrypted or unencrypted.   \nSystem   An aggregation of system elements and enabling system elements to achieve \na given purpose or provide a needed capability.   \nSystem design  System design is the set of information that defines the \nmanufacturing, behavior, and programming of a system. It may include board designs, \nfirmware, software, FPGA configuration files, etc.   \nTarget  A target refers to a specific deployed instance of a given system, or a specific \nset of systems with a common design and function.   \n \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   60 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nTargetability  The degree to which an attack may have an effect that only shows up in \ncircumstances the adversary chooses. An attack that is poorly targetable would be more \nlikely to be discovered acciden tally, have unintended consequences, or be found in \nstandard testing.   \nThird -party intellectual property (3PIP)   Functions whose development are not \nunder the control of the designer. Use of the phrase intellectual property, IP, or 3PIP in \noutlining thi s methodology of design review does not refer to property rights, such as, \nfor example, copyrights, patents, or trade secrets. It is the responsibility of the party \nseeking review and/or the reviewer to ensure that any rights needed to perform the \nreview i n accordance with the methodology outlined are obtained.   \nThreat category  A threat category refers to a part of the supply chain with a specific \nattack surface and set of common vulnerabilities against which many specific attacks \nmay be possible.   \nUtilit y  The utility of an attack is the degree to which an effect has value to an \nadversarial operation. Higher utility effects may subvert a system or provide major \ndenial of service effects. Lower utility attacks might degrade a capability to a limited \nexten t.  \nVulnerability  A flaw in a software, firmware, hardware, or service component \nresulting from a weakness that can be exploited, causing a negative impact to the \nconfidentiality, integrity, or availability of an impacted component or components.  \n  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   61 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nAppen dix B: IP Reuse Guidance  \nThere are several situations in which a program/organization would like to reuse \npreviously generated  soft IP or 3PIP.  This IP can be generated internally (i.e., by an \nauthorized DoD program , but for a different program  than the original use ) or externally \n(i.e., purchased IP).   \nIP that was not generated for or previously evaluated by a DoD program in conjunction \nwith LoA3  requirements should not be used without a program evaluation.  This includes \ncases in which vendors have had the  IP evaluated by a third  party.  That review is not \nacceptable accord ing to the DoD Microelectronics: FPGA Overall Assurance Process . \nPrograms have the  sole responsibility to perform or oversee all reviews.  \nLoA3  introduces several new threat vectors, to inc lude insiders, cleared and uncleared \npersonnel working alone or in conjunction with others , and new technologies , along with \nfunding at the nation -state level . Given the complexity of LoA3  and the types of \ncomponents  and systems that require LoA3 , JFAC str ongly recommends re -evaluation \nof all IP regardless of the source.   \nIn situations where the program chooses not to re -review the previously evaluated IP , \nthe program  should ensure the following conditions are satisfied.  \nReuse c onditions  \nTo reuse IP , the fo llowing conditions should be satisfied : \na) The IP must have been  developed internally ( i.e., by a government funded and \nmanaged program)  for an LoA3  program or the IP was successfully  internally \nevaluated at LoA3 . \nb) All documentation associated with the develop ment  and/or previous evaluation \nmust be  signed with a valid cryptographic signature and stored within the \nconfiguration management system compliant with the LoA3 requirements in this  \ndocument.  The documentation must be provided to the new program in  its \ntotality.  The documentation should clearly state any known vulnerabilities or risk \nassociated with the IP.  The documentation must be proven to have remained \nunchanged since the time the evaluation was performed.  \nc) A second copy with a different  cryptographic signature of the evaluation report \nshould be stored in a controlled environment separate from the IP.  The best \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   62 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nstorage mechanisms would include in a SCIF, and either in a safe certified at the \nSecret level or on a Secret network.  \nd) The program should verify the data in the separately stored evaluation reports is \nthe same  as what the program is using . \ne) The program cannot accept any IP in which the report has discrepancies from \nthe version received.  For example, the name of the IP, version information, hash, \netc.  \nf) After the initial evaluation, the IP must remain maintained in a configuration \nmanagement system compliant with the LoA3 requirements in this  document.  \nThe hash of the IP must also be cryptographically signed and maintained in the \nconfiguration manageme nt system.  Additionally, the hash should be stored and \nmaintained with a second cryptographic signature.  The program must verify the \nseparately stored hashes match . \ng) In the event the IP was previously evaluated and there were areas of risk \nidentified, the r isk must be documented and provided to the program that would \nlike to reuse the IP. The program has the responsibility to accept or mitigate the \nrisk based on individual program needs.  \nReuse s cenarios  \nThe following section describes several use cases that provide additional details of \nwhen IP can  or cannot be reused at LoA3 . \nScenarios in which LoA3  IP reuse is applicable:  \na) The program would like to reuse internally developed  LoA3  compliant IP , but not \npreviously evaluated outside of the initial program for u se. \nIn this scenario, the IP was developed and stored internally using the processes \ndescribed in th is document. T herefore, the IP was previously shown to be  \ncompliant.  The program has the responsibility to ensure that no modifications \nwere made to the IP  since the time of development . To reuse the IP , the program \nmust demonstrate compliance with the co nditions outlined in the Reuse \nconditions  section  above . \nb) The program would like to reuse internally developed  LoA3  IP that was \npreviously successfully ev aluated to be compliant with LoA3 .  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   63 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nIn this scenario, the fact that the IP has been evalua ted and deemed compliant to \nLoA3  makes the reuse viable provided the program can demonstrate compliance \nwith the conditions outl ined in the Reuse c onditions  section above . \nc) The program would like to use  IP that was developed by an external vendor.  The \n3PIP was previously internally verified as compliant with LoA3  for a different \nprogram .  \nIn this scenario, the IP was evaluated internally using the processes outlined in \nthis document . Therefore, the IP was previously shown to be  compliant.  To reuse \nthe IP , the program must demonstrate compliance with the co nditions outlined in \nthe Reuse c onditions  section above . \nUse cases in which an  LoA3  IP evaluation in accordance with Third -Party IP Review \nProcess for Level of Assurance 3 document would be required:  \nd) The program would like to use internally developed IP that was not developed or \nevaluated to satisfy any level of assurance.  The program wo uld like to use this IP \nat LoA3 .  \ne) In this scenario, the program should treat the IP the same as unevaluated \nexternally developed 3PIP.  The program should follow the gui dance provided in  \nTD 5: Adversary compromises third-party soft IP .  \nf) The program would like to reuse internally developed IP that was developed to \nbe compliant with LoA1  or LoA2 . \ng) Based on  the increased threat comple xity at LoA3,  the program should treat the \nIP the same as externally developed IP. The program should follow the guidance \nprovided in TD 5: Adversary compromises third-party soft IP . \nh) The program would like to reuse  internally developed IP that was developed to \nbe compliant with LoA1  or LoA2  and previously successfully evaluated to be \ncompliant with LoA1  or LoA2 . \ni) Based on  the increased threat complexity , the program should treat the IP the \nsame as externally develope d IP. The program should follow the guidance \nprovided in  TD 5: Adversary compromises third-party soft IP . \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   64 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nj) An LoA3  program would like t o use externally developed 3PIP (e.g., v1.1) . A \ndifferent version of the 3PIP (e .g., v1 .0) was previously verified to be LoA1, \nLoA2 , or LoA3 compliant.  \nk) In this scenario, the IP has been modified.  Due to the modification , the program \nshould treat the IP the same as unevaluated externally developed 3PIP. The \nprogram should follow the g uidance provided in TD 5: Adversary compromises \nthird-party soft IP . \nl) At LoA3 , the program would like to use externally developed 3PIP that was \npreviously verified by an independent third party at LoA1, LoA2 , or LoA 3. \nm) Program independent third party reviews are not acceptable.  The program \nshould treat the IP the same as not previously reviewed IP. The program should \nfollow the guidance provided  in TD 5: Adversary compromises third-party soft IP . \n  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   65 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nAppendix C: JFAC FPGA reporting template  \nEach program is requested to provide the following information to JFAC.  Multiple email \naddresses are provided to support a variety of classi fication levels; only one e mail to \nany of these is r equired. Please contact  JFAC to obtain the appropriate email address \nat https://jfac.navy.mil.  \nThe template and informati on to be included in the email are as  follows:  \n=============================================  \n*** Please Portion Mark Appropriately ***   \n(U) POC Contact Info   \n(U) Name:  \n(U) Organization/Company:  \n(U) Email:  \n(U) Phone:  \n(U) Address:  \n  \n(U) Program Info   \n(U) Program Name (top -level program, i.e. F35, M1 tank, etc.):  \n(U) US Govt Sponsor: (Air Force, Army, Marines, Navy, DOE, other)  \n(U) D o you want to be included in any future JFAC FPGA Assurance related bulletins in \nthe future?  \n(U) Estimated Number of Systems to be Built:  \n(U) Program Description (1 -3 sentences describing the top -level program in which the \nsubsystem listed below is inclu ded):  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   66 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \n  \n(U) FPGA Info (for each FPGA part number used)  \n(U) FPGA Vendor: (Intel, Lattice, MicroChip, Xilinx, other)  \n(U) FPGA Device Family:  \n(U) FPGA Device Part Number:  \n(U) FPGA Design Software Used and Version #:  \n(U) Description of Subsystem Containing  FPGA Device:  \n(U) Total Estimated Number of Subsystems to be Built:  \n(U) Operating Environment: (mil, ind, com, radiation, cryo)  \n(U) Source/seller of the FPGA devices:  \n(U) Date purchased:  \n(U) Anticipated Fielding date:  \n(U) LoA Level:  \n(U) Description o f FPGA Role in Subsystem.  If multiple instances of FPGA devices, \nnumber and describe the role of each.  \n1.  \n2.  \n3.  \n===============================================  \n \nExample  \n=============================================  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   67 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \n*** Please Portion Mark Appropriate ly ***  \n(U) POC Contact Info   \n(U) Name:  Jack Jackson  \n(U) Organization/Company: Army Research Lab  \n(U) Email : jjackson@army_email.mil  \n(U) Phone:  555-555-5555  \n(U) Address:  10 Main St, Fort Murphy, Illinois 55555  \n  \n(U) Program Info   \n(U) Program Name (top -level program, i.e. F35, M1 tank, etc.):  Next Generation \nCombat Vehicle (NGCV)  \n(U) US Govt Sponsor: (Air Force, Army, Marines, Navy, DOE, other) Army  \n(U) Do you want to be included in any future JFAC FPGA Assurance related bulletins in \nthe future? :  Yes \n(U) Est imated Number of Systems to be Built: 1400  \n(U) Program Description (1 -3 sentences describing the top -level program in which the \nsubsystem listed below is included ): \nThe Next Generation Combat Vehicle  Future Decisive Lethality (NGCV -FDL) \nwill have capabil ities that are enabled by assured position, navigation and \ntiming and resilient networks . This will enable future maneuver formations to \nexecute semi -independent operations while conducting cross -domain \nmaneuver against a peer adversary . \n  \n(U) FPGA Info (f or each FPGA part number used)  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   68 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \n(U) FPGA Vendor: (Xilinx, Intel, MicroChip, Lattice, other):  Acme MicroElectronics  \n(U) FPGA Device Family:  Big Blue Iceberg  \n(U) FPGA Device Part Number:  BBI-624L100K  \n(U) FPGA Design Software Used and Version #:  IceBreaker V2 021.15  \n(U) Description of Subsystem Containing FPGA Device:  image processing for data \noriginating from the cannon targeting sensor  \n(U) Total Estimated Number of Subsystems to be Built: 3000  \n(U) Operating Environment: (mil, ind, com, radiation, cryo):  mil \n(U) Source/seller of the FPGA devices : Digikey, online  \n(U) Date purchased: 2/25/2020  \n(U) Anticipated Fielding date:  5/1/2022  \n(U) LoA Level : 1 \n(U) Description of FPGA Role in Subsystem.  If there are  multiple instances of FPGA \ndevices, number and describe the  role of each  one.  \n1. FPGA #1  is used to perform  signal processing on raw image data coming in \nfrom the externally mounted cannon . \n2. FPGA #2  is used to perform signal processing on raw image data coming \nfrom the scout drone through the external anten nae #2 and synchronized with \nGPS positioning data.  \n================ ===============================  \n  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   69 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nAppendix D: Mitigations  and data/documentation  \nrequirements  \nChecklist for TD  1: Adversary utilizes a known FPGA platform \nvulnerability  \nTD 1 mitigation s Data/Documentation requirement  \nUse caution when selecting tools or \nplatforms   The program should document the name of the person \nperforming the research, the date timestamp of the \nresearch, the research results, and the vendor provided \nend-of-life plan or re lease notes (if available). If \nbeta/initial release is selected , the program should \ndocument the rationale behind the selection and \ncontain the signature of the programmatic approval \nauthority.  \nUse cleared personnel  In writing, t he program should designat e work that mus t \nbe done by cleared individuals . The program should \nkeep a log of personnel assigned to that work along \nwith their clearance level.  \nThe program should maintain a list of the members \ncomprising each team, with clearance level. The \nprogram should maintain audit logs demonstrating what \neach team member accessed.  \nResearch vulnerabilities  The program should document each publication that \nwas searched ( including at a minimum t hose identified \nin this guidance ), search results, the name of the \nperson who perform ed the search , and date timestamp \nwhen the search was performed.  The same information \nshould be documented by the reviewer.  \nIf a vulnerability is found, choose one of the following options:  \nOption 1:  Select a different FPGA \nplatform , device , or software  The program should document each publication that \nwas searched (minimally those identified in this \nguidance should be searched) , the search results, the \nname of the person performing the search , and the \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   70 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nTD 1 mitigation s Data/Documentation requirement  \ndate and timestamp of when the search w as \nperformed.  \nOption 2:  Work  with the vendor  The program should work through the vendor process \nto formally notify the vendor of any vulnerabilities, and \nonly accept fixes through formal , approved processes. \nThe program should maintain documentation regar ding \nthe identified vulnerability, log communication with the \nvendor, and document the source and method of the \nreceived fix.  \nOption 3:  Risk analysis  The program should maintain documentation \nidentifying the risk, any mitigations, and the approval \nauthor ity for accepting the residual risk.  \nUse revision control/version \nmanagement   The program should document, maintain, and utilize a \nprogram configuration management (CM) plan. This \nplan should include details on how configuration data \nwill be maintained f or control and audit purposes.  The \nsystem used for CM should be named, and \nimplementation specific details should be documented.  \nThe program should  document how the CM plan is \ncompliant with NIST SP 800-171 Protecting Controlled \nUnclassified Information in  Nonfederal Systems a nd \nOrganizations. If a classified system is used, the \nprogram should sto re a copy of the approve d SSP.  \nAudit logs should be reviewed with the results \nrecorded.  \nEnforce auditability  The program should maintain audit logs on all design  \ndata, includ ing requirements, architecture, design , \ncode, test s, bugs , and fixes. The audit data minimally \nshould document who requested the change with date \nand timestamp, the decision made regarding the \nchange, who made the decision with date and \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   71 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nTD 1 mitigation s Data/Documentation requirement  \ntimest amp, why was the change requested, and who \nmade the change with date  and timestamp . \nEnforce the approved design \nprocess  The program should document program design \nmilestones with clear ent ry and exit criteria. The entry \nand exit criteria should be specifi cally identified to \ninclude the peer review/code review and technical \nreview processes. The entrance and exit criteria should \nbe utilized throughout the program lifecycle. The \ndocumentation should contain artifacts demonstrating \nthe gates were satisfied, w ith signed management \napproval . \nThe program should obtain the results of independent \nreviews to include:  \n  Type and extent of verification performed, to include \nevaluation objective, methodology, and tools  \n  Findings, both positive and negative, for all \nevaluations performed  \n  Risks identified by the review team (e.g., quality \nissues, vulnerability to threats, etc.)  \n  Recommendations to mitigate identified risks  \n  Independent team should be separate from the team \ndoing the design  \n Identification and c redentials of each reviewer  \n Date and t imestamp of when the review was \nperformed  \nChecklist for TD  2: Adversary inserts malicious counterfeit  \nTD 2 mitigation s Documentation requirements  \nPurchase from DoD authorized \nvendors and distributors  The program sh ould document the name and location \nof the authorized vendor along with documentation \ndemonstrating that the vendor is authorized.  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   72 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nTD 2 mitigation s Documentation requirements  \nConsult Government -Industr y Data \nExchange Program (GIDEP)   The program should document the GIDEP search \nresults, the name or  ID of the person performing the \nsearch , and the date and timestamp of when the \nsearch was performed.  \nFollow s torage and shipping  \nguidance  The program should document, maintain and enforce a \ntransportation plan which supports the movement of \nbulky classif ied material. Minimally the plan should \ninclude:  \n Title of Plan  \n Date of movement  \n Authorization/Approval  \n Purpose  \n Description of consignment, to include unique ID \nwhen available  \n Identification of responsible government and/or \ncompany represent atives  \n Identification of commercial entities to be involved in \neach shipment  \n Packaging of the consignment  \n Routing of the consignment  \n Couriers/escorts  \n Recipient responsibilities  \n Return of material procedures  \n Other information as required  \nThe program should document, maintain , and enforce \na storage plan which supports the storage of bulky \nmaterial.  \nVerify the FPGA cryptographically \nsecure ID  The program should document and store the ID of each \nFPGA against the ID that was provided direc tly by the \nvendor . \nPerform physical \ninspection/ analysis  The program should document the results of the \nphysical analysis test with each FPGA unique ID the \ntest was performed on.  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   73 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nTD 2 mitigation s Documentation requirements  \nTo mitigate risk of a cleared insider:   \nSelect sample parts  The program sho uld document:  \n The process to secure the device and the results  \n All parties that touched the device with  the reason for \nthe interaction   \nCreate c ryptographically protected \nIDs post verification  The program should record the device serial number \nand PUF  ID. \nCompar e results anytime the programs compares the \nsoft PUF and unique ID for confirmation of the \nauthenticity of the part.  \nVerify independent lab work  The program should require:  \n The return of residual materials and detailed reports \nafter evaluatio n \n The approved storage plan to be utilized by the lab \nwith acceptable evidence  \n Documentation that demonstrates the lab identified \nthe known bad parts ; the name, address, and division \nof the two independent labs;  or resul ts of physical \ninspection  \nIn ad dition to verify ing independent lab work above , choose one of the following options:  \nOption 1:  Insert known bad parts  Document the known bad parts, the problem with the \npart, and the results from the verification facility that \nperformed the physical analy sis. \nOption 2: Use duplicate \nindependent labs  Document the credentials of the lab observer s, the \nfindings, and conclusion.  The conclusion should \nconfirm if the lab results match or are different.  \nOption 3: Use duplicate persons \nassigned to the program  Document the credentials of the observer s, the \nfindings, and conclusion.  The conclusion should \nconfirm if the  results match or are different.  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   74 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nTD 2 mitigation s Documentation requirements  \nFollow guidance for TD 4: \nAdversary compromises system \nassembly, keying, or provisioning  Provide all of the TD4: Ad versary compromises system \nassembly, keying, or provisioning  data requirements . \nChecklist  for TD  3: Adversary compromises application design cycle  \nTD 3 mitigation s Documentation requirements  \nUse Secret level cleared personnel  In writing, t he program shou ld designate work that mus t \nbe done by cleared individuals . The program should \nkeep a log of personnel assigned to that work with their \nclearance level.  \nThe program should maintain a list of the members \ncomprising each team, with clearance level. The \nprogr am should  maintain audit logs demonstrating what \neach team member accessed.  \nTrack critical data in a revision \ncontrol  system  The program should ensure the following data items \nare tracked in revision control:  \n Third -party IP (3PIP)  \n Utilized libraries  \n Devel opment files, code, software used for \ndevelopment, synthesis scripts, and tools  \n Test Benches, Test Plans and Test Procedures, \nand Test Reports  \n Tool configuration settings  \n Design documents to include:  \n Critical documents, to minimally include \nrequirements,  design artifacts, test reports, test \nplans, and discrepancy reports.  \n Documentation with approval to proceed from \norganizationally defined reviews: code reviews, \narchitecture reviews, technical design reviews, and \nverification and validation reviews.  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   75 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nTD 3 mitigation s Documentation requirements  \nEach of these artifacts should be identified in the \nprograms auditing strategy and the audit logs should \nminimally include  decisions that were made, by whom, \nfor what reason, and on what date.  \nEnforce auditability  The program should maintain audit logs on all design \ndata, includ ing requirements, architecture, design , \ncode, test s, bugs , and fixes. The audit data minimally \nshould document who requested the change with date \nand timestamp, the decision made regarding the \nchange, who made the decision with date and \ntimestamp, why was the change requested, and who \nmade the change with date  and timestamp . \nUse revision control/version \nmanagement   The program should maintain revision control \ndocumentation in accordance with requirements of \nCMMC level 3 or NIST 800 -171 Protecting Controlled \nUnclassified Information in Nonfederal Systems and \nOrganizations  and NIST 800 -172 Enhanced Security \nRequirements for Protecting Controlled Unclassified \nInformation . The program should maintain the CMMC \naudit results or NIST 800 -171 self-assessments.  \nTD 3.1 Mitigating the i ntroduction of a compromised design  into the application  \nIsolate and store the application \ndesign  The program should document the hash of the final \nconfiguration after the final design and verify the hash \nprior t o provisioning. The program should maintain the \nconfiguration management audit logs.  \nPerform reproducible build  Document the reproducible build process and results \nvalidating that the two separate build s produce the \nsame binary and hash.  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   76 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nTD 3 mitigation s Documentation requirements  \nTD 3.2 Mitigat ing the modification of test benches/plan to reduce coverage or hide \nTrojan code  \nExecute a documented test plan  The program should document and  maintain a test plan \nthat includes a mechanism to verify all requirements.  \n The test plan should explicitly li st code coverage \nmetrics, the type of testing that will be performed, and \nacceptable testing guidelines.  \n Code coverage should state how much code is \nchecked by the test bench, providing information about \ndead code in the design and holes in test suites.  \nEnsure code coverage includes statement coverage, \nbranch coverage, Finite State Machine (FSM), \ncondition, expression, and toggle coverage. Document \nany code that will not be covered and why. Ensure \nuntested code is documented and reviewed through \nthe revi ew process. Use functional test s to verify the \nFPGA does what it is supposed to do. Any deviations \nmust be documented and approved.  \n The decision to use/not use other types of testing \nsuch as directed test, constrained random stimulus, \nand assertion shou ld be documented.  \n Unexpected behavior should be documented and \nanalyzed, with final implementation conclusions \ndocumented.  \n The test plan should specify the verification \nenvironment which describes the tools, the software, \nand the equipment needed to perform the reviews, \nanalysis, and tests. Each of these items should be \nmaintained under revision control.  \n Ensure all test discrepancies, bugs, etc. are resolved \nvia a change process.  \nValidate and verify test processes  The program should document, rev iew, maintain, \nenforce , and archive the test plan. The test plan should \ninclude which tools will be used with names, version \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   77 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nTD 3 mitigation s Documentation requirements  \nnumbers, and the various test reviews that will take \nplace, type of testing to be performed, and the methods \nused to accomplish the  test.  \nThe program should maintain documentation of all \ntesting performed, including members of each team \nand role, all documentation associated with peer \nreviews, configuration logs indicating all actions taken \nby whom and when, and use of automated tool s where \napplicable. All test discrepancies, bugs, etc. should be \nresolved via a change process utilizing a change \nmanagement system. The established processes \nshould be documented, enforced, and audited.  \nMaintain t est environment via \nconfiguration managem ent The program should maintain configuration \nmanagement documentation in accordance with \nrequirements of CMMC level 3 or NIST SP 800-171 \nProtecting Controlled Unclassified Information in \nNonfederal Systems and Organizations and NIST SP \n800-172 Enhanced Se curity Requirements for \nProtecting Controlled Unclassified Information.  The \nprogram should maintain the CMMC audit results or \nNIST SP 800-171 self -assessments.  \nTD 3.3 Mitigating the  introduc tion of Trojans into the application design during \ndevelopment  \nMaintain bi -directional link to \napproved requirements   The program should document bi -directional \ntraceability for all device requirements, including \nderived requirements.  \nEnforce peer review   The program should document the results of each peer \nreview to include:  \n  Entry criteria and status,  \n  Roles and responsibilities with associated names,  \n  Attendees,  \n  Findings, including deviations or waivers and \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   78 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nTD 3 mitigation s Documentation requirements  \nassociated rationale and approval,  \n  Exit criteria and status.  \nExecute a documented test plan  The p rogram should document  and maintain a test plan \nthat includes a mechanism to verify all requirements.  \n The test plan should explicitly list code coverage \nmetrics, the type of testing that will be performed, and \nacceptable testing guidelines.  \n Code cover age should state how much code is \nchecked by the test bench, providing information about \ndead code in the design and holes in test suites. \nEnsure code coverage includes statement coverage, \nbranch coverage, Finite State Machine (FSM), \ncondition, expression,  and toggle coverage. Document \nany code that will not be covered and why. Ensure \nuntested code is documented and reviewed through \nthe review process. Use functional test s to verify the \nFPGA does what it is supposed to do. Any deviations \nmust be documented and approved.  \n The decision to use/not use other types of testing \nsuch as directed test, constrained random stimulus, \nand assertion should be documented.  \n Unexpected behavior should be documented and \nanalyzed, with final implementation conclusions \ndocu mented.  \n The test plan should specify the verification \nenvironment which describes the tools, the software, \nand the equipment needed to perform the reviews, \nanalysis, and tests. Each of these items should be \nmaintained under revision control.  \n Ensure a ll test discrepancies, bugs, etc. are resolved \nvia a change process.  \nImplement, validate , and verify test \nprocesses  The program should maintain documentation of all \ntesting performed, including members of each team \nand their roles, all documentation asso ciated with peer \nreviews , configuration logs indicating all actions taken \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   79 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nTD 3 mitigation s Documentation requirements  \nby whom and when , and use of automated tools where \napplicable.  All test discrepancies, bugs, etc. , should be \nresol ved via a change process utilizing  a change \nmanagement system.  The e stablished processes \nshould be documented, enforced , and audited   \nSelect a formal proof process   Document all code that was reviewed using LEC, any \nfunctional discrepancies, and how those discrepancies \nwere resolved.   \nTD 3.4 Mitigating the introduction  of compromised tooling/software into the \nenvironment  \nValidate cryptographic hash es The program should document the value of the \ncalculated cryptographic hash and the signed hash \nprovided by the vendor along with the software name, \nversion , and release n umber.  \nResearch vulnerabilities  The program should document each publication that \nwas searched, ( including at minimum  those identified in \nthis guidance ) search results, the name of the person \nperforming the search and  the date and timestamp \nwhen the sear ch was performed.  \nIf vulnerabilities are found in the software or tools, choose one  of the following options:  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   80 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nOption 1: Select a different  tool The program should document each publication that \nwas searched, ( including at minimum those identifi ed in \nthis guidance ) search results, the name of the person \nperforming the search and  the date and timestamp \nwhen the search was performed.  \nOption 2: Work with vendor  The program should maintain documentation regarding \nthe identified vulnerability, log communicatio n with the \nvendor, and document the source and method of the \nreceived fix.  \nOption 3 : Risk analysis  The program should maintain documentation \nidentifying risk, mitigations and approval authority .  \nTo validate tools , choose one of the following options:   \nUse a formal proof process  Document  all code that was review ed using LEC, \ndocument any functional discrepancies and how those \ndiscrepancies were resolved.  \nUse a r eproducible build process  The program should document the reproducible build \nprocess and re sults validating the separate builds \nproduce the same binary and hash.  \nTD 3.5 Mitigating intrusion into the internal network  \nAssign Roles  The program should approve, document , and maintain \nall individuals, the roles they perform , and the access \nallowed by that role.  At a minimum, these roles should \ninclude design, test, network administration , and \nsystem administration.  \nControl and monitor access  Entry/access to appropriate areas should be recorded, \nmonitored, and logged for auditability.  \nResearch  vulnerabilities  The program should document  each publication that \nwas searched, the results of the search, vulnerabilities \nand/or mitigations if applicable, name of the person \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   81 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nperforming the search, and the date and timestamp of \nthe search.  \nUse a secret o r classified network  The program should maintain documentation and audit \ndata demonstratin g a network classified at the DSCA \nSecret  level or above. The documentation should \ninclude a log of personnel with clearance information, \nall records in accordance wi th a maintaining a DSCA \nSecret  network , as well as a documented and SSP.  \nTD 3.6 Mitigating risk from compromised hire or employee  \nEnforce auditability  The program should maintain audit logs on all design \ndata, includ ing requirements, architecture, desig n, \ncode, test s, bugs , and fixes. The audit data minimally \nshould document who requested the change with date \nand timestamp, the decision made regarding the \nchange, who made the decision with date and \ntimestamp, why was the change requested, and who \nmade th e change with date  and timestamp . \nEnforce the approved  design \nprocess  The program should document and utilize the entry and \nexit criteria of each stage of the design process. This \nincludes documentation for each peer review and \ndesign review with roles a nd responsibilities along with \nassociated names, attendees, and findings , including \ndeviations or waivers and associated rationale and \napprovals.  \nAll design changes should be documented and  \napproved, and testing should adhere to organizationally \napproved t est standards.  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   82 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nReview critical  design activitie s The program should obtain the results of independent \nreviews to include:  \n  Type and extent of verification performed, to include \nevaluation ob jective, methodology, and tools  \n  Findings, both positive and  negative , for all \nevaluations performed  \n  Risks identified by the review team (e.g., quality \nissues, vulnerability to threats, etc.)  \n  Recommendatio ns to mitigate identified risks  \n  Independent team should be separate  from the team \ndoing the design  \n  Identification a nd credentials of each reviewer  \n  Date and t imestamp o f when the review was \nperformed  \nUse c leared personnel  In writing, t he program should designate work that mus t \nbe done by cleared Individuals . The program should \nkeep a log of personne l assigned to that work along \nwith their clearance level.  \nThe program should maintain a l ist of the members \ncomprising  each team with their clearance level s. The \nprogram should  maintain audit logs demonstrating what \neach team member accessed.  \nTD 3.7 Mitig ating risk associated with the compromise of device identifiers  \nStore device identifiers  Maintain access control  logs to include who has access \nto the device identifiers and who actually accesses the \ndevice identifiers.  \nLimit  access to  device identifier \ninformation  Maintain access control  logs to include who has access \nto the device identifiers and who actually accesses the \ndevice identifiers  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   83 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nChecklist for TD  4: Adversary compromises system assembly, keying, \nor provisioning  \nTD 4 mitigation s Documentation  requirements  \nPurchase from DoD authorized \nvendors and  distributors   The program should document the name and location \nof the authorized vendor along with documentation \ndemonstrating that the vendor is authorized.  \nFollow s torage and shipping \nguidance   The program should document, maintain , and enforce \na transportation plan which supports the movement of \nbulky classified material. Minimally the plan should \ninclude:  \n Title of Plan  \n Date of movement  \n Authorization/Approval  \n Purpose  \n Description of c onsignment, to include unique ID \nwhen available  \n Identification of responsible government and/or \ncompany representatives  \n Identification of commercial entities to be involved in \neach shipment  \n Packaging the consignment  \n Routing of the consignment  \n Couriers/escorts  \n Recipient responsibilities  \n Return of material procedures  \n Other information as required  \nProvide k eys and configuration \ndata  The program should document assembly house receipt \nof data packages and the hash value of the packages.  \nClear memory devices  The program should document the company, location, \nindividual , and method for clearing the contents along \nwith the contents before and after clearing.  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   84 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nTD 4 mitigation s Documentation  requirements  \nProvision private keys  The program sh ould document:  \n The company name, location , and date of \nprovisioning  \n The number of provisioned devices  and number of \nunique keys used  \n Proof of DSCA facility classification  \n Proof of DMEA Trust Cat egory  I certification   \nProtect the configuration data \npackage  The program should maintain data r eceipt \ndocumentation from each of the assembly and test \nteams showing each team either collected the data \nfrom a central repository or received it from a trusted \ntransfer mechanism.  \nPerform v erification activities  The program should maintain documentation  including \nthe procedures used to verify the PCB traces, where \nthe work was performed, when it was performed , and \nthe results of the verification.  \nThe program should maintain documentation including \nthe procedures used to authenticate the configuration \ndata, where the work was performed, who performed it, \nwhen it was performed , and the results of the \nverification.  \nThe program should maintain documentation including \nthe authentication methodology, its architecture , and its \ncompliance with appropriate NIST st andards.  \nThe program should maintain documentation including \nthe methodology used to verify the proper keys were \nloaded, where the work was performed, when it was \nperformed , and who performed the work.  \nThe program should maintain documentation including \nthe procedures used to authenticate the post assembly \nFPGA device, where the authentication was performed, \nby whom, when , and the results of the verification.  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   85 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nTD 4 mitigation s Documentation  requirements  \nAuthenticate the FPGA device by choosing  one option:  \nOption 1:  Verify the unique \ncryptographic ID   The program should document:  \n The a uthenticity verification method  \n The verification outcomes  \n The individual name or reference ID who performed \nthe verification  \nOption 2:  Verify the device on the \nPCB  The program should document:  \n The a uthenticit y verification method  \n The verification outcomes  \n The individual name or reference ID who performed \nthe verification  \nOption 3:  Use a soft PUF  The program should document:   \n The a uthenticity verification method  \n The verification outcomes  \n The individ ual name or reference ID who performed \nthe verification  \nChecklist for TD  5: Adversary compromises third -party soft IP  \nTD 5 mitigation s Documentation requirements  \nPurchase from DoD authorized  \nvendor s and distributors   The program should  document the name and location \nof the authorized vendor along with documentation \ndemonstrating that the vendor is authorized.  \nOnly a ccept IP that is unobfusca ted The program should keep a copy of the clean \nunobfuscated code, along with the name and or ID of \nthe person who received it.  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   86 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nTD 5 mitigation s Documentation requirements  \nEnsure IP deliverable packages are \ndigitally signed  The program should maintain documentation of th e \nvendor provided signature and hash, and the actual \nsoftware hash.  \nValidate  the cryptographic hash  The program should document the value of t he \ncalculated cryptographic hash and the signed hash \nprovided by the vendor along with the software name, \nversion , and release number.  \nStore IP  in a revision control  \nrepository   The program should include the initial IP and hash \ncheck -in within the system . \nExamine IP for malicious functions  The program should document all results in \naccordance with Third-Party IP Review Process for \nLevel of Assurance 3 . This document is available upon \nrequest.  \nAll interaction with JFAC  regar ding IP for malicious \nfunction s should be documented.  \nTo examine the IP for malicious functions , chose one of the following options:  \nOption 1:  At least two cleared \npersonnel  review the IP  The program should maintain documentation specific to \nthat identified in the Third-Party IP R eview Process for \nLevel of Assurance 3 . \nOption 2:  Contact JFAC to \ndetermine if an IP review of the \ncomplete IP package has been \npreviously  completed  The program should maintain documentation of \ncorrespondence between the program and JFAC . This \nshould incl ude information about the IP, system the IP \nis used in, and the role that IP serves within that \nsystem , along with proof of receipt from JFAC . \nThe program should obtain and review evidence of IP \nverification, including requirements sign -off. \nNote:  This act ivity is intended to both provide \nconfidence that the 3PIP will meet program \nspecifications and that functionality not utilized by the \ndeveloper, including testability, is understood by the \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   87 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nTD 5 mitigation s Documentation requirements  \nprogram. Data should be created and collected by the \nIP developer.  \nChecklist for TD  6: Adversary swaps configuration file on target  \nTD 6 mitigation s Documentation requirements  \nIncorporate c ryptographic \nauthentication  The program should document:  \n The method used to authenticate the configuration file \non load.  \n The verification process used to test the \nauthentication method.  \nAuthenticate  configuration data \neach time the data  is loaded  For each configuration load method used , the program \nshould document the method used to authenticate the \nconfiguration file on load , and the verification process \nused to test the authentication method.  \nPrevent direct read back  The program should document the steps taken to \nprevent direct read back of private keys.  \nUse a CNSS/ NIST approved \nalgorithm and key length  The program should  document the key length being \nused along with the version number of the latest CNSS \nor NIST FIPS guidance  approved key length.  \nUse DoD evaluat ed authentication \nmechanisms   The program should maintain documentation from \nJFAC with the security evaluation r esults.  \nDisable test access pins   The program should maintain documentation including \nthe means by which the JTAG test pins were disabled.  \nEnsure authentication for \nmodifications  Document if the FPGA allows application change s, how \nthe vendor states  authentication will apply to all \nreconfiguration data,  and test results indicating how \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   88 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nTD 6 mitigation s Documentation requirements  \nauthentication was actually applied to all \nreconfiguration data.  \nAlways program security settings \nin non -volatile storage of the \ndevice  The program should maintain document ation including \nthe means used to set security settings.  \nWhen a platform supports remote updates , chose one of the following options:  \nOption 1:  Validate that the built -\nin application change technique \nfully applies authentication to all \nthe reconfiguratio n data  The program should maintain documentation including \nthe test used to validate the application update \nmethodology and the outcome.  \nOption 2:  Perform authentication \nof the reconfig uration data in the \napplication  The program should maintain documentat ion including \nthe methodology used to perform authentication in the \napplication using partial reconfiguration.  \nUse a FIPS compliant 140-2 Level \n2 HSM   Document how the program utilizes FIPS 140 -2. \nDocument the HSM that is being used and the spec \nsheet dem onstrating FIPS compliance.  \nChecklist for TD  7: Adversary substitutes modified FPGA software \ndesign suite  \nTD 7 mitigation s Documentation requirement  \nPurchase from DoD authorized \nvendors and distributors  The program should  document the name and location \nof the authorized vendor along with documentation \ndemonstrating that the vendor is authorized.  \nPrevent a utomatic tool updates  The program should document, maintain , and follow \nthe SSP.  \nUse a t rusted computing \nenvironment  The program should maintain docume ntation and audit \ndata demonstrating one of the following computing \nenvironment s was us ed:  \n A computer and network classified at the DSCA \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   89 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nTD 7 mitigation s Documentation requirement  \nSecret  level or above. The documentation should \ninclude a log of personnel with clearance information, \nall records i n accordance with a maintaining a DSCA \nSecret  network , as well as a documented and SSP.  \n A computer and network certified for use in a Trust \nCategory 1 facility as defined by DMEA.  \n A network -isolated computer enclave with limited and \ncontrolled access , adhering to NIST and CMMC \nstandards . \nUse cleared personnel  In writing, t he program should designate work that mus t \nbe done by cleared Individuals . The program should \nkeep a log of personnel assigned to that work along \nwith their clearance level.  \nThe prog ram should maintain a l ist of the members \ncomprising  each team, with their clearance level s. The \nprogram should  maintain audit logs demonstrating what \neach team member accessed.  \nValidate the c ryptographic hash  The program should maintain the value of the \ncalculated hash and the hash that is provided by the \nvendor , along with the version/r elease number and \ndate/ timestamp.  \nTo validate the tool output,  choose one of the following options : \nOption 1:  Perform a logical \nequivalency check  Document  all code that was review ed using LEC, any \nfunctional discrepancies , and how those discrepancies \nwere resolved.  \nOption 2:  Use a reproducible b uild \nprocess  Document the reproducible build pr ocess and results \nvalidating that the  two separate build s produce d the \nsame bina ry and hash.  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   90 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nChecklist for TD  8: Adversary modifies FPGA platform family at \ndesign  \nTD 8 mitigation s Documentation requirements  \nEngage JFAC  The program should maintain a copy of the data sent to \nJFAC with a date/timestamp of when it was sent and \nan acknow ledgement of when it was received.  \nChecklist for TD  9: Adversary compromises single -board computing \nsystem (SBCS)  \nTD 9 mitigation s Documentation requirement  \nEngage a DoD vendor  to build the \nSBCS  The DoD vendor  should provide functionality and \nproduct sp ecifications.  \nVerification and authenticat ion The program should maintain a li st of the members \ncomprising the independent verification  team, with their \nclearance level s. The program should  maintain audit \nlogs demonstrating what each team member accessed , \nwhen and what reviews were conducted, and each \ndevice that was verified . \nAuthenticate the FPGA d evice s  The program should document the physical inspection \nresults for each slash sheet and unique ID for the \ndevice inspected.  \nVerify  the SBCS configuratio n \nprocess   Document the SBCS configuration process and how it \ncomplies with the LoA3  mitigation requirements for TD \n6: Adversary swaps configuration file on target . This \nincludes , but is not limited to , requirement s for:  \n NIST comp liant authentication algorithms   \n Differential p ower analysis  (DPA) resistant \nauthentication  \n Protected key storage  \n Anti-tamper de tection and response   \n Being f ree of known vulnerabilities in the \nconfi guration and security functions   \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   91 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nTD 9 mitigation s Documentation requirement  \n All enc ryption and authentication keys lengths \nmust be compliant with the requi rements \noutlined NIST SP 800 -57  \n The ability to disa ble FPGA test pins, such as \nJTAG   \nIf the configuration file memory storage device contains \nSBCS vendor code, the program should revi ew and \nevaluate that code for malicious function s, and \ndocument how the review was conducted and any \nfindings.  The proprietary SBCS support for \nconfiguration must be fully understood and validated.  If \nthe SBCS configuration process cannot be fully \nevaluate d, it should not be used at LoA3.  \nOnce the SBCSs configuration design and \nimplementation are evaluated to be free of malicious \nfunction s, the program should craft a set of tests and \nvalidation processes to verify that all the devices \ncomply with the evalu ation.  The program should \ndocument the tests and validation processes along with \nthe validation of all devices.  \nTest non-volatile memory   The program should maintain documentation including \nthe FPGA settings available in the given FPGA device, \nthe methodo logy used to read them, where they were \ntested, by whom, when and the results.  \nDocument the steps  Document the steps taken to comply with these \nrequirements. This includes all hardware and software \nthat were authenticated and verified. All associated \ndata artifacts should be auditable.  \n\n \nU/OO/ 170671 -23 | PP-23-1734  | JUN 2023 Ver. 1.0   92 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 3 Best Practices  \nChecklist for TD  10: Adversary modifies vendor FPGA software \ndesign suite during development  \nTD 10 mitigation s Documentation requirement  \nPerform all FPGA design work on \nan isolated network  Provide documentation in alignme nt with Checklist  for \nTD 3: Adversary compromises application design cycle . \nPerform logical equivalency \nchecking  The program should document any hints, all \noptimizations, and rationale for any logic that did not \nmatch the equivalency checker with managerial \napproval signature.  \n \n\n",
  "cves": [],
  "techniques": [],
  "advisory": "cybersecurity-alerts",
  "title": "ctr_dod_microelectronics_fpga_loa3_best_practices",
  "source": "nsa",
  "id": "443c5b787478086fd8b37fc77a1a06f2dd37d2ccfaf9b769106294b356a4a4f5"
}