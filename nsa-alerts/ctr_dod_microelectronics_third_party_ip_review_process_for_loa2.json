{
  "markdown": " \n \n \n \nNational Security Agency  \nCybersecurity  Technical Report  \n \n \n \n \n \nDoD Microelectronics:  \nThird -Party IP Review Process for  \nLevel of Assurance 2 \n \n \n \nFebruary  2023 \n \nU/OO/ 12090 9-23 \nPP-23-0200  \nVersion 1.0 \n \n \n \n \n  \n  \n\n\n \n \nU/OO/ 120909 -23 | PP-23-0200  | FEB 2023 Ver. 1.0  ii \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA2  \n \n \nFor additional information, guidance or assistance with this document, \nplease contact the Joint Federated Assurance Center (JFAC) at \nhttps://jfac.navy.mil . \n  \n\n\n \n \nU/OO/ 120909 -23 | PP-23-0200  | FEB 2023 Ver. 1.0  iii \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA2  \nNotices and history   \nDocument change history  \nDate  Version  Description  \nFEB 2023  1.0 Initial Publication  \n   \nDisclaimer of warranties and endorsement  \nThe information and opinions containe d in this document are provided \"as is\" and without any warranties \nor guarantees. Reference herein to any specific commercial products, process, or service by trade name, \ntrademark, manufa cturer, or otherwise, does not  constitute or imply its endorsement, recommendation, or \nfavoring by the United States Government, and this guidance shall not be used for advertising or product \nendorsement purposes.  \nPublication information  \nAuthor(s)   \nNational Security Agency  \nCybersecurity Directorate  \nJoint Federated Assurance Center  \nContact information  \nJoint Federated Assurance Center: https://jfac.navy.mil  \n \nDefense Industrial Base Inquiries and Cybersecurity Services: DIB_Defense@cyber.nsa.gov  \n \nMedia inquiries / Press Desk: Media Relations, 443-634-0721, MediaRelations@nsa.gov  \nPurpose  \nThis document was developed in furtherance of NSAs cybersecurity missions . This  includ es its \nresponsibilities to identify and disseminate threats to National Security Systems, Department of Defense \ninformation systems, and the Defense Industrial Base, and to develop and issue cybersecurity \nspecifications and mitigations. This information may be shared broadly to reach all appropriate \nstakeholders.   \n\n \n \nU/OO/ 120909 -23 | PP-23-0200  | FEB 2023 Ver. 1.0  iv \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA2  \nExecutive summary  \nThis report outlines a methodology of design review su fficient to validate that third -party  \nintellectual property (3PIP) is appropriate to incorporate i nto a Level of Assurance 2 \n(LoA2 ) system.  \nIn this context , 3PIP refers to functions whose development are not under the control of \nthe designer.  Use of the phrase intellectual property, IP , or 3PIP in outlining this \nmethodology of design review does not refer to property rights , such as, for example, \ncopyrights, patents, or trade secrets. It is the responsibility of the party seeking review \nand/or the reviewer to ensure that any rights needed t o perform the review in \naccordance with the methodology outlined are obtained.  \nThis process is intended to mitigate the Threat  Description  #5 Adversary compromises \nthird-party soft IP, as described in the FPGA Best Practices  Threat Catalog . The \nprocess  attempts to minimize the required level of effort while providing sufficient \nmitigation for use of 3PIP at LoA2 . The review process, when correctly executed, \nestablishes an independent organization that an adversary would have to compromise \nor deceive for  an attack to succeed. This directly increases the level of access  required \nto carry out an attack. The process assumes that technology and level of effort are not \nsubstantial barriers to an adversary in modifying the 3PIP design.  \nAdditionally, at LoA2  only inherently targetable and high -utility attacks are of concern. \nAttacks that provide pre -positioning as part of a more complex attack to be executed in \nthe future are out of scope. This restriction bounds the steps of review that are required . \nHowever , should issues outside the scope of highly targetable attacks be found during \nthis review process, they must be reported as appropriate1 and the intellectual prope rty \n(IP) should not be approved for use in an LoA2  system . \nThis review process can be applie d in two distinct ways. One is to approve a 3PIP in \ngeneral. The second is to approve a 3PIP for use in a specific role within a specific \nsystem. For instance, this process can be used to review a memory controller IP for use \nin any application. In some ci rcumstances, it may be less expensive to review that same \n                                                \n1 Appropriate action will depend on the specific discovery. Coding errors that appear innocent may req uire cybersecurity reporting, and in \ngeneral should be reported to the IP vendor. Apparently intentional backdoors should be reported to the Federal Bureau of Investigations (FBI) or \nthe National Intellectual Property Rights Coordination Center. JFAC is av ailable for consultation should the correct action be unclear.  \n\n \n \nU/OO/ 120909 -23 | PP-23-0200  | FEB 2023 Ver. 1.0  v \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA2  \nIP, but only for use in a single application. In this case, information about the system \naround the IP might be used to avoid reviewing certain components in detail.  \nThe JFAC FPGA Level of Assuranc e 2 Best Practices  appendix specifies this flow as \none option to assure 3PIP. In most cases that will correspond to review within a certain \nsystem. The other mitigation offered is to provide the 3PIP to the JFAC. When JFAC \nelects to review the 3PIP, it wil l be a review for any system context.  \nThis review process is summarized  below : \n Assemble review prerequisites  \n Establish suitability of the 3PIP for review  \n Partition the design  \n Perform manual code review  \n Perform test -driven code review  \n Document and sign the review package  \n \n  \n\n \n \nU/OO/ 120909 -23 | PP-23-0200  | FEB 2023 Ver. 1.0  vi \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA2  \nContents   \n \nDoD Microelectronics: Third -Party IP Review Process for Level of Assurance \n2 ................................ ................................ ................................ ................................ ........................ i \nExecutive summary  ................................ ................................ ................................ ......................  iv \nContents  ................................ ................................ ................................ ................................ ...... vi \n1. Introduction  ................................ ................................ ................................ ............................  1 \n2. Assemble review prerequisites  ................................ ................................ ...........................  1 \n2.1 Reviewer documentation  ................................ ................................ ................................ ................................ . 1 \n2.2 Record of review  ................................ ................................ ................................ ................................ .................  2 \n3. Establish suitability of the 3PIP for review  ................................ ................................ ........  3 \n4. Partition the design  ................................ ................................ ................................ ...............  4 \n4.1 Th reats of interest  ................................ ................................ ................................ ................................ ..............  4 \n4.2 Controlled effects  ................................ ................................ ................................ ................................ ................  5 \n4.3 Persistent effects  ................................ ................................ ................................ ................................ ................  7 \n4.4 De fining a functional area  ................................ ................................ ................................ ...............................  8 \n4.5 Criteria for functional areas of interest  ................................ ................................ ................................ ....... 8 \n5. Perform manual code review  ................................ ................................ ...............................  9 \n6. Perform test -driven code review  ................................ ................................ .......................  10 \n7. Document and sign the review package  ................................ ................................ ..........  11 \n8. Pre-compiler and other machine -generated code  ................................ ...........................  12 \n9. Summary  ................................ ................................ ................................ ..............................  13 \nAppendix A: Standardized terminology  ................................ ................................ ...................  15 \n \n \n\n \n \nU/OO/ 120909 -23 | PP-23-0200  | FEB 2023 Ver. 1.0  1 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA2  \n1. Introduction  \nThis report outlines a m ethodology  to perform a design review that is sufficient to \nvalidate a third-party intellectual property (3PIP) function for use within a Level of \nAssurance 2 (LoA2 ) system.  In this context, 3PIP refers to functions whose \ndevelopment are not under the control of the designer.  Use of the phrase intellectual \nproperty, IP , or 3PIP in outlining this methodology of design review does not refer to \nproperty rights , such as, for example, copyrights, patents, or trade secrets. It is the \nresponsibility of the party seeking review and/or the reviewer to ensure that any rights \nneeded to perform the review in accordance with the methodology outlined are \nobtained . \n2. Assemble review p rerequisites  \nFor a 3PIP  review to be valid, the review itself must be thorough and auditable. \nProcedural elements related to the following must be in place for the duration of the \nreview : \n A clear record of who is conducting the review, and  \n The review process itself.  \n2.1 Reviewer documentation  \nA clear record of the individuals conducting the review must be maintained. This record \nmust justify why the individuals are well suited to the task. The information required is \nsimilar to the information collected by a company during the hiring process, such as \nidentification information and a resume. The following details must be included : \n Security suitability   For LoA2  3PIP  reviews , the individuals doing the review \nmust be U.S. persons. The organization performing the review must maintain \ndocumentation proving that status.  \n Qualifications  Each reviewer must have a suitable degree and expe rience \nwith the design language or circuit type they review. For instance, a reviewer of \nVerilog code must have an understanding of digital design typically acquired with \na degree in computer or electrical engineering and experience writing Verilog. \nLikewi se, a reviewer of Very High Speed Integrated Circuit Hardware Description \nLanguage  (VHDL ) must similarly have experience with VHDL. Furthermore, if the \n3PIP incorporates hard IP blocks, specific to the Field Programmable Gate Array \n\n \n \nU/OO/ 120909 -23 | PP-23-0200  | FEB 2023 Ver. 1.0  2 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA2  \n(FPGA ) platform, the rev iewer must have experience with platforms from that \nvendor2. \n Independence   No reviewer should have worked on the development of the \n3PIP. No reviewer may be a current employee of, or contractor to, the company \nthat designed or sells the 3PIP.  \n2.2 Record o f review   \nThe review process itself must be auditable such that each decision is traceable to at \nleast two specific reviewers. In addition, time spent conducting the review must be \nrecorded for each decision. The time expected to complete the review will vary greatl y \ndepending on block complexity. The specific information to gather and document is \nsimilar to the information collected automatically by many  code review tools. \nSpecifically, for each artifact reviewed , the following must be maintained:  \n Identity of the re viewer   The name, or other ide ntifier, of the individual who \nperformed the review.  \n Times of review   A record describing the day or days on which the review took \nplace, as well as the duration of the review . \n Specific material reviewed   A record of the sp ecific design files reviewed to \nmake the determination.  \n Tests run during the review   A rec ord of any functional tests run by the \nreviewers, including test  benches, scripts, and other inputs necessary to replicate \nthem.  \n Summary   A record explaining the decision made, and the facts that led to that \ndetermination.  \nIn addition , this documentation must record how the IP was received, how it was stored , \nand how it was  distributed to the reviewers. A process should be put in place to  ensure \nthat all reviewers receive the same versions of each file.  \nThis record of review should be  stored in the revision control system , but does not need \nto be distributed as part of the final signed review package.  \n                                                \n2 For example, if a 3PIP makes explicit use of a multiply accumulate block in an Intel FPGA the reviewer must have experience with Intel FPGAs, o r previous \nrelated Altera FPGAs.  \n\n \n \nU/OO/ 120909 -23 | PP-23-0200  | FEB 2023 Ver. 1.0  3 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA2  \n3. Establish suitability  of the 3PIP for review  \nIP can only be reviewed if it is documented and developed using practices that cannot \nbe exploited to hide malicious changes. That means well -documented hardware \ndescription language ( HDL) with clean names and reasonable design. Human and \nsimulation  review of the IP are not effective outside of those conditions. This section \nintroduces a set of criteria that determines whether such practices are employed.  \nThis step determines if a review of the 3PIP is feasible in a way that will generate \nsufficient assurance. A single individual can conduct this process by reviewing the 3PIP \nand asking these guiding questions:  \n Is the 3PIP distributed in an encrypted form that prevents the reviewers from \nviewing it?  Is it distributed as a netlist3? \n If yes, the 3PIP is  not suitable for review. It  is not recommended for  this \nflow. \n Viewing the unencrypted code, or pre -synthesis code, is only sufficient \nwith additional steps to ensure equivalence of the distributed and reviewed \nfiles. If this is needed, contact JFAC for gu idance . \n Is the 3PIP obfuscated4?  \n If yes, the 3PIP is not suitable for review. It is not recommended for  this \nflow. JFAC can be contacted if the 3PIP is essential; however, the process \nto approve such a 3PIP will be expensive, time consuming, and likely to  \nreturn a negative result.  \n Is the 3PIP clearly organized, with distinct modules that are limited in scope with \ninputs/outputs (I/O) limited to those needed to achieve their scope?  \n If no, the 3PIP is not suitable for review. It  is not recommend for this flow. \n Is the 3PIP well documented? This should include comments describing the \npurpose of modules; clear, human -readable names; and top -level documentation \ndescribing the interface.  \n                                                \n3 Whether a 3PI P is distributed as a netlist is not determined by the file format, but rather the contents. For example, a Verilog file is c onsidered a netlist \ndistribution when a substantial amount of it is implemented with primitive or gate -level elements. This could e ither be primitives drawn from a library, such as a \nvendor specific primitive library, or primitives that implement individual gates, such as the and and or elements in Veri log. \n4 3PIP for which module names, signal names, or any other names, have been  replaced with non -meaningful alphanumeric codes or random words is considered \nto be obfuscated. Similarly, HDL that makes repeated use of complex Boolean operators, where it could have used complex funct ional commands, such as \nif/then blocks, is considere d obfuscated.  \n\n \n \nU/OO/ 120909 -23 | PP-23-0200  | FEB 2023 Ver. 1.0  4 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA2  \n If no, the 3PIP is not suitable for review. It is not recommended for this \nflow. \n Does the 3PIP feature parameterized code and pre -compiler directives where \nparameters are used  to explicitly generate new HDL5 that is then added to the \nsystem? Is sof tware used to write HDL based on designer input? This includes \nexplicit code that w rites code, uses terms like ifdef in Verilog, and uses the term \ngenerate in VHDL. This includes parameterized IPs that can be configured via  \nGUI or script.  \n If yes, the 3PIP is still suitable for review. Be aware that the review \nprocess will be longer.  \n Does the 3PIP have a substantial number of tests distributed with it?  \n If no, the 3PIP is still suitable for review. Be aware that the review process \nwill be longer.  \n4. Partition the design   \nOnce review suitability is established, the team reviews the overall  design of the 3PIP \nand makes a plan to split the 3PIP into one or more functional units that can be \nreviewed and tested independently. This number can vary greatly depending on the \nscale of the 3PIP being reviewed. For some simple 3PIP modules, there may be no \nclear partitioning of the code into meaningful , functional modules. For others, there may \nbe a clear top -level design that quickly yields multiple functional modules.  \n4.1 Threats of interest  \nBecause this process is spe cifically intended to catch LoA2  threats, the review team \nshould begin with analyzing targetability. At LoA2, this mean s high utility threat \noperations are  executed in a way that provides straightforward means to understand \nand predict the effect of an attack and provide a mechanism to c ontrol or time the \nattack. For example, an adversary with the ability to introduce new code into a system \ndesign can implement a broad number of malicious functions. A denial of service attack \nfalls in this category if and only if it is possible for the ad versary to control when it takes \neffect after the device is fielded. However, a simple reduction in reliability not tied to any \n                                                \n5 Generator code that exists solely to add additional logging for debug should not be counted as generator code for these purpo ses. Generator code added to \nmake a simulation work must be reviewed carefully in the process below, but should not  count as generator code for this purpose.  \n\n \n \nU/OO/ 120909 -23 | PP-23-0200  | FEB 2023 Ver. 1.0  5 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA2  \ntrigger, which therefore cannot  be controlled or timed in a controlled way, does not fall in \nthis category.  \nThe focus of this review is to look for malicious modifications to 3PIP. Because 3PIP \ncan have many origins , as well as many distribution methods, an adversary has many \noptions. In particular, an adversary can modify one copy of 3PIP destined for a \nparticula r target. This means that an in -depth 3PIP review is required.  \nThe JFAC best practice guides  do not specify what compromises are acceptable ; only \nthe scope of search that is required. If any flaw, intentional or otherwise, is identified , it \nshould be repor ted and may render a 3PIP unsuitable for use.  \nNote : While  these guidelines considerably limit the scope of attacks that reviewers must \nlook fo r, they do not limit the scope of attacks that must be reported if discovered.  \nHowever, there are some limits to the level of review required that enable specific sub -\nportions of a 3PIP design as of interest for review. These attacks are divided into two \nclasses: controlled effects and persistent effects.  \n4.2 Controlled effects  \nA controlled effect is just that: a specific control mechanism, coupled to a specific effect. \nIn more detail, such an attack contains both of the following : \n A means of command and control   The attack must be externally controlled \nthrough some pre -existing or additiona l control mechanism. Wit hin LoA2 , assume \nthat such an effect must not occur during acceptance testing or typical use of the \ndevice. To achieve this, a trigger must either be sufficiently long and unstructured \nor represent an out -of-spec behavior in a defined protocol.  \n As a rule o f thumb, unstructured triggers of interest must contain, at a \nminimum, 32 bits of unstructured data. Logic to recognize such a \nsequence could be implemented many ways, but any implementation \nwould contain at least 5 bits of state, as well as substantial ad ditional logic \ncomprised of combinatorial and state elements.  \n Structured triggers of interest must occur within the context of a specific \ndefined protocol, and cannot simply represent any out -of-spec behavior. \nFor instance, an incorrect cyclic redundancy c heck (CRC ) field is overly \n\n \n \nU/OO/ 120909 -23 | PP-23-0200  | FEB 2023 Ver. 1.0  6 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA2  \nbroad, as many systems will encounter incorre ct CRC  fields. However, a \nspecific bit error pattern within a CRC field could be sufficient.  \n A useful effect   Within LoA2 , useful effects can include specific actions on the \npart of the system, denial of service, leak of data, and so on. The presence of \ndenial of service in this list means that most any functional area in a 3PIP design \nwill cause a useful effect . \nExample tha t meets the criteria  of a controlled effect : A modification to a sub -\nblock of a 3PIP Ethernet filter waits for a specific magic packet before \ndeactivating the Ethernet entirely until the system is reset.   \nThis attack is controllable:  the adversary knows th e protocols pa rsed by the core at \ndesign time  and they can understand its role in a specific system. It is use ful: it would \nallow an adversary to deactivate an important function of the device on command. This \nis an example of a controlled effect that woul d be of interest.  \nSecond e xample that meets the criteria  of a controlled effect : A modification to a \nprogrammable filter ( e.g., a finite i mpul se response filter) is added that waits for a \nspecific input pattern, after which it causes the filter to output o nly 0 . \nThis attack is controllable if the adversary understands the role of the filter in the system \nto process external data. For example, in a sensor application where data is \npreprocessed by a filter before being passed on to other parts of the system. While an \nadversary cannot control the exact values of the inputs in many such applications, they \ncan control patterns in those values that can be detected by their additional code. In a \nsensor application such as this, this modification could give an adver sary the means to \ndeactiv ate a sensor, which is a denial of service attack.   \nExample that meets  the criteria  of a controlled effect  for LoA2 : A modification to a \nmicrocontroller core deactivates memory protection when a specific sequence of \ninstructions is  run. \nThis attack meet s the criteria  at LoA2 , as this would be considered pre positioning for an \nattack. If this is found, it is assumed there is also a secondary trojan able to take \nadvantage of the memory deactivation and execute alternative code. If this  is \ndiscovered, it must be reported and would represent a negative finding.  \n\n \n \nU/OO/ 120909 -23 | PP-23-0200  | FEB 2023 Ver. 1.0  7 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA2  \n4.3 Persistent effects  \nIn contrast to a controlled effect, a persistent effect lacks an external activation \nmechanism. Instead, it is always or regularly present. Persistent attacks against FPGA \n3PIP are constrained principally by testing performed during development and ordinary \nuse of the device. To be useful , they must evade detection during functional testing \nconducted by the designer and cannot immediately interfere with use of the system. \nThis is entirely feasible depending on the test methodology chosen by the system \ndesigners and the 3PIP block itself.  \nFor this review, the reviewers may assume that two categories of tests  are run on the \n3PIP. The first is the set of any  tests or exemplar use cases distributed with the 3PIP \nitself. The second is a system -level functional test performed to validate that a complete \nsystem using the 3PIP successfully achieves a s ystem -level outcome. When reviewing \na 3PIP block for a specific  program, rather than for general use, the team may also take \nas an input any additional tests developed for the block itself. However, the team should  \nnot assume that the 3PIP developer ran additional tests not shared with the review \nteam.  \nExample that me ets the criteria: A multipoint bus 3PIP provides direct access \nfrom the Joint Test Action Group (JTAG) port of the FPGA to the bus.   \nThis attack is specifically useful, giving the adversary access to arbitrary data on the \nbus, which could easily include critical or classified data. It would be expect ed to be \nquickly discovered if placed into a widely distributed piece of 3PIP as soon as  a user \ntried to use the JTAG port themselves. However, the attack being mitigated by this \napproach is specifically looking at the possibility that a vendor receives a special version \ntailored just for them. An adversary in this scenario may have identifie d that JTAG is \nunused in a given system. This attack also could be detected by extensive testing and \nevaluation of the fina l system. As stated above, reviewers should assume that no test \nwould have involved JTAG if the system was not implementing JTAG inte ntionally.  \nExample that fails to meet the criteria: A modification to a PCI -E engine causes it \nto degrade  system performance . \nThis does not meet the criteria  for LoA2 . Such a modification has no controlled effect, \nbut instead degrades the performance at un known times that are not necessarily useful \nto the adversary. As a resul t, this is not a concern at this assurance level . \n\n \n \nU/OO/ 120909 -23 | PP-23-0200  | FEB 2023 Ver. 1.0  8 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA2  \n4.4 Defining a functional a rea \nThe remainder of this report discusses how to evaluate each functional area. To do this , \nthe review tea m must determine the functional area boundaries they want to use. A \nfunctional area is a set of circuit designs that have a specific documented purpose. \nEach functional area should  be reviewed and tested independently. The evaluators can \nset the exact scal e of these functional areas. However, as is specified below, depending \non the design practices employed , the process may necessitate that functional areas be \ncombined into larger functional areas for evaluation, or generate considerably more \nwork bouncing between regions.  \nThe review team should make a first attempt at defining functional areas based on the \nfile structure, module documentation, and past experience. In general, small functional \nareas will be easier to evaluate. Design practices that limit the connections to a \nfunctional area to those needed to perform its function will enable small functional \nareas. In practice, design best practices encourage limiting connectivity between \nmodules and thus this process should be straightforward.  \n4.5 Criteria for functional areas  of interest  \nBased on the effe cts described at  LoA2, the following questions should b e used to \ndetermine whether a functional area is inherently of interest and what level of review is \nrequired. For example, review for controlled effects is much simpler tha n review for \npersistent effects:  \n Does the functional area have I/O s that an adversar y will be able to influence? \nWhen evaluating a 3PIP for a specific system, system knowledge can be used. \nWhen evaluating a 3PIP block in general, almost all I/O to the 3PIP block meet \nthis criteria .6 Additionally, inputs that are indirectly connected to to p-level I/O \nthrough other modules must be reviewed.  \n If so, it is a functional area of interest for controlled effects. All avenues \nfrom external stimuli that may affec t the device during normal use  should \nbe noted for investigation.  \n                                                \n6 In certain cases it is possible to rule out external connections and influence when clear best practices would prevent this b ehavior. Specifically, in the case of \nJTAG and other test infrastructure, the final wr ite-up should simply note that these must not be exposed remotely to an adversary. They do not need to be \nreviewed as remote connections, as any system where an adversary could access them is compromised by definition.  \n\n \n \nU/OO/ 120909 -23 | PP-23-0200  | FEB 2023 Ver. 1.0  9 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA2  \n Does the functional are a have internal behaviors that serve a well -defined role in \nthe system, but could fail in wa ys that would not be apparent during  normal use ?7 \n If so, it is a functional area of interest for persistent effects. The specific \ndifficult to test internal behav iors should be noted for investigation.  \n Does the functional area have a well -constrained set of I/Os that limit it to \ninformation relevant to its intended function?  \n If not, it needs to be re -evaluated in combination with the other functional \nareas connecte d to it as a single , larger functional area.  \n5. Perform m anual code r eview  \nEach region requires two kinds of reviews: manual and test -driven. These two reviews \nmay influence each other and do not need to be completed in a particular sequence. A \nmanual review is a traditional code review where knowledgeable designers read the \ncode to evaluate it for correctness.  \nBefore beginning manual code review , automatic tools should be used to generate lines \nof interest within the code. Both a lint tool and the synthesis  process must be run. A lint \ntool can be run with settings derived from the apparent code standard used in \ndevelopment of the source. Any lines identified by the lint code should be marked for \nextra review during the review process. Additionally, the 3PIP should be synthesized \nwith an appropriate FPGA synthesis tool. Any warnings generated should be marked for \nextra review.  \nEach functional area of interest must be reviewed independently. For functional areas \nthat are inspected for a controlled effect, this review should focus on looking at the data \npath for unexpected behaviors or unexpected connections from the functional area to \nother functional areas. For persistent effects, the specific hard -to-test function should be \nevaluated in depth for correctness.  \nFor assessing a controllable effect, the following factors must be evaluated : \n Where does the information from the I/O propagate?  \n                                                \n7 An example of such a region would b e encryption on an internal communications bus. If the bus is responsible for encryption at one end, and decryption on the \nother end, no normal user of the bus would necessarily notice an error.  \n\n \n \nU/OO/ 120909 -23 | PP-23-0200  | FEB 2023 Ver. 1.0  10 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA2  \n If it propagates largely intact or is transformed in a simple way to other \nfunctional modules, then they must be added as func tional areas of \ninterest.  \n What logic handles the I/O? The logic within the functional area that processes \nthe I/O must be evaluated for signs of a trigger : \n If the logic is simple , then accounting for the state elements can be \nsufficient to remove it from c onsideration. This is possible when many bits \nof previous input are not stored natively within the functional area. In these \ncases, that storage, or a finite state machine  (FSM) , would need to be \nadded in order to add a trigger.  \n If the logic is complex, it will require additional investigation : \n One phase of the investigation should focus on reviewing the code \nfor an unexpected, but explicit trigger. That is to say, specific lines \nof HDL or specific groups of gates, waiting for a specifi c command.  \n The second phase of the investigation should focus on more subtle \neffects. In particular, this should look at each FSM  state not \ncovered by a test, and validate that it has a well -documented and \nmeaningful purpose. Special attention is needed if  multiple \ninteracting FSMs  are present. In those cases, additional in -depth \nconsideration must be paid to the interactions between FSMs . \nParticular attention must be given to out -of-spec behavior in known \nprotocols.  \nFor investigating a persistent effect, t he process is more complex. Pote ntial persistent \neffects at LoA2  are rare. However, appropriate experts must determine whether each \nsuch block is properly implemented. They must focus on the correctness and suitability \nof the implementation, with the entir e implementation being carefully reviewed. Review \nof persistent effects is more dependent on the test-driven code review.  \n6. Perform t est-driven code r eview  \nAs part of the review process, simulation tests must be used. The simulation testing can \nbe performed  at the top 3PIP level, the functional module level, or a mix of the two. With \nline-by-line code review, the test  benches that were provided with the 3PIP may be \nused as part of these tests.  JFAC will provide g uidance for blocks too large to efficiently \nsimulate.  \n\n \n \nU/OO/ 120909 -23 | PP-23-0200  | FEB 2023 Ver. 1.0  11 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA2  \nFor regions that are being investigated for a controlled effect, sufficient tests must be \ndeveloped such that all major desired functions are executed. In some cases, this may \ncorrespond to a requirements document or to the functions listed in a sp ecification. In \naddition to that, code coverage8 must be recorded and any lines not included in the final \ncode coverage must be reviewed in detail, with a specific note identifying why they are \nnot of interest in the context of their functional module.  \nFor regions that are being investigated for a persistent effect, sufficient tests must be \ndeveloped such that all major desired functions are executed. In addition to this, \nconstrained random testing must be performed on the 3PIP. Code coverage must be \nrecord ed. In some cases , such as redundant serial safety checks, it may be impossible \nto achieve 100% code coverage. For each line not covered in the final code coverage, a \nspecific note must be recorded  identifying why a test  bench did not reach that line and \nwhy it is acceptable in the context of the larger system.  \nIn both cases, formal methods that prove equivalency between another reference \ndesign that has been vetted through this process and a 3PIP or functional module can \nsubstitute for the use of testing a s described above. Similarly reviewed software blocks \ncan also be used as a reference in this formal verification process.  \n7. Document  and sign the review p ackage  \nA review package should summarize the results of the review and minimally provide the \nfollowing  items : \n The name of the vendor who sells the 3PIP ; \n The name of the 3PIP ; \n Any version information of the 3PIP ; \n The hash of the 3PIP package delivered by the vendor9; \n A list of any use restrictions that were ident ified in the review of the 3PIP ; \n The hash of a  summary document detailing the review process, including all \nauditable information specified in the prerequisites section of this document ; \n The auditing organization that maintains  this summary document ; \n The portion of the summary document above not conta ining personally \nidentifiable information ; \n                                                \n8 For the purposes of this specific document, code coverage is measured by statement and branch coverage, not signal or gate.  \n9 This review will be valid solely for 3PIP packages that match this hash.  \n\n \n \nU/OO/ 120909 -23 | PP-23-0200  | FEB 2023 Ver. 1.0  12 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA2  \n The hash of all test benches and scripts used in the review process ; \n If allowed by the 3PIP vendor , the collection of the test benches and scripts used \nin the review process ; \n A summary hash of all the listed eleme nts above, including hashes of packages \nreviewed but not the package itself ; and  \n A cryptographic signature of the above information, tied to the reviewing \norganization.  \nThis set of information is a tradeoff designed to produce a distributable report not \nconstrained by agreements  that can be validated back to the specific files. The hashes \nin this report must be validated against the 3P IP for it to be used, and the report must \nbe maintained as a design artifact for future audits. For instance, explicitly inc lude a \nhash of the 3PIP, but not the 3PIP itself, so that other users can determine if they have \nthe same files without needing to include the files themselves in the document.  \nShould an issue be identified during the review process , it should be reported to the \norganization requesting the r eview.  In the case of Department of Defense  organizations, \nany identified issue must be shared with the JFAC.  \n8. Pre-compiler and other machine -generated c ode \nMachine -generated code refers to any code that takes a set of pa rameters and uses \nthem to construct or pre -process HDL before a synthesis process. This can be custom \nC code that generates Verilog. This can be compiler directives such as ifdef in Verilog \nor generate in VHDL.  \nReview of a 3PIP block containing machine -generated code can be substantially more \nchallenging, and, depending on the complexity of the pre -compiler directives or scripts, \nmay not be possible. Instead, it may require review with specific settings in place. \nContact JFAC for additional guidance  for handling machine -generated code.  \nWhen reviewing 3PIP for use in a specific system, it is acceptable to run the pre -\ncompiler with the desired parameters and evaluate the result. In this case, the \ngenerated HDL must comply with the same standards as specifi ed for any 3PIP block. \nIn addition, the review package must note those precise settings as a use restriction on \nthe 3PIP.  \n\n \n \nU/OO/ 120909 -23 | PP-23-0200  | FEB 2023 Ver. 1.0  13 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA2  \n9. Summary  \nFor review of a 3PIP in general, the following guidelines govern the review:  \n Assemble review p rerequisites:  \n In cases where sof tware is used to write HDL, members of the review \nteam that review the generator code must have experience in both the \nlanguage that the generator is written in and the language that is \ngenerated. For instance, to review C code that generates VHDL, the \nreviewer must have expertise in both.  \n Establish suitability of the 3PIP for review:  \n Are the purposes of all uses of pre -compiler directives or scripts clear and \nwell documented?  \n If not, the code is not suitable for review.  \n Are the parameters of the IP well documented with clear constraints?  \n If not, the code is not suitable to review.  \n Is the function of the 3PIP limited to a specific  function, rather than an \narbitrary programmable set of functions10? \n If so, the code is suitable to review.  \n Partition the design : \n Partitions must be made with specific regard to the boundaries of the pre -\ncompiler directives. The I/O between partitions should not dynamically \nchange based on parameters outside of width changes.  \n Perform manual  code r eview : \n Manual code review must be pe rformed with the pre -compiler directives in \nplace. The generator itself must be reviewed.  \n Perform test-driven code r eview : \n Simulations must be done on a representative sample of \nparameterizations.  \n All conditional code blocks within ifdef, generate, or similar statements \nmust be included in at least one of the samples simulated, though \nindividual lines within a block may be reviewed as in the normal flow.  \n Document  and sign the review package : \n List the specific details of all parameter sets used in the ev aluation.  \n                                                \n10 For example, a parameter specifying the length of a fi lter is not problematic. A parameter indicating whether a CPU has a JTAG port is not problematic. A \nparameter that specifies a complex equation or programmatic behavior to implement is problematic.  \n\n \n \nU/OO/ 120909 -23 | PP-23-0200  | FEB 2023 Ver. 1.0  14 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA2  \n Include this  data in the final summary hash.  \n  \n\n \n \nU/OO/ 120909 -23 | PP-23-0200  | FEB 2023 Ver. 1.0  15 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA2  \nAppendix  A: Standardized terminology  \nThe following terms are used in the Joint Federated Assurance Center Field \nProgrammable Gate Array Best Practices documents. These terms are modified from \nDefense  Acquisition University definitions to support common understanding.  \nApplication design   The collection of schematics, constraints, hardware description \nlanguage (HDL), and other implementation files developed to generate an FPGA \nconfiguration file for u se on one or many FPGA platforms.  \nApplication domain   This is the area of technology of the system itself, or a directly \nassociated area of technology. For instance, the system technology domain of a radar \nsystem implemented using FPGAs would be \"radar\" o r \"electronic warfare.\"  \nConfiguration file   The set of all data produced by the application design team and \nloaded into an FPGA to personalize it. Referred to by some designers as a bitstream, \nthe configuration file includes that information, as well as  additional configuration \nsettings and firmware, which some designers may not consider part of their bitstream.  \nControllable effect   Program -specific, triggerable function allowing the adversary to \nattack a specific target.  \nDevice/FPGA device   A specif ic physical instantiation of an FPGA.  \nExternal facility   An unclassified facility that is out of the control of the program or \ncontractor.  \nField programmable gate array (FPGA)   In this context FPGA includes the full range \nof devices containing substantial reprogrammable digital logic. This includes devices \nmarketed as FPGAs, complex programmable logic devices (CPLD), system -on-a-chip \n(SoC) FPGAs, as well as devices marketed as  SoCs and containing reprogrammable \ndigital logic capable of representing arbitrary functions. In addition, some FPGAs \nincorporate analog/mixed signal elements alongside substantial amounts of \nreprogrammable logic.  \nFPGA platform   An FPGA platform refers t o a specific device type or family of devices \nfrom a vendor.  \n\n \n \nU/OO/ 120909 -23 | PP-23-0200  | FEB 2023 Ver. 1.0  16 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA2  \nHard IP   Hard IP is a hardware design captured as a physical layout, intended to be \nintegrated into a hardware design in the layout process. Hard IP is most typically \ndistributed as Graphic Des ign System II (GDSII). In some cases, Hard IP is provided by \na fabrication company and the user of the IP does not have access to the full layout, but \nsimply a size and the information needed to connect to it. Hard IP may be distributed \nwith simulation har dware description language (HDL) and other soft components, but is \ndefined by the fact that the portion that ends up in the final hardware was defined by a \nphysical layout by the IP vendor.  \nLevel of assurance (LoA)   A Level of Assurance is an established guideline that \ndetails the appropriate mitigations necessary for the implementation given the impact to \nnational security associated with subversion of a specific system, without the need for \nsystem -by-system custo m evaluation.  \nPhysical unclonable function (PUF)   This function provides a random string of bits of \na predetermined length. In the context of FPGAs, the randomness of the bitstring is \nbased upon  variations in the silicon of the device due to manufacturing . These bitstrings \ncan be used for device IDs or keys.   \nPlatform design   The platform design is the set of design information that specifies \nthe FPGA platform, including physical layouts, code, etc.  \nSoft IP   Soft IP is a hardware design captured in hardw are description language \n(HDL), intended to be integrated into a complete hardware design through a synthesis \nprocess. Soft IP can be distributed in a number of ways, as functional HDL or a netlist \nspecified in HDL, encrypted or unencrypted.  \nSystem   An ag gregation of system elements and enabling system elements to achieve \na given purpose or provide a needed capability.  \nSystem design  System design is the set of information that defines the \nmanufacturing, behavior, and programming of a system. It may inclu de board designs, \nfirmware, software, FPGA configuration files, etc.  \nTarget  A target refers to a specific deployed instance of a given system, or a specific \nset of systems with a common design and function.  \n\n \n \nU/OO/ 120909 -23 | PP-23-0200  | FEB 2023 Ver. 1.0  17 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA2  \nTargetability  The degree to which an attack m ay have an effect that only shows up in \ncircumstances the adversary chooses. An attack that is poorly targetable would be more \nlikely to be discovered accidentally, have unintended consequences, or be found in \nstandard testing.  \nThird -party intellectual pro perty (3PIP)   Functions whose development are not \nunder the control of the designer. Use of the phrase intellectual property, IP, or 3PIP in \noutlining this methodology of design review does not refer to property rights, such as, \nfor example, copyrights,  patents, or trade secrets. It is the responsibility of the party \nseeking review and/or the reviewer to ensure that any rights needed to perform the \nreview in accordance with the methodology outlined are obtained.  \nThreat category  A threat category refers  to a part of the supply chain with a specific \nattack surface and set of common vulnerabilities against which many specific attacks \nmay be possible.  \nUtility  The utility of an attack is the degree to which an effect has value to an \nadversarial operation. Higher utility effects may subvert a system or provide major \ndenial of service effects. Lower utility attacks might degrade a capability to a limited \nextent.  \nVulnerability  A flaw in a software, firmware, hardware, or service component \nresulting from a w eakness that can be exploited, causing a negative impact to the \nconfidentiality, integrity, or availability of an impacted component or components.  \n\n",
  "cves": [],
  "techniques": [],
  "advisory": "cybersecurity-alerts",
  "title": "ctr_dod_microelectronics_third_party_ip_review_process_for_loa2",
  "source": "nsa",
  "id": "7d24e2bb75c46da73c30b6e8234a84345748ed71fa344243aef0370acdcc3425"
}