{
  "markdown": "Guidelines for secure AI \nsystem development\n\n\nMinisterstwo\nCyfryzacji\n\n\n03About this document  \nThis document is published by the UK National Cyber Security Centre (NCSC), the US Cybersecurity and \nInfrastructure Security Agency (CISA), and the following international partners:  \n >National Security Agency (NSA) \n >Federal Bureau of Investigations (FBI)  \n >Australian Signals Directorates Australian Cyber Security Centre (ACSC) \n >Canadian Centre for Cyber Security (CCCS) \n >New Zealand National Cyber Security Centre (NCSC-NZ) \n >Chiles Government CSIRT \n >National Cyber and Information Security Agency of the Czech Republic (NUKIB)\n >Information System Authority of Estonia (RIA)\n >National Cyber Security Centre of Estonia (NCSC-EE) \n >French Cybersecurity Agency (ANSSI) \n >Germanys Federal Office for Information Security (BSI) \n >Israeli National Cyber Directorate (INCD) \n >Italian National Cybersecurity Agency (ACN) \n >Japans National center of Incident readiness and Strategy for Cybersecurity (NISC) \n >Japans Secretariat of Science, Technology and Innovation Policy, Cabinet Office \n >Nigerias National Information Technology Development Agency (NITDA) \n >Norwegian National Cyber Security Centre (NCSC-NO) \n >Poland Ministry of Digital Affairs  \n >Polands NASK National Research Institute (NASK) \n >Republic of Korea National Intelligence Service (NIS)  \n >Cyber Security Agency of Singapore (CSA)\nAcknowledgements\nThe following organisations contributed to the development of these guidelines:  \n >Alan Turing Institute\n >Anthropic\n >Databricks\n >Georgetown Universitys Center for Security and Emerging Technology\n >Google\n >Google DeepMind\n >IBM\n >Imbue\n >Inflection\n >Microsoft\n >OpenAI\n >Palantir\n >RAND\n >Scale AI\n >Software Engineering Institute at Carnegie Mellon University\n >Stanford Center for AI Safety\n >Stanford Program on Geopolitics, Technology and Governance\nDisclaimer\n \nThe information in this document is provided as is by the NCSC and the authoring organisations who shall \nnot be liable for any loss, injury or damage of any kind caused by its use save as may be required by law. \nThe information in this document does not constitute or imply endorsement or recommendation of any \nthird party organisation, product, or service by the NCSC and authoring agencies. Links and references to \nwebsites and third party materials are provided for information only and do not represent endorsement or \nrecommendation of such resources over others.   \nThis document is made available on a TLP:CLEAR basis ( https://www.first.org/tlp/ ).Guidelines for secure AI system development\n\n04Guidelines for secure AI system development\nContents\nExecutive summary.................................................................................................................5\nIntroduction..................................................................................................................................6\nWhy is AI security different?..........................................................................................6 \nWho should read this document?.............................................................................7\nWho is responsible for developing secure AI?.................................................7\nGuidelines for secure AI system development....................................................8\n1. Secure design.......................................................................................................................9\n2. Secure development....................................................................................................12\n3. Secure deployment.......................................................................................................14\n4. Secure operation and maintenance................................................................16\nFurther reading..........................................................................................................................17\n\n05Guidelines for secure AI system development\nExecutive summary\nThis document recommends guidelines for providers of any systems that use artificial intelligence (AI), \nwhether those systems have been created from scratch or built on top of tools and services provided by \nothers. Implementing these guidelines will help providers build AI systems that function as intended,  are \navailable when needed, and work without revealing sensitive data to unauthorised parties.  \nThis document is aimed primarily at providers of AI systems who are using models hosted by an \norganisation, or are using external application programming interfaces (APIs). We urge all stakeholders \n(including data scientists, developers, managers, decision-makers and risk owners) to read these guidelines \nto help them make informed decisions about the design , development , deployment  and operation  of their \nAI systems.  \n \nAbout the guidelines\nAI systems have the potential to bring many benefits to society. However, for the opportunities of AI to b e \nfully realised, it must be developed, deployed and operated in a secure and responsible way.\nAI systems are subject to novel security vulnerabilities that need to be considered alongside standard \ncyber security threats. When the pace of development is high  as is the case with AI  security can often \nbe a secondary consideration. Security must be a core requirement, not just in the development phase, but \nthroughout the life cycle of the system.\nFor this reason, the guidelines are broken down into four key areas within the AI system development life \ncycle: secure design , secure development , secure deployment , and secure operation and maintenance . \nFor each section we suggest considerations and mitigations that will help reduce the overall risk to an \norganisational AI system development process. \n1. Secure design    \nThis section contains guidelines that apply to the design stage of the AI system development life cycle. It \ncovers understanding risks and threat modelling, as well as specific topics and trade-offs to consider on  \nsystem and model design.  \n2. Secure development    \nThis section contains guidelines that apply to the development stage of the AI system development life \ncycle, including supply chain security, documentation, and asset and technical debt management.  \n3. Secure deployment     \nThis section contains guidelines that apply to the deployment stage of the AI system development \nlife cycle, including protecting infrastructure and models from compromise, threat or loss, developing \nincident management processes, and responsible release.  \n4. Secure operation and maintenance    \nThis section contains guidelines that apply to the secure operation and maintenance stage of the AI \nsystem development life cycle. It provides guidelines on actions particularly relevant once a system has \nbeen deployed, including logging and monitoring, update management and information sharing.\nThe guidelines follow a secure by default approach, and are aligned closely to practices defined in the \nNCSCs Secure development and deployment guidance , NISTs Secure Software Development Framework , \nand  secure by design principles  published by CISA, the NCSC and international cyber agencies. They \nprioritise:\n >taking ownership of security outcomes for customers\n >embracing radical transparency and accountability\n >building organisational structure and leadership so secure by design is a top business priority\n\nIntroduction\nArtificial intelligence (AI) systems have the potential to bring many benefits to society. However, for the \nopportunities of AI to be fully realised, it must be developed, deployed and operated in a secure and \nresponsible way. Cyber security is a necessary precondition for the safety, resilience, privacy, fairness, \nefficacy and reliability of AI systems.\nHowever, AI systems are subject to novel security vulnerabilities that need to be considered alongside \nstandard cyber security threats. When the pace of development is high  as is the case with AI  security \ncan often be a secondary consideration. Security must be a core requirement, not just in the development \nphase, but throughout the life cycle of the system. \nThis document recommends guidelines for providers1 of any systems that use AI, whether those \nsystems have been created from scratch or built on top of tools and services provided by others. \nImplementing these guidelines will help providers build AI systems that function as intended, are \navailable when needed, and work without revealing sensitive data to unauthorised parties.\nThese guidelines should be considered in conjunction with established cyber security, risk management, \nand incident response best practice. In particular, we urge providers to follow the secure by design2  \nprinciples developed by the US Cybersecurity and Infrastructure Security Agency (CISA), the UK National \nCyber Security Centre (NCSC), and all our international partners. The principles prioritise:\n > taking ownership of security outcomes for customers\n > embracing radical transparency and accountability\n > building organisational structure and leadership so secure by design is a top business priority.\nFollowing secure by design principles requires significant resources throughout a systems life cycle. It \nmeans developers must invest in prioritising features , mechanisms , and implementation  of tools that \nprotect customers at each layer of the system design, and across all stages of the development life cycle. \nDoing this will prevent costly redesigns later, as well as safeguarding customers and their data in the near \nterm.  \nWhy is AI security different?\nIn this document we use AI to refer specifically to machine learning (ML) applications3. All types of ML are in \nscope. We define ML applications as applications that:  \n >involve software components (models) that allow computers to recognise and bring context to patterns \nin data without the rules having to be explicitly programmed by a human\n >generate predictions, recommendations, or decisions based on statistical reasoning\nAs well as existing cyber security threats, AI systems are subject to new types of vulnerabilities. The term \nadversarial machine learning (AML), is used to describe the exploitation of fundamental vulnerabilities in ML \ncomponents, including hardware, software, workflows and supply chains. AML enables attackers to cause \nunintended behaviours in ML systems which can include: \n >affecting the models classification or regression performance\n >allowing users to perform unauthorised actions\n >extracting sensitive model information\nThere are many ways to achieve these effects, such as prompt injection attacks in the large language \nmodel (LLM) domain, or deliberately corrupting the training data or user feedback (known as data \npoisoning).\n06Guidelines for secure AI system development\n\n07Guidelines for secure AI system development\nWho should read this document?\nThis document is aimed primarily at providers of AI systems, whether based on models hosted by an \norganisation or making use of external application programming interfaces (APIs). However, we urge all \nstakeholders (including data scientists, developers, managers, decision-makers and risk owners) to read \nthese guidelines to help them make informed decisions about the design , deployment  and operation  of \ntheir machine learning AI systems.  \n \nThat said, not all of the guidelines will be directly applicable to all organisations. The level of sophistication \nand the methods of attack will vary depending on the adversary targeting the AI system, so the guidelines \nshould be considered alongside your organisations use cases and threat profile. \n \nWho is responsible for developing secure AI? \nThere are often many actors in modern AI supply chains. A simple approach assumes two entities:\n >the provider who is responsible for data curation, algorithmic development, design, deployment and \nmaintenance\n >the user, who provides inputs and receives outputs\nWhile this provider-user approach is used in many applications, it is becoming increasingly uncommon, as \nproviders may look to incorporate software, data, models and/or remote services provided by third parties \ninto their own systems. These complex supply chains make it harder for end users to understand where \nresponsibility for secure AI lies.  \nUsers (whether end users, or providers incorporating an external AI component) do not typically have \nsufficient visibility and/or expertise to fully understand, evaluate or address risks associated with systems \nthey are using. As such, in line with secure by design principles, providers of AI components should take \nresponsibility for the security outcomes of users further down the supply chain.  \nProviders should implement security controls and mitigations where possible within their models, pipelines \nand/or systems, and where settings are used, implement the most secure option as default. Where risks \ncannot be mitigated, the provider should be responsible for:  \n >informing users further down the supply chain of the risks that they and (if applicable) their own users \nare accepting\n >advising them on how to use the component securely  \nWhere system compromise could lead to tangible or widespread physical or reputational damage, \nsignificant loss of business operations, leakage of sensitive or confidential information and/or legal \nimplications, AI cyber security risks should be treated as critical .\n\nGuidelines for secure AI system development\nThe guidelines are broken down into four key areas within the AI system development life cycle: secure \ndesign , secure development , secure deployment , and secure operation and maintenance . For each area, \nwe suggest considerations and mitigations that will help reduce the overall risk to the organisational AI \nsystem development process.  \nThe guidelines set out in this document are aligned closely to software development life cycle practices \ndefined in:\n >the NCSCs Secure development and deployment guidance\n >the National Institute of Standards and Technology (NIST) Secure Software Development Framework  \n(SSDF)6\n08Guidelines for secure AI system development\n\nThis section contains guidelines that apply to the design  stage of the AI system development life cycle. It \ncovers understanding risks and threat modelling, as well as specific topics and trade-offs to consider on  \nsystem and model design.\nRaise staff awareness of threats and risks\nSystem owners and senior leaders understand threats to secure AI and their mitigations. Your data \nscientists and developers maintain an awareness of relevant security threats and failure modes and \nhelp risk owners to make informed decisions. You provide users with guidance on the unique security \nrisks facing AI systems (for example, as part of standard InfoSec training) and train developers in secure \ncoding techniques and secure and responsible AI practices. 1. Secure design\n09\nDesign your system for security as well as functionality and performance\nYou are confident that the task at hand is most appropriately addressed using AI. Having determined \nthis, you assess the appropriateness of your AI-specific design choices. You consider your threat model \nand associated security mitigations alongside functionality, user experience, deployment environment, \nperformance, assurance, oversight, ethical and legal requirements, among other considerations.  \nFor example:  \n >you consider supply chain security when choosing whether to develop in house or use external \ncomponents, for example:  \n >your choice to train a new model, use an existing model (with or without fine-tuning) or \naccess a model via an external API is appropriate to your requirements\n >your choice to work with an external model provider includes a due diligence evaluation of \nthat providers own security posture\n >if using an external library, you complete a due diligence evaluation (for example, to \nensure the library has controls that prevent the system loading untrusted models without \nimmediately exposing themselves to arbitrary code execution9)\n >you implement scanning and isolation/sandboxing when importing third-party models or \nserialised weights, which should be treated as untrusted third-party code and could enable \nremote code executionModel the threats to your system\nAs part of your risk management process, you apply a holistic process to assess the threats to your \nsystem, which includes understanding the potential impacts to the system, users, organisations, and \nwider society if an AI component is compromised or behaves unexpectedly7. This process involves \nassessing the impact of AI-specific threats8 and documenting your decision making.\n \nYou recognise that the sensitivity and types of data used in your system may influence its value as a \ntarget to an attacker. Your assessment should consider that some threats may grow as AI systems \nincreasingly become viewed as high value targets, and as AI itself enables new, automated attack \nvectors.  Guidelines for secure AI system development\n\n >if using an external APIs, you apply appropriate controls to data that can be sent to services \noutside of your organisations control, such as requiring users to log in and confirm before \nsending potentially sensitive information\n >you apply appropriate checks and sanitisation of data and inputs; this includes when \nincorporating user feedback or continuous learning data into your model, recognising that \ntraining data defines system behaviour  \n >you integrate AI software system development into existing secure development and operations \nbest practices; all elements of the AI system are written in appropriate environments using coding \npractices and languages that reduce or eliminate known classes of vulnerabilities where plausible  \n >if AI components need to trigger actions, for example amending files or directing output to external \nsystems, you apply appropriate restrictions to the possible actions (this includes external AI and non-\nAI fail-safes if necessary)  \n >decisions around user interaction are informed by AI-specific risks, for example:  \n >your system provides users with usable outputs without revealing unnecessary levels of \ndetail to a potential attacker\n >if necessary, your system provides effective guardrails around model outputs\n >if offering an API to external customers or collaborators, you apply appropriate controls that \nmitigate attacks on the AI system via the API\n >you integrate the most secure settings into the system by default\n >you apply least privilege principles to limit access to a systems functionality\n >you explain riskier capabilities to users and require users to opt in to use them; you \ncommunicate prohibited use cases, and, where possible, inform users of alternative \nsolutions\n10\nConsider security benefits and trade-offs when selecting your AI model\nYour choice of AI model will involve balancing a range of requirements. This includes choice of model \narchitecture, configuration, training data, training algorithm and hyperparameters. Your decisions are \ninformed by your threat model, and are regularly reassessed as AI security research advances and \nunderstanding of the threat evolves.  \nWhen choosing an AI model, your considerations will likely include, but are not limited to:  \n >the complexity of the model you are using, that is, the chosen architecture and number of parameters; \nyour models chosen architecture and number of parameters will, among other factors, affect how \nmuch training data it requires and how robust it is to changes in input data when in use  \n >the appropriateness of the model for your use case and/or feasibility of adapting it to your specific \nneed (for example by fine-tuning)  \n >the ability to align, interpret and explain your models outputs (for example for debugging, audit or \nregulatory compliance); there may be benefits to using simpler, more transparent models over large \nand complex ones which are more difficult to interpret  \n >characteristics of training dataset(s), including size, integrity, quality, sensitivity, age, relevance and \ndiversityGuidelines for secure AI system development\n\n >the value of using model hardening (such as adversarial training), regularisation and/or privacy-\nenhancing techniques  \n >the provenance and supply chains of components including the model or foundation model, training \ndata and associated tools \nFor more information about how many of these factors impact security outcomes, refer to the NCSCs \nPrinciples for the Security of Machine Learning, in particular Design for security (model architecture) .\n11Guidelines for secure AI system development\n\nThis section contains guidelines that apply to the development  stage of the AI system development \nlifecycle, including supply chain security, documentation, and asset and technical debt management.\nSecure your supply chain\nYou assess and monitor the security of your AI supply chains across a systems life cycle, and require \nsuppliers to adhere to the same standards your own organisation applies to other software. If \nsuppliers cannot adhere to your organisations standards, you act in accordance with your existing risk \nmanagement policies.  \nWhere not produced in-house, you acquire and maintain well-secured and well-documented hardware \nand software components (for example, models, data, software libraries, modules, middleware, \nframeworks, and external APIs) from verified commercial, open source, and other third-party developer s to \nensure robust security in your systems.  \nYou are ready to failover to alternate solutions for mission-critical systems, if security criteria are not met. \nYou use resources like the NCSCs Supply Chain Guidance  and frameworks such as Supply Chain Levels \nfor Software Artifacts (SLSA)10 for tracking attestations of the supply chain and software development life \ncycles.\nIdentify, track and protect your assets\nYou understand the value to your organisation of your AI-related assets, including models, data \n(including user feedback), prompts, software, documentation, logs and assessments (including \ninformation about potentially unsafe capabilities and failure modes), recognising where they represent \nsignificant investment and where access to them enables an attacker. You treat logs as sensitive data \nand implement controls to protect their confidentiality, integrity and availability.  \nYou know where your assets reside and have assessed and accepted any associated risks. You have \nprocesses and tools to track, authenticate, version control and secure your assets, and can restore to a \nknown good state in the event of compromise.  \nYou have processes and controls in place to manage what data AI systems can access, and to manage \ncontent generated by AI according to its sensitivity (and the sensitivity of the inputs that went into \ngenerating it).\nDocument your data, models and prompts\nYou document the creation, operation, and life cycle management of any models, datasets and meta- \nor system-prompts. Your documentation includes security-relevant information such as the sources \nof training data (including fine-tuning data and human or other operational feedback), intended \nscope and limitations, guardrails, cryptographic hashes or signatures, retention time, suggested review \nfrequency and potential failure modes. Useful structures to help do this include model cards, data cards \nand software bills of materials (SBOMs). The production of comprehensive documentation supports \ntransparency and accountability11.\n122. Secure developmentGuidelines for secure AI system development\n\nManage your technical debt\nAs with any software system, you identify, track and manage your technical debt throughout an AI \nsystems life cycle (technical debt is where engineering decisions that fall short of best practices to \nachieve short-term results are made, at the expense of longer-term benefits). Like financial debt, \ntechnical debt is not inherently bad, but should be managed from the earliest stages of development12. \nYou recognise that doing so can be more challenging in an AI context than for standard software, and \nthat your levels of technical debt are likely to be high due to rapid development cycles and a lack of \nwell-established protocols and interfaces. You ensure your life cycle plans (including processes to \ndecommission AI systems) assess, acknowledge and mitigate risks to future similar systems. \n13Guidelines for secure AI system development\n\n14This section contains guidelines that apply to the deployment  stage of the AI system development life \ncycle, including protecting infrastructure and models from compromise, threat or loss, developing incident \nmanagement processes, and responsible release.\n \nSecure your infrastructure\nYou apply good infrastructure security principles to the infrastructure used in every part of your systems \nlife cycle. You apply appropriate access controls to your APIs, models and data, and to their training and \nprocessing pipelines, in research and development as well as deployment. This includes appropriate \nsegregation of environments holding sensitive code or data. This will also help mitigate standard cyber \nsecurity attacks which aim to steal a model or harm its performance. \nProtect your model continuously\nAttackers may be able to reconstruct the functionality of a model13 or the data it was trained on14, by \naccessing a model directly (by acquiring model weights) or indirectly (by querying the model via an \napplication or service). Attackers may also tamper with models, data or prompts during or after training, \nrendering the output untrustworthy. \nYou protect the model and data from direct and indirect access, respectively, by:\n >implementing standard cyber security best practices\n >implementing controls on the query interface to detect and prevent attempts to access, modify, and \nexfiltrate confidential information  \nTo ensure that consuming systems can validate models, you compute and share cryptographic hashes \nand/or signatures of model files (for example, model weights) and datasets (including checkpoints) as \nsoon as the model is trained. As always with cryptography, good key management is essential15.\n \nYour approach to confidentiality risk mitigation will depend considerably on the use case and the threa t \nmodel. Some applications, for example those involving very sensitive data, may require theoretical \nguarantees that can be difficult or expensive to apply. If appropriate, privacy-enhancing technologies \n(such as differential privacy or homomorphic encryption) can be used to explore or assure levels of risk \nassociated with consumers, users and attackers having access to models and outputs.3. Secure deploymentGuidelines for secure AI system development\nDevelop incident management procedures \nThe inevitability of security incidents affecting your AI systems is reflected in your incident response, \nescalation and remediation plans. Your plans reflect different scenarios and are regularly reassessed a s \nthe system and wider research evolves. You store critical company digital resources in offline backups. \nResponders have been trained to assess and address AI-related incidents. You provide high-quality audit \nlogs and other security features or information to customers and users at no extra charge, to enable their \nincident response processes. \n\nRelease AI responsibly\nYou release models, applications or systems only after subjecting them to appropriate and effective \nsecurity evaluation such as benchmarking and red teaming (as well as other tests that are out of scope \nfor these guidelines, such as safety or fairness), and you are clear to your users about known limitations \nor potential failure modes. Details of open-source security testing libraries are given in the further reading \nsection  at the end of this document.\nMake it easy for users to do the right things\nYou recognise that each new setting or configuration option is to be assessed in conjunction with the \nbusiness benefit it derives, and any security risks it introduces. Ideally, the most secure setting will be \nintegrated into the system as the only option. When configuration is necessary, the default option should  \nbe broadly secure against common threats (that is, secure by default). You apply controls to prevent the \nuse or deployment of your system in malicious ways. \nYou provide users with guidance on the appropriate use of your model or system, which includes \nhighlighting limitations and potential failure modes. You state clearly to users which aspects of security \nthey are responsible for, and are transparent about where (and how) their data might be used, accessed \nor stored (for example, if it is used for model retraining, or reviewed by employees or partners).\n15Guidelines for secure AI system development\n\nThis section contains guidelines that apply to the secure operation and maintenance  stage of the AI \nsystem development life cycle. It provides guidelines on actions particularly relevant once a system has \nbeen deployed, including logging and monitoring, update management and information sharing.\n \nMonitor your systems behaviour\nYou measure the outputs and performance of your model and system such that you can observe \nsudden and gradual changes in behaviour affecting security. You can account for and identify potential \nintrusions and compromises, as well as natural data drift.\nMonitor your systems inputs\nIn line with privacy and data protection requirements, you monitor and log inputs to your system (such \nas inference requests, queries or prompts) to enable compliance obligations, audit, investigation and \nremediation in the case of compromise or misuse. This could include explicit detection of out-of-\ndistribution and/or adversarial inputs, including those that aim to exploit data preparation steps (such as \ncropping and resizing for images).  \nFollow a secure by design approach to updates\nYou include automated updates by default in every product and use secure, modular update procedures \nto distribute them. Your update processes (including testing and evaluation regimes) reflect the fact that \nchanges to data, models or prompts can lead to changes in system behaviour (for example, you treat \nmajor updates like new versions). You support users to evaluate and respond to model changes (for \nexample by providing preview access and versioned APIs).\nCollect and share lessons learned\nYou participate in information-sharing communities, collaborating across the global ecosystem of \nindustry, academia and governments to share best practice as appropriate. You maintain open lines \nof communication for feedback regarding system security, both internally and externally to your \norganisation, including providing consent to security researchers to research and report vulnerabilities. \nWhen needed, you escalate issues to the wider community, for example publishing bulletins responding \nto vulnerability disclosures, including detailed and complete common vulnerability enumeration. You \ntake action to mitigate and remediate issues quickly and appropriately.4. Secure operation and maintenance\n16Guidelines for secure AI system development\n\nAI development  \nPrinciples for the security of machine learning\nThe NCSCs detailed guidance on developing, deploying or operating a system with an ML component.\nSecure by Design - Shifting the Balance of Cybersecurity Risk: Principles and Approaches for Secure by \nDesign Software\nCo-authored by CISA, the NCSC and other agencies, this guidance describes how manufacturers of \nsoftware systems, including AI, should take steps to factor security into the design stage of product \ndevelopment, and ship products that come secure out of the box. \nAI Security Concerns in a Nutshell\nProduced by the German Federal Office for Information Security (BSI), this document provides an \nintroduction to possible attacks on machine learning systems and potential defences against those \nattacks. \nHiroshima Process International Guiding Principles for Organizations Developing Advanced AI Systems  \nand Hiroshima Process International Code of Conduct for Organizations Developing Advanced AI Systems \nThese documents, produced as part of the G7 Hiroshima AI Process, provide guidance for organisations \ndeveloping the most advanced AI systems, including the most advanced foundation models and \ngenerative AI systems with the aim of promoting safe, secure, and trustworthy AI worldwide. \nAI Verify\nSingapores AI Governance Testing Framework and Software toolkit that validates the performance of AI \nsystems against a set of internationally recognised principles through standardised tests.\nMultilayer Framework for Good Cybersecurity Practices for AI  ENISA (europa.eu)\nA framework to guide National Competent Authorities and AI stakeholders on the steps they need to \nfollow to secure their AI systems, operations and processes\nISO 5338: AI system life cycle processes (Under review)  \nA set of processes and associated concepts for describing the life cycle of AI systems based on machine \nlearning and heuristic systems.\nAI Cloud Service Compliance Criteria Catalogue (AIC4)\nBSIs AI Cloud Service Compliance Criteria Catalogue provides AI-specific criteria, which enable \nevaluation of the security of an AI service across its lifecycle.\nNIST IR 8269 (Draft) A Taxonomy and Terminology of Adversarial Machine Learning\nA set of processes and associated concepts for describing the life cycle of AI systems based on machine \nlearning and heuristic systems. \nMITRE ATLAS   \nA knowledge base of adversary tactics, techniques, and case studies for machine learning (ML) systems, \nmodelled after and linked to MITRE ATT&CK framework.\nAn Overview of Catastrophic AI Risks (2023)\nProduced by the Center for AI Safety, this document sets out areas of risk posed by AI.\n \nLarge Language Models: Opportunities and Risks for Industry and Authorities\nDocument produced by BSI for companies, authorities and developers who want to learn more about the \nopportunities and risks of developing, deploying and/or using LLMs.Further reading\n17Guidelines for secure AI system development\n\n18Guidelines for secure AI system development\nIntroducing Artificial Intelligence  \nBlog from the Australian Cyber Security Centre which provides approachable guidance on Artificial \nIntelligence and how to securely engage with it.\nOpen-source projects to help users security test AI models include:  \n >Adversarial Robustness Toolbox  (IBM)\n >CleverHans  (University of Toronto)\n >TextAttack  (University of Virginia)\n >Prompt Bench  (Microsoft)\n >Counterfit  (Microsoft). \n >AI Verify  (Infocomm Media Development Authority, Singapore)\nCyber security\n \nCISAs Cybersecurity Performance Goals  \nA common set of protections that all critical infrastructure entities should implement to meaningfully \nreduce the likelihood and impact of known risks and adversary techniques.\nNCSC CAF Framework\nThe Cyber Assessment Framework (CAF) provides guidance for organisations responsible for vitally \nimportant services and activities.\nMITREs Supply Chain Security Framework\nA framework for evaluating suppliers and service providers within the supply chain.\nRisk management\n \nNIST AI Risk Management Framework (AI RMF)\nThe AI RMF outlines how to manage socio-technical risks to individuals, organisations, and society \nuniquely associated with AI.\nISO 27001: Information security, cybersecurity and privacy protection\nThis standard provides organisations with guidance on the establishment, implementation and \nmaintenance of an information security management system\nISO 31000: Risk management\nAn international standard that provides organisations with guidelines and principles for risk management \nwithin organisations \n \nNCSC Risk Management Guidance\nThis guidance helps cyber security risk practitioners to better understand and manage the cyber security \nrisks affecting their organisations.\n\n19NotesGuidelines for secure AI system development\n1. Here defined as a person, public authority, agency or other body that develops an AI system (or that \nhas an AI system developed) and places that system on the market or puts it into service under its \nown name or trademark  \n2. For more information on secure by design, see CISAs Secure by Design  web page and guidance \nShifting the Balance of Cybersecurity Risk: Principles and Approaches for Secure by Design Software  \n3. As opposed to non-ML AI approaches such as rule-based systems  \n4. CEPS describe seven different types of AI development interaction in their publication  Reconciling the \nAI Value Chain with the EUs Artificial Intelligence Act  \n5. ISO/IEC 22989:2022(en)  defines this as a functional element that constructs an AI system  \n6. NIST is tasked with producing guidelines (and taking other actions) to advance the safe, secure, and \ntrustworthy development and use of Artificial Intelligence (AI). See NISTs Responsibilities Under the \nOctober 30, 2023 Executive Order  \n7. More information on threat modelling is available from the OWASP Foundation  \n8. See MITRE ATLAS Adversarial Machine Learning 101  \n9. GitHub: RCE PoC for Tensorflow using a malicious Lambda layer  \n10. SLSA:  Safeguarding artifact integrity across any software supply chain  \n11. METI (Japanese Ministry of Economy, Trade and Industry, 2023),  Guide of Introduction of Software Bill of \nMaterials (SBOM) for Software Management  \n12. Google research: Machine Learning: The High Interest Credit Card of Technical Debt  \n13. Tramr et al 2016, Stealing Machine Learning Models via Prediction APIs  \n14. Boenisch, 2020, Attacks against Machine Learning Privacy (Part 1): Model Inversion Attacks with the \nIBM-ART Framework  \n15. National Cyber Security Centre, 2020, Design and build a privately hosted Public Key Infrastructure\n\n \n  Crown copyright 2023. Photographs and infographics may include material under licence from \nthird parties and are not available for re-use. Text content is licenced for re-use under the Open \nGovernment Licence v3.0.  \n(https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/)\n@N CSC NCSC.GOV.UKNational Cyber \nSecuri ty Centre@C YBERHQ @C YBERHQ\n\n",
  "cves": [],
  "techniques": [],
  "advisory": "cybersecurity-alerts",
  "title": "guidelines-for-secure-ai-system-development",
  "source": "nsa",
  "id": "6892ad861ea4ceada5da96f1fd1f53250341e8d68618b5495049e37f93f89cc3"
}