{
  "markdown": " \nThis information is marked TLP:CLEAR. Recipients may share this information without restriction. \nInformation is subject to standard copyright rules. For more information on the Traffic Light Protocol, see \ncisa.gov/ tlp/.  | Cybersecurity Information Sheet  \nTLP:CLEAR  \nTLP:CLEAR  \n \nNational  \nSecurity  \nAgency  \nFederal  \nBureau of  \nInvestigation  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nContextualizing Deep fake Threats  to Organizations  \nExecutive summary  \nThreats from synthetic media, such  as deepfake s, present a growing challenge for all \nusers of modern technology and communications, includ ing National Security Systems \n(NSS), the Department of Defense (DoD), the Defense Industrial Base (DIB) , and \nnational critical infrastructure owners and operators . \nAs with many technologies, synthetic media \ntechniques can be used for both positive and \nmalicious purposes. While there are limited \nindications of significant use of synthetic media \ntechniques by malicious state -sponsored actors, the \nincreasing availability and efficiency of synthetic \nmedia techniques available to less capable \nmalicious cyber act ors indicate these types of \ntechniques will likely increase in frequency and \nsophistication.  \nSynthetic media threats broadly exist across \ntechnologies associated with the use of text, video, audio , and images which are used \nfor a variety of purposes onlin e and in conjunction with communications of all types. \nDeepfakes are a particularly concerning type of synthetic media that utilizes artificial \nintelligence/machine learni ng (AI/ML) to create believable and highly  realistic media . [1] \nThe most substantial threats from the abuse of synthetic media include techniques that \nthreaten an organizations  brand, impersonat e leaders and financial officers, and use \nfraudulent c ommunications to enable access to an organizations  networks, \ncommunications, and sensitive information.  \nOrganizations  can take a variety of steps to identify, defend against, and respond  to \ndeepfake  threats. They  should  consider implementing a number of t echnologies to \ndetect deepfake s and determine media provenance , including real -time verification \ncapabilities, passive detection techniques, and protection of high priority officers and \ntheir communications. [2] [3] Organizations  can also take steps to minimize the impact Deepfakes are AI -generated, \nhighly realistic synthetic \nmedia that can be abused to:  \n Threaten an organizations \nbrand  \n Impersonate leaders and \nfinancial officers  \n Enable access to networks, \ncommunications, and \nsensitive information  \n\n \n \n \n \nU/OO/199197 -23 | PP-23-3076  | SEP 2023  Ver. 1.0   Joint CSI  | Contextualizing Deepfake Threats to Organizations  \nTLP:CLEAR  \nTLP:CLEAR  2 of malicious deepfake  technique s, including information sharing, planning for and \nrehearsing responses to exploitation attempt s, and personnel training.  \nIn particular, phishing using deepfakes will be an even harder challenge than it is today, \nand organizations should proactively prepare to identify and count er it. Several  public \nand private consortiums also offer opportunit ies for organizations  to get involved in \nbuilding resilience to deepfake  threats, including the Coalition for Content Provenance \nand A uthenticity  and Project Origin . [4] [5] \nThis cybersecurity  information sheet, authored by the National Security Agency  (NSA) , \nthe Federal Bureau of Investigation  (FBI) , and the Cybersecurity and Infrastructure \nSecurity Agency (CISA) , provides an overview of synthetic media threats, techniques, \nand trends. It also offers recommendations for security professionals focused on \nprotecting organizations  from these evolving threats through advice on defensive and \nmitigation  strategies .  \nSynt hetic  media threats  \nTools and techniques that can be used to manipulate authentic multimedia have been \naround for decades [6]; however, the scale in use of media m anipulation has \ndramatically increased as the complexity of leveraging manipulated media has fallen. \nMaking a sophisticated fake with specialized software previously could take a \nprofessional days to weeks to construct, but now , these fakes can be produced  in a \nfraction of the time with limited or no technical expertise. This is largely due to advances \nin computational power and deep learning, which make it not only easier to create fake \nmultimedia, but also less expensive to mass produce. In addition, the market is now \nflooded with free, easily accessible tools (some powered by deep learning algorithms) \nthat make the creation or manipulation of multimedia essentially plug -and-play. As a \nresult, these publicly available techniques have increased in value and become widely \navailable  tools for adversaries of all types , enabl ing fraud  and disinformation  to exploit \ntargeted individuals and organizations. The democratization of these tools has made the \nlist of top risks for 2023.  [7] \n\n \n \n \n \nU/OO/199197 -23 | PP-23-3076  | SEP 2023  Ver. 1.0   Joint CSI  | Contextualizing Deepfake Threats to Organizations  \nTLP:CLEAR  \nTLP:CLEAR  3 Apart from the obvious implications for misinformation and propaganda during times of \nconflict, national security challenges associated with deepfake s manifest in threats to \nthe U .S. Government , NSS, the DIB , critical \ninfrastructure organizations, and others . \nOrganizations  and their employees may be \nvulnerable to deepfake  tradecraft and techniques \nwhich may include fake online accounts used in \nsocial engineering attempt s, fraudulent text and \nvoice messages used to avoid technical defenses, \nfaked videos used to spread disinformation, and \nother techniques. Many organizations  are attractive \ntargets for advanced actors and criminals interested in executive impersonation, \nfinancial fraud, and illegitimate access to internal communications and operations.  \nDefining the problem  \nSeveral terms are used to describe media that have been synthetically generated \nand/or manipulated. Some of the most common include:  \n Shallow/Cheap Fakes:   \nMultimedia that has been manipulated using techniques that do \nnot involve machine/ deep learning, which  in many cases  can \nstill be as effective as the more technically sophisticated \ntechniques, are often referred to as shallow or cheap fakes. \nThese fakes are often generated through the manipulat ion of an original  \nmessage conveyed in some  real media. Some explicit examples of this include:  \no Selectively copying  and pasting content from an original scene to remove an \nobject in an image and thereby change the story. There are many examples \nof this in history, including when Josef Stalin remove d an individual from an \nimage after they became enemies. [8] \no The slowing down of a video by adding repeat frames to make it sound like an \nindividual is intoxicated. [9] \no Combining audio clips from a different source and replacing the audio on a \nvideo to change the story. [10] \nDeepfake social engineering \nincludes:  \n Fraudulent texts  \n Fraudulent voice \nmessages  \n Faked videos  \n\n \n \n \n \nU/OO/199197 -23 | PP-23-3076  | SEP 2023  Ver. 1.0   Joint CSI  | Contextualizing Deepfake Threats to Organizations  \nTLP:CLEAR  \nTLP:CLEAR  4 o Using false text to push a narrative and cause financial loss  and other \nimpacts . [11]  \n \nFigure 1: Deepfake image , courtesy of https://farid.berkeley.edu/misc/deepfakefaces.html  [12] \n Deepf akes:   \nMultimedia that have either been crea ted (fully synthetic) or \nedited  (partially synthetic) using some form of machine/deep \nlearning (artificial intelligence) are referred to as deepfake s. \nSome explicit examples include:  \no LinkedIn experienced  a huge increase in deepfake  images for profile pictures \nin 2022. [13] \no An AI -generated scene that is the product of AI hallucination made up \ninformation that may seem plausible but is not true that depict s an explosion \nnear the Pentagon was shared around the internet in May 2023, causing \ngeneral confusion and t urmoil on the stock market. [14] \no A deepfake  video showed  Ukrainian President Volodomyr Zelenskyy  telling  \nhis country to surrender to Russia.  [15] More recently, several Russian TV \nchannels and radio stations were hacked and a purported deepfake  video of \nRussian President Vladimir Putin  was aired claiming he was enacting martial \nlaw due  to Ukrainians invading Russia. [16] \n\n\n \n \n \n \nU/OO/199197 -23 | PP-23-3076  | SEP 2023  Ver. 1.0   Joint CSI  | Contextualizing Deepfake Threats to Organizations  \nTLP:CLEAR  \nTLP:CLEAR  5  \nFigure 2: Deepfake of Vladimir Putin  \no Another example  of technology being developed in video is Text -to-Video \nDiffusion Models , which are fully synthetic videos developed by AI. [17] \no In 2019, deepfake  audio was used to steal $243,000 from a UK Company [18] \nand, more recently , there  has been a massive increase in personalized AI \nscams given the release of sophisticated and highly trained AI voice cloning \nmodels. [19] [20] \no Openly accessible Large Language Models (LLMs) , are now being used now \nto generate the text for phishing emails. [21]  \n Generative AI:   \nAs of 2023,  Generative AI  is gaining popularity for many \ncapabilities that produce synthetic media . Generative AI (Machine \nLearning Techniques), such as Generative Adversarial Networks, \n[22] Diffusion Models, [23] and Large Language Models [24] (or a \ncombination there of) are the machinery that is enabling the production of highly \nrealistic synthetic multimedia based on  much larger datasets.  \n Computer Generated Imagery (CGI):   \nCGI is the use  of computer graphics to create or improve visual \nmedia (image and video). Traditionally , these methods have \nbeen the go -to visual effect for most major movies, but now that \n\n\n \n \n \n \nU/OO/199197 -23 | PP-23-3076  | SEP 2023  Ver. 1.0   Joint CSI  | Contextualizing Deepfake Threats to Organizations  \nTLP:CLEAR  \nTLP:CLEAR  6 Generative AI techniques are getting better and cheap er, the merging of these \ntwo technologies is being used to produce even more convincing fakes. [25] \nDetection vs authentication  \nFor several years, public and private organizations have expresse d concern over \nmanipulated multimedia and develop ed means to detect and identify countermeasures. \nMany  public and private partnerships have since emerged, focusing on cooperative \nefforts to detect these manipulations and verify/authenticate multimedia. There are \nmany differences between the detection and authentication efforts , as they have \ndifferent goals . The greatest of these  is that detection methods are  often passive \nforensic technique s, whereas authentication methods are active forensic techniques \nthat are purposely embedded at the time of capture or time of edit of the media in \nquestion. [26] \nDetection efforts typically focus on develop ing methods that seek evidence of \nmanipulation and present that evidence in the form of a numerical output  or a \nvisualization t o alert an analyst that the media  needs further analysis . These methods \nare developed under the assumption that modifications to original media or completely \nsynthetic media contain statistically significant traces that can be found. This form of \ndetection  is a cat and mouse game; as detection methods are developed and made \npublic, there is often a quick response from the generation community to counter  them . \nHowever, until there is a universal adoption of authentication standards, these methods \nare necessa ry to support forensic analysis.  \nAuthentication methods are designed to be embedded at the time of capture/creation or \ntime of edit to bring transparency to the provenance of the media. Some examples \ninclude digital watermarking [27] which can be used in synthetically generated media, \nactive signals in real -time capture to verify liveness,  [28] and cryptographic asset \nhashing on a device.  [29]  \nSome of the ongoing efforts in detection and authentication include several \npublic /private c ollaborative initiatives , such as:  \n The D ARPA Semantic Forensics program  is currently developing advanced \nsemantic capabilities for media forensics and authentication. Program \n\n \n \n \n \nU/OO/199197 -23 | PP-23-3076  | SEP 2023  Ver. 1.0   Joint CSI  | Contextualizing Deepfake Threats to Organizations  \nTLP:CLEAR  \nTLP:CLEAR  7 participants include NVIDIA, PAR Government Systems, SRI International, and \nseveral research institutions. [30] \n The Air Force Research Lab (AFRL) recently awarded a contract to the small \nbusiness , DeepMedia , for the development of deepfake  detection capabilities. \n[31] \n Deepfake detection tools have been fielded by several com panies , including \nMicrosoft, Intel , and Google.  \no Prior to the 2020 elections, Microsoft introduced the Microsoft Video \nAuthenticator and in 2023 they rolled out the About this Image tool to get \nmore context for the authenticity of images they may receive . [32] \no Intel introduced a real -time deepfake  detector in late 2022 labeled \nFakeCatcher  which detects fake videos. [33] \no Google , in collaboration with academic researchers in Europe,  contributed a \nlarge dataset of visual deepfakes to the FaceForensics Benchmark in 2019. \n[34] [35] \n Adobe launched the Content Authenticity Initiative (CAI) in 2019 to push for \nprovenance of di gital content. CAI has several hundred members seeking to \ndevelop open content attribution standards. [36] CAI developed the Coalition for \nContent Providence and Au thenticity (C2PA). C2PA unifies the efforts of the \nAdobe -led Content Authenticity Initiative (CAI) which focuses on systems to \nprovide context and history for digital media, and Project Origin, a Microsoft - and \nBBC-led initiative that tackles disinformati on in the digital news ecosystem. [4] \nHow deepfakes can i mpact organizations  \nPublic concern around synthetic media includes its use  with disinformation operations, \ndesigned to influence the public and spread false information about political, social, \nmilitary or economic issues to cause confusion, unrest, and uncertainty. However, \nsynthetic media threats that organizations  most often encounter include activities that \nmay undermine the brand, financial position, security , or integrity of the organization  \nitself. The most significant synthetic media threats  to the DoD, NSS, the DIB , and critical \ninfrastructure  organizations , based on pot ential impact and risk, include, but are not \nlimited to:  \n\n \n \n \n \nU/OO/199197 -23 | PP-23-3076  | SEP 2023  Ver. 1.0   Joint CSI  | Contextualizing Deepfake Threats to Organizations  \nTLP:CLEAR  \nTLP:CLEAR  8  Executive impersonation for brand manipulation:  Malicious actors may use \ndeepfake s, employing manipulated audio and video, to try to impersonate an \norganizations  executive officers and other high -ranking personnel. Malicious \nactors may employ convincing audio and video impersonations of key leaders to \ndamage the reputation and value of a n organization s brand, such as quickly \ndistributing a convincing deepfake  publicly across social media platforms b efore \nit can be stopped or disputed. Types of manipulated media operations have been \nobserved targeting high profile political figures such as Ukraines Volodymr \nZelenskyy to spread disinformation and confusion. This technique  can have high \nimpact, especia lly for international brands where stock prices and overall \nreputation may be vulnerable to disinformation campaigns. Considering the high \nimpact, this type of deepfake is a significant concern for many CEOs  and \ngovernment leaders . \n Impersonation for financ ial gain: Malicious actors, many of them likely cyber \ncriminals, often use multiple types of manipulated media in social engineering \ncampaigns for financial gain. These may include impersonating key leaders or \nfinancial officers and operating over various mediums using manipulated audio, \nvideo , or text, to illegitimately authorize the disbursement of funds to accounts \nbelonging to the malicious actor . Business Email Compromise (BEC) schemes \nare counted among these types of social engineering , costing compan ies \nhundreds of millions of dollars in losses. Similar types of techniques may also be \nused to manipulate the trade or sale of crypto assets. These types of schemes \nare far more common in practice and several partners report ed being targeted by \nthese types of operations.  \n Impersonation to gain access:  Malicious actors may use the same types of \nmanipulated media techniques and technologies to gain access to an \norganizations  personnel, operations, and information. These techniques may \ninclude the use of manipulated media during job interviews, especially for remote \njobs. In 2022, malicious actors reportedly employed synthetic audio and video \nduring online interviews, although the content was often unaligned or \nunsynchroni zed, indicating the fraudulent nature of the calls. These malicious \nattempts were enabled by stolen personal identifiable information (PII). [37] \nSuccessful compro mises may enable actors to obtain sensitive financial, \nproprietary, or internal security information. Manipulated media techniques used \n\n \n \n \n \nU/OO/199197 -23 | PP-23-3076  | SEP 2023  Ver. 1.0   Joint CSI  | Contextualizing Deepfake Threats to Organizations  \nTLP:CLEAR  \nTLP:CLEAR  9 to impersonate specific customers may also be employed to gain access to \nindividual customer accounts for account access  or other information gathering \npurposes.  \n \nThe following are examples of synthetic media attempts used to target \norganizations. In many cases these appear to be executed by cyber criminals \nintending to defraud the organization for financial gain. Beyond examples of \nfaked personal profiles on social networking sites and deceptive smishing or \nvishingphishing using SMS texts or phone calls attempts, there is limited \nevidence that state -sponsored actors are specifically using sophisticated \ndeepfakes so far.  \nIn May 2023, an unknown malicious actor targeted a company using \nsynthetic visual and audio media techniques to impersonate the CEO of the \ncompany. A product line manager in the company was contacted over \nWhatsApp and invited to an interactive call with a  sender claiming to be \nthe CEO of the company. The voice sounded like the CEO and the image and \nbackground used likely matched an existing image from several years \nbefore and the home background belonging to the CEO.  \nIn May 2023, an unknown malicious actor  targeted a company for financial \ngain using a combination of synthetic audio, video, and text messages. The \nactor, impersonating the voice of a company executive, reached a member \nof the company using a poor quality audio call over WhatsApp. The actor \nthen suggested a Teams meeting and the screen appeared to show the \nexecutive in their office. The connection was very poor, so the actor \nrecommended switching to text and proceeded to urge the target to wire \nthem money. The target became very suspicious and t erminated the \ncommunication at this point. The same executive has also been \nimpersonated via text message on other occasions by likely financial \ncriminals.  \n\n \n \n \n \nU/OO/199197 -23 | PP-23-3076  | SEP 2023  Ver. 1.0   Joint CSI  | Contextualizing Deepfake Threats to Organizations  \nTLP:CLEAR  \nTLP:CLEAR  10 Emerging trends in deepfakes and generative AI  \nDynamic trends in technology development associated with the creation of synthetic \nmedia will continue to drive down the cost and technical barriers in using  this technology \nfor malicious purposes. By 2030 the generative AI market is expected to exceed $100 \nbillion, growing at a rate of more than an average  of 35 percent per year. [38] However, \nwhile capabilities available to malicious actors  will dramatically increase, technologies \nand techniques available to defenders seeking to identify and mitigate deepfake s will \nalso improve substantially. One example announced in late 2022, GPTZero, is a \nprogram designed to identify computer generated tex t, including ChatGPT, Googles \nLaMDa, and other AI models, that has already garnered more than a million users. [39] \nHowever, detectors of AI -generated content can suffer from false positives where they \nidentify human -written content as AI -generated  as well . A technological escalation  is \nexpected in synthetic media technologies and capabilities to detect AI generated \ncontent and authenticate legitimate content. [40] \nMajor trends in the generation of media include the increased use and improvement of \nmultimodal models , such as the merging of LLMs and diffusion models ; the impro ved \nability to lift a 2D image to 3D to enable the realistic generation of video based on a \nsingle image ; faster and tunable methods for real time modified video generation ; and \nmodels that require less input data to customize results , such as synthetic au dio that \ncaptures the characteristics of an individual with just a few seconds of reference data. \n[41] All of these trends point to better, faster, and cheaper ways  to generate fake \ncontent.  \nThe major trends o n detection and authentication are toward education, detection \nrefining , and increased pressure from the community to employ authentication \ntechniques for media. Eventually, the se trends may lead to policies tha t will require  \ncertain changes. For now, efforts like the public/private detection and authentication \ninitiatives  referenced in this report and ethical considerations prior to releasing models  \n[42] will help organizations take proactive steps toward more trans parent content \nprovenance. [27] \nRecom mendations for resisting deepfakes  \nOrganizations  can take a variety of steps to prepare to identify, defend against, and \nrespond to deepfake  threats. Synthetic media experts from several U .S. Government \n\n \n \n \n \nU/OO/199197 -23 | PP-23-3076  | SEP 2023  Ver. 1.0   Joint CSI  | Contextualizing Deepfake Threats to Organizations  \nTLP:CLEAR  \nTLP:CLEAR  11 agencies collaborated between 2021 -2022 to consider t hese threats  and establish a \nrecommended list of best practices to employ in preparation for and response to \ndeepfake s. [2] [3] Another good reference for best practices and understanding what \npolicies can be enacted from a company perspective can be found on  the Columbia \nLaw Blue Sky blog . [43] Several of these recommendations are describ ed below and \nexplored in further detail.  \n1. Select and implement technologies to detect deepfake s and demonstrate \nmedia provenance : \n Real -time verifica tion capabilities and p rocedures:  Organizations should \nimplement identity verification capable of operating during real -time \ncommunications. Identity verification for real -time communications will now  \nrequire testing for liveness given the rapid improvemen ts in generative -AI and \nreal-time rendering. Mandatory multi -factor authentication (MFA), using a unique \nor one -time generated password or PIN, known personal details, or biometrics, \ncan ensure those entering sensitive communication channels or activities are \nable to prove their identity. These verification steps are especially important \nwhen considering procedures for the execution of financial transactions.  \no Companies that offer liveness tests powered by virtual injection techniques \ninclude ID R&D  [44], Facetec [45], IProov [46], and many more.  \no The Center for Identification, Technology Research (CITeR ) is a research \ninitiative , funded in part by the National Science Foundation and other \npartners from the academic, commercial , and government sectors, that \nconducts research on techniques to achiev e these goals.  [47]  \no Passive d etection of deepfakes: Passive detection techniques should be u sed \nwhen trying to determine the authenticity of previously created media. In \nthese cases, recommendations for forensic analysis are as follows:  \nBasic r ecommendations:  \no Make a copy  of the media prior to any analysis.  \no Hash both the original and the copy  to verify an exact copy.  \n\n \n \n \n \nU/OO/199197 -23 | PP-23-3076  | SEP 2023  Ver. 1.0   Joint CSI  | Contextualizing Deepfake Threats to Organizations  \nTLP:CLEAR  \nTLP:CLEAR  12 o Check the source  (i.e., is the organization  or person reputable) of the media \nbefore drawing conclusions.  \no Reverse image searches , like TinEye, [48] Google Image Search, [49] and \nBing Visual Search, [50] can be extremely useful if the media is a composition \nof images.  \no Visual/audio e xamination   look and listen to the media first as there may \nbe obvious signs of manipulation  \n Look for physical properties that would not be possible , such as feet not \ntouching the ground.  \n Look for presence of audio filters , such as noise added for ob fuscation.  \n Look for inconsistencies.  \no Metadata examination  tools  can sometimes provide additional insights \ndepending on the situation.  \n All metadata intact is an indication of authenticity . \n Some metadata stripped indicates the media  was potentially man ipulated, \nbut further investigation is required.  \n All metadata stripped may indicate the media was obtained  through a \nsocial media platform or other process that automatically strips the \ninformation.  \nAdvanced r ecommendations:  \no Physics based examination s  complete checks to verify vanishing points, \nreflections, shadows, and more using ideas from Hany Farid [see Chapter 1 \nof Fake Photos for more information] [51] and other methods that use Fluid \nDynamics. [52] \no Compression based examination   Use tools designed to look for \ncompression artifacts , knowing that lossy compression in  media will inherently \ndestroy lots of forensic artifacts.  \n\n \n \n \n \nU/OO/199197 -23 | PP-23-3076  | SEP 2023  Ver. 1.0   Joint CSI  | Contextualizing Deepfake Threats to Organizations  \nTLP:CLEAR  \nTLP:CLEAR  13 o Content based examination s (when appropriate)   Use tools designed to \nlook for specific manipulations when suspected . For example:  \n Use tools like those available on GitHub  [53] if a GAN was  suspected for \ndeepfake production .  \n Consider p lug-ins to detect  suspected  fake profile pictures.  [54] \n Explore the Antispoofing Wiki with various  deepfake detection tools and \nsoftware. [55] \n Use o pen source algorithms and papers for various manipulation tasks , \nsuch as grip -unina [56] and the deepfake detection challenge. [57] \n In addition to the techniques and categories mentioned above, other \ntechniques can be deployed to detect deepfakes of high priority \nindividuals. Such techniques are based off the unique characteristics  of \nthe individual and are sometimes referred to as Person of Interest (POI) \nmodels. Train ing these models for a particular person can be time \nconsuming and , in some cases,  requires hours of data . However,  if the \nconcern is to protect a particular individual, these methods are designed \njust for that. Some examples include:  \n ID-Reveal [58] and Audio -Visual Person -of-Interest DeepFake \ndetection ; [59] \n Protecting World Leaders Against Deepfakes [60] and Protecting \nPresident Zelenskk y; [61] and  \n Person Specific Audio Deepfake Detection . [62] \n Note on POI  models:  if organizations wish to protect their executives \nwith POI models, they should consider actively collecting and curating \nlegitimate video and audio recordings of these individuals. Such \ncollections of data will be necessary to develop detection mo dels.  \n2. Protect public data of high -priority individuals.  \nTo protect media that contains the individual from being used or repurposed for \ndisinformation, one should consider beginning to use active authentication techniques \nsuch as watermarks and/or CAI standards. This is a good preventative measure to \n\n \n \n \n \nU/OO/199197 -23 | PP-23-3076  | SEP 2023  Ver. 1.0   Joint CSI  | Contextualizing Deepfake Threats to Organizations  \nTLP:CLEAR  \nTLP:CLEAR  14 protect med ia and make it more difficult for an adversary to claim that a fake media \nasset portraying the individual in these controlled situations is real. Prepare for and take \nadvantage of opportunities to minimize the impact of deepfake s. \n Plan and r ehearse:  Ensure  plans are in place among organizational security \nteams to respond to a variety of deepfake  techniques . These should be \nprioritized by the likelihood and unique vulnerabilities of each organization  and \ntheir industry. Some organizations  will be more suscep tible to executive \nimpersonation or misinformation which may impact brand status or public stock \nshares. Other organizations  relying on high volumes of virtual financial \ntransactions may be more vulnerable to financial fraud.  \nOnce a plan is established, do  several tabletop exercises to practice and analyze \nthe execution of the plan. These should involve the most likely targets of \ndeepfake s and include executives who may be prime targets. [63] \n Reporting and sharing e xperiences:  Report the details of malicious deepfakes \nwith appropriate U.S. Government partners, including the NSA Cybersecurity \nCollaboration Center for Department of Defense and Defens e Industrial Base \nOrganizations  and the FBI (including local offices or CyWatch@fbi.gov), to \nspread awareness of trending malicious techniques and campaigns.  \n Training p ersonnel:  Every organization  should incorporate an overview of \ndeepfake  techniques into their training program. This should include an overview \nof potential uses of deepfake s designed to cause reputational damage, executive \ntargeting and BEC attempts for financial gain, and manipulated media used to \nundermine hiring or operat ional meetings for malicious purposes. Employees \nshould be familiar with standard procedures for responding to suspected \nmanipulated media and understand the mechanisms for reporting this activity \nwithin their organization.  \nTraining resources specific to deepfake s are already available from the following \nsources:  \n SANS Institute  Learn a New Survival Skill: Spotting Deepfakes ; [64] \n MIT Media Lab  Detect DeepFakes : How to counteract information \ncreated by AI [65] and MIT Media Literacy ; [66] and  \n\n \n \n \n \nU/OO/199197 -23 | PP-23-3076  | SEP 2023  Ver. 1.0   Joint CSI  | Contextualizing Deepfake Threats to Organizations  \nTLP:CLEAR  \nTLP:CLEAR  15  Microsoft  Spot the Deepfake . [67] \n Leveraging cross -industry partnerships:  C2PA is a significant effort launched \nin 2021 to address the prevalen ce of misleading information online through the \ndevelopment of technical standards for certifying the provenance of media \ncontent. Specifications and principles for ensu ring media provenance can be \nfound on the C2PA website. [4] Additional information on issues relating to \nmisinformation and content provenance is available from C2PA associated \nefforts at CAI [36] and Project Origin. [5] \nAs of 2023, CAI encompassed more than 1 ,000 private companies across tech, \nmedia, new s publishers, re searchers, and NGO s. CAI offers several free open \nsource tools to implement media provenance, a regular newsletter, and a \ncommunity channel on Discord.  \nProject Origin, a cross industry effort involving Microsoft and several major media \nproducers, aims to s imilarly establish a chain of content provenance through \nsecure signatures and web browser extensions. Technical background can be \nfound on their website at originproject.info.  \n Understand what private  companies  are doing to preserve the proven ance \nof online content : Organizations  should actively pursue partnerships with \nmedia, social media, career networking, and similar companies in order to learn \nmore about how these companies are preserving the provenance of online \ncontent. This is especially important considering how they may be working to \nidentify and mitigate the harms of synthetic content , which may be used as a \nmeans to exploit organizations  and their employees.  \n  \n\n \n \n \n \nU/OO/199197 -23 | PP-23-3076  | SEP 2023  Ver. 1.0   Joint CSI  | Contextualizing Deepfake Threats to Organizations  \nTLP:CLEAR  \nTLP:CLEAR  16 Works cited  \n[1] NSA, The Next Wave 2021 Vol. 23 No. 1: Deepfakes: Is a Picture Worth a Thousand Lies?, \nhttps://media.defense.gov/2021/Jul/06/2002756456/ -1/-1/0/TNW_23 -1.PDF   \n[2] DHS Public -Private Analytic Exchange Program, Increasing Threat of Deepfake Identities, \nhttps://www.dhs.gov/sites/default/files/publications/increasing _threats_of_deepfake_identities_0.p\ndf  \n[3] DHS Public -Private Analytic Exchange Program, Increasing Threats of Deepfake Identities  \nPhase 2: Mitigation Measures , https://www.dhs.gov/sites/default/files/2022 -\n10/AEP%20DeepFake%20PHASE2%20FINAL%20corrected20221006.pdf   \n[4] The Coalition for Content Provenance and Authenticity (C2PA) , https://c2pa.org/   \n[5] Project Origin: Protecting Trusted Media, https://www.originproject.info/   \n[6] Farid H., Digital Image Forensics,  \nhttps:// farid.berkeley.edu/downloads/tutorials/digitalimageforensics.pdf   \n[7] Eurasia Group, Top Risks 2023, https://www.eurasiagroup.net/issues/top -risks -2023   \n[8] 11 Points, 11 Famous Doctored Photos of Dictators , https://11points.com/11 -famous -doctored -\nphotos -dictators/   \n[9] Slate, Beware the Cheapfakes , https://slate.com/technology/2019/06/drunk -pelosi -deepfakes -\ncheapfakes -artificial -intelligence -disinformation.html   \n[10] VERIFY , Video of First Lady at Eagles game manipulated to include audible boos, anti -Biden \nchants , https://www.verifythis.com/article/news /verify/national -verify/video -of-first-lady-at-eagles -\ngame -manipulated -to-include -audible -boos -anti-biden -chants/536 -c11989d4 -e842 -4a6b -bc18 -\n9ed7c494dd84   \n[11] The Hustle, One fake Tweet may have cost Twitter a lot , https://thehustle.co/11152022 -eli-lilly/  \n[12] Farid H., Deep -fake faces , https://farid.berkeley.edu/misc/deepfakefaces.html   \n[13] NPR, That smiling LinkedIn profile face might be a compu ter-generated fake , \nhttps://www.npr.org/2022/03/27/1088140809/fake -linkedin -profiles  \n[14] CNN, Verified Twitter accounts share fake image of explosion near Pentagon, causing \nconfusion , https://www.cnn.com/2023/05/22/tech/twitter -fake-image -pentagon -\nexplosion/index.html   \n[15] Salon, Deepfake videos are so convincing  and so easy to make  that they pose a political \nthreat , https://www.salon.com/2023/04/15/deepfake -videos -are-so-convincing --and-so-easy -to-\nmake --that-they-pose -a-political/   \n[16] The Independent, Deepfake Putin declares martial law and cries: Russia is under attack , \nhttps://www. independent.co.uk/news/world/europe/deepfake -putin -martial -law-state -media -\nb2353005.html   \n[17] NVIDIA, Align your Latents: High -Resolution Video Synthesis with Latent Diffusion Models, \nhttps: //research.nvidia.com/labs/toronto -ai/VideoLDM/   \n[18] Trend Micro, Unusual CEO Fraud via Deepfake Audio Steals US$243,000 From UK Company , \nhttps://www.trendmicro.com/vinfo/mx/security/news/cyber -attacks/unusual -ceo-fraud -via-\ndeepfake -audio -steals -us-243-000-from-u-k-company   \n[19] TECHCiRCLE, Nearly half of Indian internet users faced AI -driven voice scams this year , \nhttps://www.techcircle.in/2023/05/02/nearly -half-of-indian -internet -users -faced -ai-driven -voice -\nscams -this-year  \n[20] The New York Tim es, Voice Deepfakes Are Coming for Your Bank Balance , \nhttps://www.nytimes.com/2023/08/30/business/voice -deepfakes -bank -scams.html   \n[21] Schneier on Security, LLMs and Ph ishing, https://www.schneier.com/blog/archives/2023/04/llms -\nand-phishing.html   \n[22] Ian J. Goodfellow, Jean Pouget -Abadie, Mehdi Mirza, Bing Xu, David Warde -Farley, Sherjil Oz air, \nAaron Courville, and Yoshua Bengio , Generative Adversarial Nets, \nhttps://arxiv.org/abs/1406.2661   \n[23] Florinel -Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah , Diffusion Models in \nVision: A Survey , https://arxiv.org/abs/2209.04747   \n\n \n \n \n \nU/OO/199197 -23 | PP-23-3076  | SEP 2023  Ver. 1.0   Joint CSI  | Contextualizing Deepfake Threats to Organizations  \nTLP:CLEAR  \nTLP:CLEAR  17 [24] Anjana Samindra Perera , Large Language Models , https://levelup.gitconnected.com/best -papers -\non-large -language -models -ac01b13b94b3   \n[25] Couch Investor, The Impact of Generative AI on CGI Production in the Movie and Graphic Design \nIndustries , https://couchinvestor.substack.com/p/the -impact -of-generative -ai-on-cgi  \n[26] Valentina Conotter , Active and Passive Multimedia Forensics , \nhttps://farid.berkeley.edu/downloads/publications/vcthesis11.pdf   \n[27] CNBC, Google will label fake images created with its A.I. , \nhttps://www.cnbc.com/2023/05/10/google -will-label -fake-images -created -with-its-ai-.html   \n[28] Candice R. Gerstner and Hany Farid, Detecting Real -Time Deep -Fake Videos Using Active \nIllumination, https://farid.berkeley.edu/downloads/publications/cvpr22a.pdf   \n[29] Content Authenticity Initiative, How It Works, https://contentauthenticity.org/how -it-works   \n[30] Defense Ad vanced Research Projects Agency, Semantic Forensics (SemaFor) , \nhttps://www.darpa.mil/program/semantic -forensics   \n[31] DeepMedia, DeepMedia to Help AFRL Spot Deep Fakes, \nhttps://www.deepmedia.ai/press/deepmedia -to-help-afrl-spot-deep -fakes   \n[32] Google, Get helpful context with About this image, https://blog.google/products/search/about -this-\nimage -google -search/   \n[33] Intel, Intel Introduces Real -Time Deepfake Detector, \nhttps://www.intel.com/content/www/us/en/newsroom/news/intel -introduces -real-time-deepfake -\ndetector.html#gs.zllvh5   \n[34] Technica l University of Munich, FaceForensics++: Learning to Detect Manipulated Facial Images, \nhttps://github.com/ondyari/FaceForensics/   \n[35] Google Research, Contributing Data to Deepfake Detection Research, \nhttps://blog.research.google/2019/09/contributing -data-to-deepfake -detection.html   \n[36] Content Authenticity Initiative, https://contentauthenticity.org/  \n[37] FBI, Deepfakes and Stolen PII Utilized to Apply for Remote Work Positions, \nhttps://www.ic3.gov/Media/Y2022/PSA220628   \n[38] The New York Times, Another Side of the A .I. Boom: Detecting What A.I. Makes, \nhttps://www.nytimes.com/2023/05/18/technology/ai -chat-gpt-detection -tools.html   \n[39] Forbes, With Seed Funding Secured, AI Detect ion Tool GPTZero Launches New Browser Plugin, \nhttps://www.forbes.com/sites/rashishrivastava/2023/05/09 /with-seed -funding -secured -ai-\ndetection -tool-gptzero -launches -new-browser -plugin/   \n[40] IEEE Spectrum, Detection Stays One Step Ahead of Deepfakes for Now, \nhttps://spectrum.ieee.org/deepfake   \n[41] Microsoft Research , VALL -E (X): A neural codec language model for speech synthesis, \nhttps://www.microsoft.com/en -us/research/project/vall -e-x/  \n[42] MetaStellar, Adobe, Nvidia announce ethical AI image ge neration, \nhttps://www.metastellar.com/nonfiction/news/adobe -nvidia -announce -ethical -ai-image -generation/   \n[43] Columbia Law School, The CLS Blue Sky B log, https://clsbluesky.law.columbia.edu/   \n[44] ID R&D, IDLive Face Plus Injection Attack Detection Helps Prevent Deepfake Fraud, \nhttps://www.idrnd.ai/idlive -face-plus-injection -attack -detection -deepfake -protection/   \n[45] FaceTec, https://www.facetec .com/  \n[46] iProov, https://www.iproov.com/  \n[47] Clarkson University, Center for Identification Technology Research (CITeR), \nhttps://citer.clarkson.edu/   \n[48] TinEye, https://tineye.com/   \n[49] Google Image Search (search by image), https://images.google.com/   \n[50] Microsoft Bing (search using an image), https://www.b ing.com/   \n[51] Hany Farid, Fake Photos (2019), ISBN: 9780262537490  \n[52] The Conversation, Deepfake audio has a tell  researchers use fluid dynamics to spot artificial \nimposter voices, https://theconversation.com/deepfake -audio -has-a-tell-researchers -use-fluid-\ndynamics -to-spot-artificial -imposter -voices -189104   \n[53] NVIDIA Labs, StyleGAN3 Synthetic Image Detection, https://github.com/NVlabs/stylegan3 -\ndetector   \n\n \n \n \n \nU/OO/199197 -23 | PP-23-3076  | SEP 2023  Ver. 1.0   Joint CSI  | Contextualizing Deepfake Threats to Organizations  \nTLP:CLEAR  \nTLP:CLEAR  18 [54] V7, Fake Profile Detector (Deepfake, GAN), https ://chrome.google.com/webstore/detail/fake -\nprofile -detector -dee/jbpcgcnnhmjmajjkgdaogpgefbnokpcc   \n[55] Antispoofing Wiki, Deepfake Detection Software: Types and Practical Application, \nhttps://antispoofing.org/deepfake -detection -software -types -and-practical -application/   \n[56] Grip-unina, https://github.com/grip -unina   \n[57] The unofficial  deepfake -detection -challenge repo, https://github.com/drbh/deepfake -detection -\nchallenge/   \n[58] Grip-unina, ID -Reveal: Identity -aware Deepfake Video Detection, https://githu b.com/grip -unina/id -\nreveal   \n[59] Davide Cozzolino, Alessandro Pianese, Matthias Niener, and Luisa Verdoliva, Audio -Visual \nPerson -of-Interest DeepFake Detection, https://arxiv.org/pdf/2204.03083.pdf   \n[60] Shruti A garwal, Hany Farid, Yuming Gu, Mingming He, Koki Nagano, and Hao Li, Protecting \nWorld Leaders Against Deep Fakes, \nhttps://farid.berkeley.edu/downloads/publications/cvpr19/ cvpr19a.pdf   \n[61] Maty Bohcek and Hany Farid, Protecting President Zelenskyy Against Deep Fakes, \nhttps://arxiv.org/pdf/2206.12043.pdf   \n[62] U.S. Department of Energy Office of Scientific and Technical Informati on, Speaker -targeted \nSynthetic Speech Detection, https://www.osti.gov/biblio/1844063   \n[63] Debevoise & Plimpton, The Value of AI Incident Response Plans and Tabletop Exercises, \nhttps://www.debevoisedatablog.com/2022/04/27/the -value -of-airps -and-ai-tabletops/   \n[64] SANS, Learn a New Survival Skill: Spotting Deepfakes, \nhttps://www.sans.org/newsletters/ouch/learn -a-new-survival -skill-spotting -deepfakes/   \n[65] MIT Media Lab, Detect DeepFakes: How to counteract misinformation created by AI, \nhttps://www.media.mit.edu/projects/detect -fakes/overview/   \n[66] The MIT Center for Advanced Virtuality, Media Literacy in the Age of Deepfakes, \nhttps://deepfakes.virtuality.m it.edu/   \n[67] Center for an Informed Public at UW, Spot the Deepfake: Spotting deepfakes isnt as easy as you \nmight think, https://www.spotdeepfakes.org/   \nDisclaimer of endorsement  \nThe information and opinions contained in this document are provided \"as is\" and without any warranties or \nguarantees. Reference herein to any specific commercial entity,  product, process, or service by trade name, \ntrademark, manufacturer, or otherwise doe s not constitute or imply its endorsement, recommendation, or favoring by \nthe United States Government, and this guidance shall not be used for advertising or product endorsement purposes.  \nPurpose  \nThis document was developed in furtherance of the authoring  organizations cybersecurity missions, including their \nresponsibilities to identify and disseminate cyber threats to National Security Systems, Department of Defense, \nDefense Industrial Base, and critical infrastructure information systems, and to develop  and issue cybersecurity \nspecifications and mitigations.  This information may be shared broadly to reach all appropriate stakeholders.  \nContact  \nCybersecurity Report Feedback: CybersecurityReports@nsa.gov  \nCISA's 24/7 Operations Center to report incidents and anomalous activity: Report@cisa.gov  or (888) 282 -0870  \nGeneral Cybersecurity Inquiries or Customer Requests: Cybersecurity_Requests@nsa.gov   \nDefense Industrial Base Inquiries and Cybersecurity Services: DIB_Defense@cyber.nsa.gov  \nMedia Inquiries / Press Desk:  \n NSA Media Relations: 443-634-0721, MediaRelations@nsa.gov  \n CISA Media Relations: 703 -235-2010, CISAMedia@cisa.dhs.gov  \n\n",
  "cves": [],
  "techniques": [],
  "advisory": "cybersecurity-alerts",
  "title": "csi-deepfake-threats",
  "source": "nsa",
  "id": "86b19bfb42cf985d6de7d3acf2101f2733dc6f67d8292df716cac148b6adb5f9"
}