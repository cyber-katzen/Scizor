{
  "markdown": " \n \n \n \nNational Security Agency  \nCybersecurity  Technical Report  \n \n  \n \n \n \nDoD Microelectronics:  \nField Programmable Gate Array  \nLevel of Assurance 1 Best Practices  \n \n \n \n \nMay 2024 \n \nU/OO/ 156781 -24 \nPP-24-1960  \nVersion 1.1 \n \n \n \n \n  \n\n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  ii \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \n \n \n This document was created through collaboration with each of the \nJFAC labs: National Security Agency (NSA), Air Force Research \nLab (AFRL) RYDT, Naval Surface Warfare Center (NSWC) Crane, \nand Army Development Command (DEVCOM)/AVMC.  \nFor additional information, guidance, or assistance with this \ndocument, please contact the Joint Federated Assurance Center \n(JFAC) at JFAC_HWA@radium.ncsc.mil . \n  \n  \n\n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  iii \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nNotices and history   \nDocument change history  \nDate  Version  Description  \nDecember  2022  1.0 Initial Publication  \nMay 2024  1.1 Minor updates throughout  \nDisclaimer of warranties and endorsement  \nThe information and opinions contained in this document are provided \"as is\" and without any warranties \nor guarantees. Reference herein to any specific commercial products, process, or service by trade name, \ntrademark, manufacturer, or otherwise, does not constitute or imply its endorsement, recommendation, or \nfavoring by the United States Government, and this guidance shall not be used for advertising or product \nendorsement purposes.  \nPublication information  \nAuthor(s)  \nNational Security Agency  \nCybersecurity Directorate  \nJoint Federated Assurance Center  \nContact information  \nJoint Federated Assurance Center : JFAC_HWA@radium.ncsc.mil  \nGeneral Cybersecurity Report Inquiries: CybersecurityReports@nsa.gov  \nDefense Industrial Base Inquiries and Cybersecurity Services: DIB_Defense@cyber.nsa.gov  \nMedia inquiries / Press Desk: Media Relations, 443 -634-0721, MediaRelations@nsa.gov  \nPurpose  \nThis document was developed in furtherance of NSAs cybersecurity missions. This includes its \nresponsibilities to identify and disseminate threats to National  Security Systems and  Depart ment of \nDefense information systems, and to develop and issue cybersecurity specifications and mitigations. This \ninformation may be shared broadly to reach all appropriate stakeholders.   \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  iv \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nExecutive summary  \nIn support of securing Field  Programmable Gate Array (FPGA) based systems from \nadversary influence during the manufacturing process, this report outlines  the \ncategories of relevant threats and the best practices for mitigating them at Level of \nAssurance 1 (LoA1 ). LoA1 captures the th reats most likely to be exercised against a \nDoD system based upon their low cost and high value of return. At this level, these \nthreats have the following characteristics : \n Access   Exploit a single available point of access , \n Technology   Use existing publi c technology , \n Investment   Require minimal investment of resources , \n Value of effect   Disable or subvert system capabilities , and \n Targetability   Are inherently targetable and controllable.  \nOrganized by threat, this report provides  multiple technical mitig ations to choose from to \nmitigate each threat  and allow the program the best fit for their program needs. The \nfollowing table identifies the threat descriptions (TD) addressed by this guidance.  \n# Threat description (TD)  \nTD 1  Adversary utilizes a known FPGA platform vulnerability  \nTD 2  Adversary inserts malicious counterfeit  \nTD 3  Adversary compromises application design cycle  \nTD 4  Adversary compromises system assembly , keying , or provisioning  \nTD 5  Adversary compromises third -party soft intellectual property (IP)  \nTD 6  Adversary swaps configuration file on target  \nTD 7  Adversary substitutes modified FPGA software design suite  \nTD 8  * Adversary modifies FPGA platform family at design  \nTD 9  Adversary compromises single -board computing system (SBCS)  \n* TD 8 is not a threat at LoA 1; however , JFAC request s that DoD programs even at LoA1 provide \ninformation about FPGA device use as described in JFAC Survey Requ est. This information relates to TD \n8 mitigations.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  v \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nEach subsection in this report contains mitigations described in detail to enable clear \nimplementation  guidelines  for DoD systems . Secondary doc uments are referenced in \ncases where the suggested mitigation is highly detailed, specific to individual FPGA \nplatforms, or subject to frequent change.  Appendix E: Checklist s and \ndata/documentation  requirements  contains a quick reference list of threats and \nassociated data requirements . \nOnce the DoD program has mitigated these threats, they have achieved an assurance \nlevel of LoA 1. \n  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  vi \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nContents  \nDoD Microelectronics: Field Programmable Gate Array Level of Assurance 1 \nBest Practices  ................................ ................................ ................................ ............................. i \nExecutiv e summary  ................................ ................................ ................................ ......................  iv \nContents  ................................ ................................ ................................ ................................ ...... vi \n1. Introduction  ................................ ................................ ................................ ............................  1 \n2. Terms  ................................ ................................ ................................ ................................ ...... 1 \n3. Overview of Level of Assurance 1 threats and mitigations  ................................ .............  2 \n3.1 Complementary standards and guidance  ................................ ................................ ...........................  5 \n3.2 Exclusions  ................................ ................................ ................................ ................................ .......................  6 \n3.3 Document use  ................................ ................................ ................................ ................................ ................  7 \n3.4 General Comments on Mitigations  ................................ ................................ ................................ ........ 8 \n4. Threat Descriptions (TD)  ................................ ................................ ................................ ...... 8 \nTD 1: Adversary utilizes a known FPGA platform vulnerability  ................................ ..............  8 \nTD 1 mitigations  ................................ ................................ ................................ ................................ ..........................  9 \nTD 1 mitigation descriptions  ................................ ................................ ................................ ................................ .. 9 \nUse caution when selecting tools or platforms  ................................ ................................ ..........................  9 \nResearch vulnerabilities  ................................ ................................ ................................ ................................ .... 10 \nUse a revision control/version management system  ................................ ................................ ............  10 \nEnforce auditability  ................................ ................................ ................................ ................................ .............  12 \nPerform a vulnerability data review  ................................ ................................ ................................ ..............  12 \nPerform routine employment monitoring  ................................ ................................ ................................ .... 12 \nPrevent a compromised insider  ................................ ................................ ................................ .....................  12 \nTD 2: Adversary inserts malicious counterfeit  ................................ ................................ ....... 12 \nTD 2 mitigations  ................................ ................................ ................................ ................................ ........................  14 \nTD 2 mitigation descriptions  ................................ ................................ ................................ ................................  14 \nPurchase from DoD authorized vendors and distributors  ................................ ................................ ... 14 \nFollow storage and shipping  guidance  ................................ ................................ ................................ ....... 14 \nValidate the authenticity of the FPGA device ................................ ................................ ...........................  16 \nTD 3: Adversary compromises application design cycle  ................................ ......................  19 \nTD 3 mitigations  ................................ ................................ ................................ ................................ ........................  20 \nUse cleared personnel in a cleared environment  ................................ ................................ ...................  20 \nTD 3 mitigation descriptions  ................................ ................................ ................................ ................................  21 \nTrack critical data in a revision control system  ................................ ................................ ........................  21 \nEnforce auditability  ................................ ................................ ................................ ................................ .............  21 \nUse a re vision control/version management system  ................................ ................................ ............  21 \nTD 3.1: Mitigating the introduction of a compromised design into the application  ........................  22 \nIsolate and store the application design  ................................ ................................ ................................ ..... 23 \nPerform a reproducible build  ................................ ................................ ................................ ...........................  23 \nTD 3.2: Mitigating the modification of test benches or plans to reduce coverage or hide Trojan \ncode  ................................ ................................ ................................ ................................ ................................ ...............  23 \nExecute a documented test plan  ................................ ................................ ................................ ...................  24 \nValidate and verify test  processes  ................................ ................................ ................................ ................  25 \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  vii \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nEnsure the test environment is maintained via configuration management  ................................  25 \nUse a revision control/version management system  ................................ ................................ ............  25 \nTD 3.3: Mitigating the introduction of a Trojan into the application design during \ndevelopment  ................................ ................................ ................................ ................................ ...............................  26 \nEnsure all design artifacts have a direct bi -directional link to approved requirements  ...........  26 \nEnforce peer review  ................................ ................................ ................................ ................................ ...........  26 \nExecute a documented test plan  ................................ ................................ ................................ ...................  27 \nImplement, validate, and verify test processes  ................................ ................................ .......................  28 \nSelect a formal proof process  ................................ ................................ ................................ ......................  28 \nTD 3.4: Mitigating the introduction of compromised tooling or software into the environment  . 29 \nAccept only digitally signed s oftware deliveries  ................................ ................................ ......................  29 \nValidate cryptographic hashes  ................................ ................................ ................................ .......................  30 \nResearch vulnerabilities  ................................ ................................ ................................ ................................ .... 30 \nUse a revision control/version management system  ................................ ................................ ............  31 \nUtilize a reproducible build process  ................................ ................................ ................................ .............  31 \nSelect a formal proof process  ................................ ................................ ................................ ......................  32 \nTD 3.5: Mitigating intrusion into the internal network  ................................ ................................ .................  32 \nUse a revision control/version management system  ................................ ................................ ............  33 \nAssign privileges and accesses based on roles  ................................ ................................ .....................  34 \nControl and monitor access  ................................ ................................ ................................ ............................  34 \nResearch vulnerabilities  ................................ ................................ ................................ ................................ .... 35 \nPurchase from DoD authorized vendors and distributors  ................................ ................................ ... 35 \nUse protected computing environments  ................................ ................................ ................................ .... 35 \nTD 3.6: Mitigating risk from a compromised hire or employee  ................................ ..............................  36 \nEnforce auditability  ................................ ................................ ................................ ................................ .............  36 \nTrack critical data in revision control  ................................ ................................ ................................ ...........  36 \nAdopt a structured application design process  ................................ ................................ .......................  37 \nReview critical activities  ................................ ................................ ................................ ................................ .... 37 \nEnforce reviewer criteria  ................................ ................................ ................................ ................................ ... 37 \nTD 4: Adversary compromises system assembly, keying, or provisioning  ........................  38 \nTD 4 mitigations  ................................ ................................ ................................ ................................ ........................  39 \nTD 4 mitigation descriptions  ................................ ................................ ................................ ................................  40 \nPurchase from DoD and vendor authorized distributors  ................................ ................................ ..... 40 \nFollow storage and shipping guidance  ................................ ................................ ................................ ....... 40 \nProvide keys and configuration data  ................................ ................................ ................................ ...........  41 \nClear memory devices  ................................ ................................ ................................ ................................ ....... 41 \nProvision private keys  ................................ ................................ ................................ ................................ ........ 42 \nProtect the FPGA from attack during assembly and provisioning  ................................ ...................  42 \nAuthenticate the FPGA device  ................................ ................................ ................................ .......................  43 \nTD 5: Adve rsary compromises third -party soft IP  ................................ ................................ .. 44 \nTD 5 mitigations  ................................ ................................ ................................ ................................ ........................  44 \nTD 5 mitigation descriptions  ................................ ................................ ................................ ................................  44 \nPurchase from DoD authorized vendors and distributors  ................................ ................................ ... 44 \nOnly accept IP that is unobfuscated  ................................ ................................ ................................ ............  45 \nValidate the cryptographic hash of the IP  ................................ ................................ ................................ .. 45 \nCheck IP into revision control  ................................ ................................ ................................ .........................  45 \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  viii \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nExamine IP for malicious functions  ................................ ................................ ................................ ..............  45 \nTD 6: Adversary swaps configuration file on target  ................................ ...............................  45 \nTD 6 mitigations  ................................ ................................ ................................ ................................ ........................  47 \nTD 6 mitigation descriptions  ................................ ................................ ................................ ................................  47 \nIncorporate cryptographic authentication  ................................ ................................ ................................ .. 47 \nAuthenticate configuration data each time the data is loaded  ................................ ..........................  48 \nPrevent direct read back ................................ ................................ ................................ ................................ ... 48 \nUse a CNSS/NIST -approved algorithm and key length  ................................ ................................ ....... 48 \nDisable operation or use of test access pins  ................................ ................................ ............................  48 \nEnsure authentication is enabled for application modifications  ................................ ........................  48 \nUse a FIPS 140 -2 compliant, Level 2 HSM  ................................ ................................ ..............................  50 \nTD 7: Adversary substitutes modified FPGA software d esign suite  ................................ .... 50 \nTD 7 mitigations  ................................ ................................ ................................ ................................ ........................  51 \nTD 7 mitigation descriptions  ................................ ................................ ................................ ................................  51 \nPurchase from DoD authorized vendors and distributors  ................................ ................................ ... 51 \nPrevent automatic tool updates  ................................ ................................ ................................ .....................  51 \nUse a protected computing environment  ................................ ................................ ................................ ... 51 \nValidate the cryptographic hash  ................................ ................................ ................................ ....................  52 \nTD 9: Adversary compromises single -board computing system (SBCS)  ...........................  54 \nTD 9 mitigations  ................................ ................................ ................................ ................................ ........................  54 \nTD 9 mitig ation descriptions  ................................ ................................ ................................ ................................  54 \nPurchase from DoD authorized vendors and distributors  ................................ ................................ ... 54 \nAuthenticate the FPGA devices ................................ ................................ ................................ .....................  55 \nPopulate and inspect the SBCS  ................................ ................................ ................................ ....................  55 \nDocument the steps taken to demonstrate compliance  ................................ ................................ ....... 56 \n5. JFAC Survey Request  ................................ ................................ ................................ .........  56 \n6. Summary  ................................ ................................ ................................ ..............................  57 \nAppendix A: Standa rdized terminology  ................................ ................................ ...................  58 \nAppendix B: IP reuse guidance  ................................ ................................ ................................ . 61 \nReuse Conditions  ................................ ................................ ................................ ................................ .....................  61 \nReuse Scenarios  ................................ ................................ ................................ ................................ ......................  62 \nAppendix C: JFAC FPGA reporting template  ................................ ................................ ..........  64 \nAppendix D: Guidance for embedded FPGA IP  ................................ ................................ ...... 68 \nLoA1 Introduction  ................................ ................................ ................................ ................................ .....................  68 \neFPGA Guidance  ................................ ................................ ................................ ................................ .....................  68 \nTD 3: Adversary compromises application design cycle  ................................ ................................ ..... 68 \nTD 4: Adversary compromises system assembly, keying, or provisioning  ................................ .. 69 \nTD 5: Adversary compromises third -party soft IP  ................................ ................................ ..................  69 \nTD 6: Adversary swaps configuration fi le on target  ................................ ................................ ...............  69 \nTD 7: Adversary substitutes modified FPGA software design suite  ................................ ...............  70 \nTD 10: Adversary modifies FPGA software design suite  ................................ ................................ .... 70 \nAppendix E: Checklists and data/documentation requirements  ................................ ..........  71 \nChecklist for TD 1: Adversary utilizes a kno wn FPGA platform vulnerability  ................................ ... 71 \nChecklist for TD 2: Adversary inserts malicious counterfeit  ................................ ................................ .... 73 \nChecklist for TD 3: Adversary compromises application design cycle  ................................ ...............  75 \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  ix \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nChecklist for TD 4: Adversary compromises system assembly, keying, or provisioning  ............  86 \nCheckl ist for TD 5: Adversary compromises third -party soft IP  ................................ .............................  88 \nChecklist for TD 6: Adversary swaps configuration file on target  ................................ .........................  89 \nChecklist for TD 7: Adversary substitutes modified FPGA software design suite ..........................  90 \nChecklist for TD 9: Adversary compromises single -board computing system (SBCS)  ...............  91 \n \n \nTables  \nTable 1: LoA1 threats  ................................ ................................ ................................ ................................ ....................  5 \nTable 2: List of AS6171 slash sheets  ................................ ................................ ................................ ...................  18 \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  1 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \n1. Introduction  \nThis document provides JFACs recommended hardware assurance \nstrategies for Field Programmable Gate Array (FPGA) devices  used by \nDoD programs . The guidance outlined by this document provides hardware assurance \nto systems requiring Level of Assurance 1 (LoA1 ). Additionally, it provides the requisite \nstrategies and details for implementing each threat mitigation. Secondary documents \nare referenced in cases where the suggested mitigation is highly detailed, specific to \nindividual FPGA platforms, or subject to frequent change.  \nThe mitigations included within this document are the responsibility of the program, to \ninclude all subcontractors. These miti gations are not written for third -party providers to \nuse for the purpose of becoming LoA compliant. Since the third -party provider \nrepresents potential malicious access points and the program has no positive control \nover them, work or products coming fro m them should be verified against threats. Third -\nparty statements of mitigation resol ution are not satisfactory; the  program has the \nresponsibility to verify the mitigation . Third -party entities could include third -party IP \nproviders (3PIP), software vendo rs, and manufacturers.  \nThis guidance is meant to stand on its own and not require the participation of JFAC in \nthe development process of a programs product, unless required by a specific \nmitigation. However, JFAC does remain at the ready to aid programs who seek to better \nunderstand this gui dance, to incorporate a program specific mitigation , or are seeking \nalternatives to the guidance contained herein. For further information or support, please \ncontact JFAC_HWA@radium.ncsc.mil . \n2. Terms  \nTo aid the reader, a glossary of terms is at the end of this document in Appendix  A. \nHowever, it will be helpful to highlight several terms before proceeding. Below are \nseveral important terms to understand:  \n Application or application design  a DoD programs design that is programmed \ninto an FPGA device.  This can refer to the design in any of its various format s. \nLoA1  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  2 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \n Configuration file or data  also known as a bitstream, this is th e data used to \nconfigure the FPGA including the application design and all other programming \ninformation.  \n FPGA d evice or device  a specific individual FPGA packaged device.  \n Hard IP  a hardware design , also called intellectual property  or IP , which  is \nrepresented by its physical layout format. In FPGAs,  this is IP that is embedded \nby the FPGA vendor into the FPGA physical design.  \n Soft IP  IP that is represented in a logical human readable format such as a \nhardware description language.  \n Platform design  the design of an FPGA family of devices, not just an individual \ndevice.  \n3. Overview of Level of Assurance 1 threats and mitigations  \nLoA1  requires mitigations against FPGA assurance threats that have the following \ncharacteristics:  \n Access  A single point of access  to some portion of the FPGA supply chain.  \nThis point of access does not contain cleared personnel.  This is defined by the \nfollowing:  \n An Internet connected network, regardless of other security measures  \n Any single uncleared U.S. person  \n A group of as sociated foreign nationals within a U.S. organization, such as \na corporate office operated in a foreign country  \n A foreign owned company servicing part of the supply chain  \n Any number of foreign nationals from a high threat country or its allies with \naccess to some part of the FPGA supply chain  \nFor a mitigation based on access to be effective, it needs to raise the access required to \ncarry out the attack to one necessitating multiple points of access  or complex points of \naccess . For example , the access mitiga tion could result in an attack needing access \nfrom differing areas of the supply chain or by multiple personnel in an area  of the supply \nchain.   \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  3 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \n Technology  Existing public technology  means that an attack can be \nconducted using tools that are already avai lable in the public or commercial \ndomain , or are straightforward advances of public technology. Examples would \ninclude:  \n Development tools provided by FPGA vendors  \n Internal debugging features that are capable of changing device \nconfiguration  \n Lab equipment used as intended  \n Publicly available open source projects  \n Published academic research  \n Results of U.S. Government (USG) Research & Development (R&D) \ninvestment at the unclassified level, even when protected by International \nTraffic in Arms Regulations  \nFor a mitigation based on technological complexity to be effective, it must increase the \nlevel of technology needed to carry out the attack to that which is beyond what can be \nfound in the public domain. The most common method to achieve this mitigation is to \napply cryptographic authentication based on USG standards.  \n Investment  Minimal  investment of resources  means that an attack requires \na team with existing FPGA knowledge and skills and individuals with domain \nknowledge in the technology area of the system being assured. Minimal  \nresources are defined as any effort consisting of less than six person -years of \nFPGA/domain expertise focused solely on attacking the target of interest.  \nFor a mitigation based on investment of resources to be effective, it must for ce the \nattacker to expend greater resources in the form of engaging a complex interdisciplinary \nteam comprised of a mix of specialties that are outside of the application design domain \nand FPGA technology to carry out an attack. Additionally , the mitigatio n would lengthen \nthe time necessary to develop the attack to more than six person -years . Obfuscation of \ndesign data is not considered effective in this context. Common use of obfuscation has \nhistorically led to technological development in those additional  disciplines that quickly \nnullified the obfuscation.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  4 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \n Value of Effect   Attacks that disable or subvert capabilities  enable an \nadversary to remove a capability from service or cause it to perform specific \ndeleterious actions. When combined with high targeta bility, these represent the \nworst -case scenario for a failure of hardware assurance: enabling an adversary \nto take over or disable capabilities on command.  \nFor a mitigation based upon value of effect to the adversary, it must constrain the \nseverity of the  outcome on the target to one of lesser effect.  \n Targetability - Inherently targetable and controllable  threat operations are \nexecuted in a way that provides straightforward means to understand  and predict \nthe effect of an attack , and also provide a mechani sm to control or time the \nattack. For example, an adversary with the ability to introduce new code into a \nsystem design can implement a broad number of malicious functions. A denial of \nservice attack falls in this category, if and only if, it is possible f or the adversary to \ncontrol when it takes effect after the device is fielded. However, a simple \nreduction in reliability not tied to any trigger, which therefore cannot be controlled \nor timed in a planned way, does not fall in this category.  \nFor a mitigat ion based on targetability to be effective, it must prevent the attack from \nbeing controlled effectively. That is, it must prevent the FPGA device from being used to \nattack a specific target at a specific time with a controllable trigger. This can be \naccom plished by performing mitigations that specifically target opportunities for \ncommunication.  \nFor a program to achieve Level of Assurance 1, it must provide mitigations against \nthreats that fall within these characteristics. LoA1 addresses threats that origi nate from \nan adversary whose intent is malicious and does not cover commercial assurance risks , \nsuch as re -marked parts. Economically motivated assurance threats have reliability \nrisks associated with them. These threats should be addressed by the reliability testing \nof a program. For programs with stringent or specific reliability requirements, it is \nstrongly recommended that the appropriate level of testing be conducted to ensure the \nproper operation of the product rather than relying on assurance mitigations. However, \nall programs with radiation -hardened requirements are an exception and in almost  all \ncases should mitigated at a Level of Assurance 2 or Level of Assurance 3.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  5 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nThe following table lists the nine FPGA threats that are addressed by LoA1. Each threat \nis explained and accompanied by examples  in more detail within the JFAC FPGA Best \nPractic es  Threat Catalog . \nTable 1: LoA1 threats  \n# Threat description (TD)  \nTD 1  Adversary utilizes a known FPGA platform vulnerability  \nTD 2  Adversary inserts malicious counterfeit  \nTD 3  Adversary compromises application design cycle  \nTD 4 Adversary compromises system assembly , keying , or provisioning  \nTD 5  Adversary compromises third -party soft intellectual property ( IP) \nTD 6  Adversary swaps configuration file on target  \nTD 7  Adversary substitutes modified FPGA software design suite  \nTD 8* Adversary modifies FPGA platform family at design  \nTD 9  Adversary compromises single -board computing system (SBCS)  \n*TD 8 does not become a relevant threat until L oA2; however, JFAC requires LoA 1 programs to answer \nan information query required by TD8 mitigations.  \nEach threat  listed here has corresponding mitigations . These mitigations are derived \nfrom various commercial/government standards and existing best practices . The use of \nthese standards  and best practices should not preclude the use of any other standards \nor best practices.  In particular, DoD projects identified as National Security Systems \n(NSS) should utilize the appropriate guidance as required by the Committee on National \nSecurity Systems (CNSS ). \n3.1 Complementary standards and guidance  \nMicroelectronic  quantifiable assurance (MQA) standards are intended to be \ncomplementary to other government and industry  recognized risk management \npractices and standards. The following are st andards for various mitigations:  \n National Institute of Standards and Technology (NIST ) Federal Information \nProcessing Standards Publication (FIPS)  186 Digital Signature Standard  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  6 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \n NIST FIPS 198 The Keyed -Hash Mes sage Authentication Code (HMAC)  \n NIST Special P ublication (SP) 800-53 Security and Privacy Controls for Federal \nInforma tion Systems and Organizations  \n NIST SP 800-57 Recommendation for Key Man agement  \n The Configuration Management section of NIST SP 800-60 Systems Security \nEngineering: Considerations for a Multidisciplinary Approach in the Engineering  \nof Trustworthy Secure Systems  \n NIST  SP 800-171 Protecting Controlled Unclassified Information in Nonfedera l \nSystems and Organizations  \n NIST SP 800-172 Enhanced Security Requirements for Protecting Cont rolled \nUnclassified Information  \n Committee on National Security Systems ( CNSS ) Policies   \n SAE International AS6171 Test Methods Standard; General Requirements, \nSuspect/Counterfeit, Electrical, Electro nic and Electromechanical Parts  \n Trusted Sys tems and Network (TSN)  Analysis  \n Defense Acquisition Guidebook Chapter  Nine  Program Protection Plan  \n DoD guidance for storage of Secret materials can be found in DODM 5200.01 -\nV3. \n JFAC FPGA Best Practices Documents  contact JFAC for available documents \nto support implementation  practices for t he FPGA standards in this guide . \nProgram offices should review and adhere to the standards provided in each document, \nas applicable. Additionally, programs are encouraged to apply  applicable  standards in \naddition to the standards described in this document . \n3.2 Exclusions  \nThis FPGA Level of Assurance 1 Best P ractice  guide  does not address the following \nconcerns:  \n Non-malicious and profit driven reliability risks , such as re -marked parts. \nPrograms are responsible for establishing and enforcing sy stem reliability \nrequirements. However, compliance to SAE International AS 6171 Test Methods \nStandard: General Requirements  Suspec t/Counterfeit, Electrical, Electronic and \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  7 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nElectromechanical Parts  as recommended by this report is an effective detection \nmechanism for these kinds of counterfeit parts.  \n Threats to the confidentiality of the application design. The program application \ncan be loaded apart from the manufacturing process and under the protection \nand oversight of the program. Confidentiality is p reserved using existing \nengineering practices, bitstream encryption , and other anti -tamper practices. For \nmore g uidance in this area, see the Do Ds Anti -tamper Executive Agent  \n(https://at.dod.mil ). \n3.3 Document use  \nThese FPGA  assurance best practices instruct programs on protecting manufacturing \nand provisioning processes from adversarial influence. Specifically, they apply to the \nmanufacturing, acquisition, programming , and first attachment of the FPGA devices. \nThe program should define its own protection methods as boards become integrated \ninto subcomponents, components, and then final systems.  \nFor LoA1  compliance, each program should perform each mitigation listed in the  TD # \nMitigation s section s. The  Mitigation Descript ions section s provide details for each \nmitigation.  In some cases, the full description contains additional options that are \nrequired to be LoA1  compliant.  An asterisk * next to any mitigation indicates additional \noptions should be implemented.  \nWhen mitigations for all the threats listed under LoA1  are completed, that device can be \nsaid to have achieved LoA1 . However, compliance with LoA1  can be impacted by \nchanges in several areas during the systems life.  \nThe Program Protection Plan (PPP) emph asizes the need to maintain and update \nprotection measures throughout the lifecycle of a program. It is strongly recommended \nthat each program identify events that would trigger a review of the PPP and hardware \nassurance practices after fielding. These eve nts should include but not be limited to:  \n Changes to the system , \n Changes to the supplier of critical components including the FPGA devices , \n Changes to the FPGA design software (new releases, fixes, etc.) , \n Changes to the threat environment , and  \n Revelations of new vuln erabilities to the FPGA devices . \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  8 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nThe PPP documents list resources with which the program can track the latest available \nintelligence on threats  and supply chain vulnerabilities. It is recommended that c hanges \nin any of these areas should prompt  a review of the most up -to-date assurance \nmitigations against the triggering event. If threats or vulnerabilities threaten the system, \nit is recommended that new mitigations should be implemented to remain compliant to \nLoA1 . Absent any changes in these are as, the devices should be considered to have \nachieved LoA1 . \n3.4 General Comments on Mitigations  \n Programs are encouraged to own as much of the fabrication process as possible \nand avoid third parties to the fullest extent possible.  \n When third party sources are r equired, p rograms are encouraged to diversify \ntheir supply sources to minimize malicious targeting.  \n Programs are encouraged to utilize cleared personnel and classified resources to \nthe fullest extent possible.  \n Programs are encouraged to use verification of  all manufacturing steps to the \nfullest extent possible.  \n4. Threat Descriptions (TD)  \nTD 1: Adversary utilizes  a known FPGA platform v ulnerability  \nIn this attack, a foreign adversary utilizes a vulnerability in an FPGA platform or vendor \ndevelopment softwar e package to initiate an attack. A t LoA1 , a vulnerability is  an \nunclassified published weakness or vulnerability in the design of a specific FPGA \nplatform or software program that would allow the attacker  the ability to use it for \nmalicious purposes.  \nVulnerabilities  could allow for leakage of sensitive information or keys; compromise of \nsecurity or tamper detection functions; or unauthorized reconfiguration of the product. \nUnclassified and public  vulnerabilities are published in various places , includ ing vendor \nadvisories, errata bulletins, and  databases , such as those listed in the Research  \nVulnerabilities  mitigation below . This threat can be introduced by  a program not \nperforming vulnerability research , an insider not disclosing the fact of the vulner ability \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  9 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nsuch that it may be used for nefarious purposes or adding/modifying design features for \nuse with  or triggering  the vulnerability.  \nTD 1 mitigations  \n Use caution when  select ing tools or platforms . When possible do not select tools \nor platforms  that a re end -of-life or beta/initial releases . Also, e nsure  known \nvulnerabilities in tools/platforms  have been adequately addressed in newer \nreleases.  \n *Research vulnerabilities  affecting  tools/platforms . \n Use a revision control/version management  system  that includes document/data \ncontrol, document/data release, backups and archives, refresh of backup media, \nsoftware, test equipment , and test environment.  \n Enforce auditability  of the application  requirements, architecture, design , code, \ntests, bugs, and fixes. At a minimum, audit data should include what decisions \nwere made, by whom, for what reason, and on what date.  \n Perform a vulnerability data review . \n Perform routine employment monitoring  to unmask a compromised insider who \nmight hide the existence of a vulnerability . \n *Take action to prevent a compromised insider  from hiding a vulnerability.  \nTD 1 m itigation descriptions  \nUse caution when s elect ing tools or platforms  \nConsider the longevity o f selected tools and FPGA platforms. Newly released devices \nmay not yet have a vulnerability history . Programs  should proceed with caution  when \nusing newly released devices . End-of-life devices may not have support to mitigate \nvulnerabilities once identifi ed. \nIn general, JFAC recommends  program s use more modern device families when \npossible . These families  possess more mature design architectures that encompass \nvulnerability fixes and advanced assurance features.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  10 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nResearch vulnerabilities  \nResearch the respective FPGA platform and software for existing vulnerabilities in \ndatabases such as:  \n Common Vulnerabilities and Exposures (CVE)  https://cve.mitre.org   \n Common Weakness Enumeration (CW E)  https://cwe.mitre.org   \n NIST National Vulnerabilit y Database (NVD)  https://nvd.nist.gov   \n Government Industry Data Exchange Program (GIDEP)  \nhttps://www.gidep.org/products/products.htm  \n DISA Security Technical Implementation Guides  (STIGs)  \nhttps://public.cyber.mil/stigs/   \n Searches for vendor advisories , errata bulletins , publications , and academic \npapers detailing vulnerabilities in the device in question.  \nIf vulnerabilities are found in the FPGA device, choose one  of the following options:  \nOption 1 : Select a different FPGA platform device or software that does not have \npublished vulne rabilities and that meet s the program requirements . \nOption 2:  Use standard formal processes and procedures to work with the vendor to \nresolve the vulnerability. Once a fix is identified, only accept formal releases, do not \naccept custom beta fixes, custom patches, etc. for incorporation . \nOption 3 : The program can internally determine the vulnerability poses no significant \nrisk to their product . JFAC is available to provide assistance in assess ing the risk that \nthe vulnerability poses to the system and acqui re recommended mitigations for a \nparticular vulnerability.   \nNote: If a vulnerability is identified, it is recommended to report it to Government \nIndustry Data Exchange Program (GIDEP) and to contact the vendor so they may \ncorrect it . \nUse a r evision control/version management  system  \nTo prevent vulnerable software from being loaded into the environment, it is important \nthat robust configuration management and revision management systems are in place. \nBefore any version is installed, research should be performed for any known \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  11 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nvulnerabilities, how they have been addressed, and their impact s and vulnerabilities . \nOnly au thorized changes are allowable.  All changes to the system or artifacts should be \ndocumented, approved , and auditable.  \nThese systems should  fulfill the following requirements:  \n Allow only authorized system administrators to make changes to the underlying \nrevision control tool and underlying server.  \n Use a backup system that syncs to the primary and is maintained by a separate \nadministrator. Eac h system should be managed by separate system \nadministrators . \n Enforce administrative restrictions;  restrict privileged access to only an \nauthorized set  of administrators;  limit what users can do to the database; ensure \nall users are verified; encrypt datab ase information both in transit and at rest; \nenforce secure passwords; introduce role -based access control and privileges; \nand remove unused accounts.  \n Remove any components or functions that are not necessary (for example, \nremove all sample files and defau lt passwords).  \n Ensure the system provides a complete and immutable , long-term change history \nof every file. The system should  log every change made by individuals. This \nincludes changes such as creating and deleting files and editing content.  The \nhistory should  identify the person who made  the change, what was changed, the \ndate of the change, and the purpose of the change.  \n Ensure the system stores a reliable copy of assets that are currently in \nproduction.  \n Ensure the system stores reliable copies of previo us production versions of \nassets, allowing for the complete retrieval of those versions.  \n Ensure password best practices (password rotation, length, etc.) are enforced. In \nlieu of a password, two -factor authentication can be utilized.  \n All changes to the sys tem or artifacts should be documented, approved, and \nauditable.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  12 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nEnforce auditability  \nEnforce auditability of the requirements, architecture, design , code, test s, bugs, and \nfixes. At a minimum, audit data includes what decisions were made, by whom, for what  \nreason, and on what date.   \nPerform a v ulnerability data review  \nTo prevent a compromised insider from hiding a vulnerability , ensure  all critical activities \nare identified and documented . Ensure the entire  design is reviewed by multiple people \nor a cleared  individual . The original designer should not be the responsible party for \nperforming the review . The reviewers should assess  all vulnerability activities, including  \nidentification of vulnerabilities and the appropriateness of the mitigations.  \nPerform r outine employment monitoring  \nPerform routine employment monitoring in which  employee  work patterns are observed. \nPatterns to look for include hostility toward other employees, late or excessive missing \nwork, unexplained work outside normal work hours, and dec lining performance.  \nPrevent a c ompromised insider  \nTo limit the potential effects of a compromised insider , select one of the following \noptions:  \nOption 1: Ensure all critical activities are identified and documented.  Independent third -\nparty reviewers should  assess  all vulnerability activities, including  identification of \nvulnerabilities and whether  the approp riate mitigations are in place. Note:  For LoA1, \nindependent is defined as not the originator . The reviewer can be on the same team if \nnecessary.  \nOptio n 2: Perform designated work using personnel with at least a Secret -level \nclearance.  \nTD 2: Adversary inserts  malicious counterfeit  \nAt LoA1,  this threat  assumes that the adversary already has  access to an existing  \nfabrication process for manufacturing counterfeits . Using this access, the adversary  \ninserts additional logic in the FPGA die for their malicious purposes. In general, \nmodifications to an FPGA design during the fabrication phase  are considered to require \na high effort.  However, in cases where an adversary is known to have already  \ncounterfeited a device , that effort has already been invested and is thus reduced. \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  13 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nExplicitly, this threat is only of a counterfeit that contains functional differences  that are \nsecurity relevant.  \nWhile commercial (non -malicious) counterfeits , such as re -marked parts , may represent \na reliability risk, they are not included under this level of assurance. Those counterfeits \nare not malicious by design, not controllable/target able, and are economic in nature. \nPrograms with specific reliability requirements should plan for the appropriate level of \ntesting to verify that their design and components meet those goals. The exception to \nthis statement are parts that have radiation -harden ing requirements.  In all cases, these \ndevices should comply with Level of Assurance 2 or Level of Assurance 3 in addition to \nthe appropriate level of reliability testing to secure the operation of their design.  \nThe insert ion of  counterfeit parts can happen dur ing any part of a device s lifecycle.  This \nincludes prior to purchase, during  transit, while in storage by the program, during \nassembly , and at distribution prior to fielding.  \nJFAC relies on substantial physical device inspection  to address these threats because \nthe program has no positive control over the fabrication facility or its processes . Most of \nthe FPGA fabrication facilities are foreign owned, not aligned  to the goals of the United \nStates , and not controllable by the program or D oD. JFAC can identify numerous \ntechnically feasible  attacks for all fabrication countermeasures considered.  \nOverlapping personnel and multi -party review in the verification process  along with \ncryptographically protected IDs  and reliability testing of sampled devices  provides \nadditional assurance protections . \nGuidelines for conducting physical inspection are provided by the SAE AS6171 \ncounterfeit detection standard. These guidelines are organized into slash sheets. Each \nslash sheet is a description of a singular type  of inspection process. For the purposes of \nthis document, the slash sheets may be divided into several purposes:  \n Slash sheet s 2-10: describe physical inspections able to identify devices that \nwere manufactured in an unauthorized fab.  \n Slash sheet 11: descr ibes physical inspections able to identify maliciously altered \ndevices that were manufactured in an authorized fab.  \n Slash sheets 3, 4, 6, 10: describe physical inspections intended to uncover \nmalicious alterations made to the package internals of an authen tic device.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  14 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nMore details regarding the physical inspection process are outlined in the mitigations  \nbelow . \nTD 2 mitigations  \n For DoD system owners, purchase from DoD authorized vendor s and \ndistributors . The D oD program acquisition grou p can provide this information.   \n Follow storage and shipping  guidance  for classified Secret or Trust Category I \nmaterials when storing and transferring FPGA devices between locations .  \n Validate the authenticity of the FPGA device . \nTD 2 mitigation descriptions  \nPurchase from D oD authorized vendors and distributors  \nFor DoD system owners, u tilize DoD  authorized vendors and distributors for all \npurchases.  Authorized vendors c an be  identified  through the acquisition organization.  \nFollow storage  and shipping  guidance   \nThe program sh ould document, maintain, and enforce both device storage  and shipping \nprocedures. Minimally, the procedures  should enforce the verification and acceptance \ntestin g of all devices upon receipt. In addition to device verification, storage  and \nshipping processes should focus on preventing and detecting  adversary access to the \nparts. Access controls  should be designed to thwart those seeking to alter or rep lace \nthe devices while in storage  or shipping.   \nStorage  protections  should include the following characteristics:   \n Production  devices should be stored and maintained in a restricted area separate \nfrom non -production devices ( i.e., ones for design, test, etc.).  \n The restricted area should enforce access control s that limit  access to only those \npersonnel who require access to suppo rt direct job responsibilities.  This \nshould  exclude all members of the design team.   \n All accesses to the restricted area should be recorded and audited.   \n The FPGA devices should be audited on a reg ular cadence to ensure they have not \nbeen removed or modified.   \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  15 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \n Production devices should be continuously tracked to include arrival of the device by \nunique identifier, interaction anyone has with the device, and exi t of the device from \ninventory.  \n The res tricted area should have a clearly defined perimeter, but physical barriers are \nnot required.   \n Personnel within the area should be responsible for challenging all persons who may \nlack appropriate access. Accesses to t he restricted area should be audited to  \ninclude data containing who entered/exited the area, with a timestamp, and reason \nfor entry.   \n When possible, the FPGA devices should be stored within a tamper detection seal \nor tape to detect unauthorized access.   \nShipping protections  should include the f ollowing characteristics:   \n Following acceptance testing, non -configured and non -keyed devices should be \nusing a commercial carrier that has been approved by the appropriate Cognizant \nSecurity Agency  (CSA) to transport Secret shipments , although  the material is not \nSecret. Commercial carriers may be used only within and between the 48 \ncontiguous States and the District of Columbia or wholly within Alaska, Hawaii, \nPuerto Rico, or a U.S. possession or trust territory.   \n Utilize protections against p ilferage, theft, and compromise , including:  \n Hardened containers   \n Tamper detection seals and tapes.  The seals should be numbered,  and the \nnumbers indicated on all copies of the bill of lading (BL). When seals are \nused, the BL should  be annotated substantially as follows:  \nDO NOT BREAK SEALS EXCEPT IN CASE OF EMERGENCY OR UPON \nPRIOR AUTHORITY OF THE CONSIGNOR OR CONSIGNEE. IF FOUND \nBROKEN OR IF BROKEN FOR EMERGENCY REASONS, APPLY \nCARRIER'S SEALS AS SOON AS POSSIBLE AND IMMEDIATELY NOTI FY \nBOTH THE CONSIGNOR AND THE CONSIGNEE.   \n Closely track s hipment times and depot stops.  Packages that are lat e should be \nconsidered suspect.   \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  16 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nValidate the a uthenticity of the FPGA device  \nTo validate the a uthenticity of the FPGA devices , choose one of the f ollowing options \n(descriptions are below):  \nOption 1: Use an FPGA  device that includes  a cryptographically secure ID  that can be  \nvendor verified . \nOption 2: Perform physical analysis  on a random sampling of devices to detect  \ncounterfeit parts .  \nCryptographic ally secure  identifier  \nFor LoA1 , the program should u tilize an FPGA device that incorporates a  \ncryptographically protected ID that can be verified against information sent by the  \nvendor (not the authorized distributor). The use of this type of device ID mi tigates the \nsub-threat of counterfeit parts made in an existing, non -authorized fabrication facility . \nWhile the specifics of each FPGA vendor and platform vary, many newer FPGA \nplatforms contain this type of anti -counterfeiting feature. When these features  are \nsufficiently secure, such mechanisms provide an extremely cost -effective method to \ndetect counterfeits both at acquisition and throughout the FPGA devices lifecycle in a \nsystem. The two biggest advantages of such techniques are the ability to validat e a \ndevice remotely and the ability to non -destructively re -validate  a device at any time.  \nIn contrast to physical anti -counterfeiting techniques, properly implemented \ncryptographic identifiers do not require destructive analysis for verification. A typical \nscheme could validate such a device simply by placing it in a socket. A design can \nfacilita te access to the identifier through local access , such as a board header , or \nremotely. Depending on the exact mitigations selected, this potentially saves two \ndistinct destructive steps: one at acquisition of the devices and one after assembly of \nthe printed circuit board  (PCB).  \nThis kind of validation is where details matter, each FPGA vendor offers a unique \napproach, and each FPGA platform offers a unique variation. In no case is a fully \nreadable ID acceptable. Instead, these schemes all detail cases whe re the device \npossesses a specific private cryptographic key. The device ID in this scheme can be \ncloned only if an adversary is able to get access to that private key. Regardless of the \nspecific platform used , the public keys/identifiers of the devices be ing authenticated \nshould  be delivered and maintained in a secure way. For delivery, the vendor should  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  17 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nprovide this information to the program using a NIST approved authentication algorithm \nto transmit the data. Ex amples would be an ECC -signed e mail with a verified certificate, \nor an https -based file distribution system using a verified certificate. Once received, the \nintegrity of that list should  be maintained by storing with protections appropriate to \nCritical Protected Information (CPI). This should inclu de restricted role -based access on \na network that is compliant with the contract defined Cybersecurity Maturity Model \nCertification ( CMMC ) level.  \nSpecific criteria required for an appropriate device ID to support anti -counterfeiting  are \nbelow:  \n Cryptographi cally protected IDs utilize a private asymmetric key for which no \nread function exists. This should  use a National Institute of Standards and \nTechnology (NIST) approved asymmetric authentication algorithm.  \n The provenance of the key should  be understood in  detail.  \n The device should  be able to authenticate a nonce using this key. Each devices \nID should  be authenticated by the vendor -provided public key through decryption \nof the nonce.  \nPhysical analysis   \nPerform phys ical analysis on a sampling of random devices to detect counterfeit parts. \nThis analysis applies specific, industry standard counterfeit inspection techniques, \nincluding package analysis, x -ray of the part, and examination of the die with \ncomparisons against FPGA vendor -provided golden samples . This physical analysis is \nintended to catch parts that have been remarked or contain counterfeit die. The details \nof what steps to conduct in the analysis and recommendations on how to execute them \nare contained in the commercial standard document, SAE T est Methods Standard; \nGeneral Requirements, Suspect/Counterfeit, Electrical, Electronic, and \nElectromechanical Parts , AS6171 . These inspections should be carried out by cleared \npersons at a Secret level or higher  or a lab independent of the program or its \nperformers.  \nPhysical analysis is a sequence of device analysis steps, from least destructive to most \ndestructive, designed to ensure that the part in question is authentic. If a device fails a \ngiven step, it is not authentic, and there is no need to comple te further steps. If all steps \nare completed and the device passes, it is likely authentic, with likelihood \ncommensurate with the amount of effort it would take to get a counterfeit device to pass \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  18 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nthese tests, subject to LoA1  criteria . Each AS6171 test is detailed in a separate \ndocument called a slash sheet. Listed below are the slash sheets that comprise the \nstandard.  Users should ensure to use the latest version of AS6171 and associated \nslash sheets.  \nTable 2: List of AS6171 slash  sheets \nTest Number  Description  \nAS6171  Test Methods Standard; General Requirements, Suspect/Counterfeit, \nElectrical, Electronic, and Electromechanical Parts  \nAS6171/1  Suspect/Counterfeit Test Evaluation Method  \nAS6171/2  Techniques for Suspect/Counterfeit EEE Parts Detection by External \nVisual Inspection, Remarking and Resurfacing, and Surface Texture \nAnalysis Test Methods  \nAS6171/3  Techniques for Suspect/Counterfeit EEE Parts Detection by X -ray \nFluorescence Test Methods  \nAS6171/4  Techniques for Suspect/Counterfeit EEE Parts Detection by \nDelid/Decapsulation Physical Analysis Test Methods  \nAS6171/5  Techniques for Suspect/Counterfeit EEE Parts Detection by \nRadiological Test Methods  \nAS6171/6  Techniques for Suspect/Counterfeit EEE Parts Detection by Acoustic \nMicroscopy (AM) Test Methods  \nAS6171/7  Techniques for Suspect/Counterfeit EEE Parts Detection by Electrical \nTest Methods  \nAS6171/8  Techniques for Suspect/Counterfeit EEE Parts Detection by Raman \nSpectroscopy Test Methods  \nAS6171/9  Techniques for Suspect/Counterfeit EEE Parts Detection by Fourier \nTransform Infrared Spectroscopy (FTIR) Test Methods  \nAS6171/10  Techniques for Suspect/Counterfeit EEE Parts Detection by \nThermogravimetric Analysis (TGA) Test Methods  \nAS6171/11  Techniques for Suspect/Counterfeit EEE Parts Detection by Design \nRecovery Test Methods  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  19 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nFor the purposes of LoA1 , the program should follow  the lot sampling guidelines found \nin the AS6171 document and exercise the tests defined by slash sheets 1 -10. The tests \ndefined in slash sheet 11 are not necessary for LoA1 but will become more important at \nhigher assurance levels.  The waiving of slash sh eet 11 testing  for LoA1  does not \nsupersede any other D oD standard that require s it.  \nSelect sampl e parts  \nThe selection of parts to be physically sampled should  be handled in such a way that a \ncompromised insider could not knowingly select only genuine parts to be sampled. \nPossible sampling  options include the following:  \nOption  1: An independent part y handle s part selecti on before shipping.  They should \nphysically verify that the parts selected make it all the way to the physical inspection \nprocesses and verify upon receipt that the right parts were received.  \nOption 2 : Use a non -human random selection automated process for sampling.  \nOption 3:  Physical verification and sampling work should be conducted by personnel \nholding clearances of at least the Secret level and carried out in facilities cleared to at \nleast the Secret level.  \nTD 3:  Adversary compromises application design cycle  \nIn this threat , a compromised insider has access to the design process and data related \nto an FPGA application development effort . This insider can use their access to modify \ndesign code, design constraints, or FPGA configuration settings , or swap in a distinct \nconfiguration file that is authenticated and built with the same tools and keys being used \nby the design team. The actor  is in a particularly advantageous position because they \ncan modify the product during any phase of the design process. This same threat \nsurface may also be attacked via remote network intrusion. An attacker with network \naccess may also be able to modify impo rtant design data in a way that introduces a \nTrojan or other nefarious function.  \nAs TD 3 is comprised of numerous  scenarios , this threat is  broken into eight sub -threats \nor scenarios and addressed separately . Collectively, t hese scenarios describe the entire \nthreat at TD 3 and each of the mitigations for each scenario should be implemented.  \nThe specific scenarios are as follows:  \n Introduction of a compromised design into the application,  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  20 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \n Modification of test benches  or plans to reduce coverage or hide Trojan code,  \n Introduction of a Trojan into the application design during development,  \n Introduction of compromised tooling or software into the environment,  \n Network intrusion,  \n Compromised employee,  \n Modification of revision control that hides code or test bench modification \n(associated mitigations are captured in the in all cases section below), and  \n Introduction of modified configuration data after generation (associated \nmitigations are captured in the  in all cases sectio n below).  \nTD 3 mitigations  \nThe best practices presented here  do not describe  a standalone engineering FPGA \ndesign flow , but rather  the assurance practices that  should be integrated into the \nexisting design procedures. These assurance  practices incorporate industry -accepted \ndesign best practices with emphasis on documented and approved design, review , and \ntest procedures.  \nTD 3 mitigations focus strongly on the  insider threat.  At LoA1, access is defined as \nsingular  with uncleared people . As such, this threat  can be mitigated by following the \nguidance in the  use cleared personnel in a cleared environment  section . In the event \nthat is not possible, the mitigations associated with each scenario should be \nincorporated.   \nUse c leared personnel in a cleared environment   \nUse cleared personnel with at least a Secret -level clearance in an environment suitable \nfor a network cleared at the Secret level.  \nWhen the program select s not to use cleared personnel and a cleared environment, \nthey should implement all TD 3 sub-threat mitigations, as there are multiple threats to \nbe mitigated. T he following mitigations are applicable to all the  sub-threats  identified in \nthis section :  \n Track critical data in a revision control  system . \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  21 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \n Enforce auditability  of the requirements, architecture, design , code, test s, bugs, \nand fixes. At a minimum, audit data includes what decisions were made, by \nwhom, for what reason, and on what date.  \n Use a revision control/version management  system  that meets the requirements \ndescribed later in this section.   \nTD 3 mitigation d escriptions  \nTrack critical data in a revision control  system  \nThe program should identify and document all data that is considered critical.  Each \ncritical data item should be stored and tracked in the revisi on control system.  Minimally, \nthe following documents, data artifacts , and tool configurations should be managed in \nthe revision control system : \n Third -party IP (3PIP)  \n Utilized libraries  \n Development files, code, software used for development, synthesis scripts, and \ntools  \n Test benches, test plans, test procedures, and test reports  \n Tool configuration settings  \n Design documents  \nEnforce auditability  \nEnforce auditability of the requirements, architecture, design , code, test s, bugs, and \nfixes. At a minimum, a udit data includes what decisions were made, by whom, for what \nreason, and on what date.  \nUse a r evision control/version management  system  \nRevision control/version management systems should meet the following requirements:  \n Allow only authorized system admi nistrators to make changes to the underlying \nrevision control tool and underlying server.  \n Implement a backup system that mimics the primary system and is maintained by \na separate administrator. Separate system administrators should manage each \nsystem.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  22 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \n Enforce administrative restrictions;  restrict privileged access to only an \nauthorized set  of administrators;  limit what users can do to the database; ensure \nall users are verified; encrypt database information both in transit and at rest; \nenforce secure pass words; introduce role -based access control and privileges; \nand remove unused accounts.  \n Remove any components or functions that are not needed; for example, remove \nall sample files and default passwords.  \n Ensure the system provides a complete and immutable long-term change history \nof every file. The system should  log every change made by individuals. This \nincludes creation and deletion of files and content edits. The history should  \ninclude the person who made the change, what was changed, the date, and \nwritten notes on the purpose of each change.  \n Ensure the system stores a reliable copy of assets that are currently in \nproduction.  \n Ensure the system stores reliable copies of previous  production versions of \nassets, allowing for the complete retrieval of those versions.  \n Enforce password best practices (password rotation, length, etc.). In lieu of a \npassword, two -factor authentication can be used.  \nTD 3.1: Mitigating  the introduction of a compromised design  into the \napplication  \nIn this scenario , the adversary is able to insert a Trojan into the design after the design \nhas been verified, but before the design is loaded for final deployment. Strict controls on \nthe revision control system wi ll help prevent the adversary from making unmonitored \nchanges.   \nTo accomplish this task , the adversary would have to compromise t he revision \nmanagement system.  That compromise could allow the adversary to switch the verified \nconfiguration files, settings, hash or other pertinent information.  To protect against this , \nthe program should store and isolat e the verified  FPGA application configuration files, \nsettings , and associated hash.  Before the design is loaded for final deployment , the \nprogram should  verify  the hash to know the verified version is the same as what they \nwant to deploy . For extra assurance , the program has all the necessary data to \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  23 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nreproduce the build which can be used to verify the stored versions against the \nreproduced version.  \nMitigations  \n Physically isolate and store the application design  until it is delivered.  \n Perform a reproducible build  of the application .  \nDescriptions  \nIsolat e and store the application design  \nTo protect the application design after verification but before deployment, the final \nconfiguration file and hash should be physically isolated and stored until it is delivered \nfor provisioning . Ensure the file can only be accessed via authentication of tw o distinct \nparties. No single individual should be able to access the files. The limited set of people \nwith access should have to follow access control procedures such that access is  \ncontrolled, monitored,  logged , and auditable.   \nPerform a reproducible bui ld \nA reproducible build process is a methodology to verify the integrity of the FPGA \nsynthesis and build software. Reproducible build performs the synthesis process taking \nin human readable HDL, and other human readable inputs, and consistently generates \nthe same final configuration file (bitstream). At LoA1 reproducible builds should be \nperformed using independently acquired software and installed independently on two \ndistinct computers. It is expected that this process will, in most cases, require the use  of \nthe same version of the electronic design automation (EDA) tools, and, in some cases, \nthe same operating system version. This process will highlight the possession of \nmodified software when there is a mismatch. Contact the FPGA software vendors for \nmore information on how to perform reproducible builds.  \nTD 3.2: Mitigating the m odification of test benches  or plans to reduce \ncoverage or hide Trojan code  \nIn this threat, the adversary makes changes to the test bench to hide malicious code, \nreduce coverage , or reduce functionality.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  24 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nMitigations  \n Create and execute a documented test plan  that identifies the various test \nreviews that will take place, analysis to be performed, type of testing to be \nperformed, and the methods used to accomplish the test.  \n Validate  and verify test processes  which include design/test team separation, \npeer reviews, and use of automated tools where applicable . \n Ensure th e test environment is maintained via configuration management  as a \ncritical system.  \n Use a revision  control /version management  system . \nDescriptions  \nExecute a documented test plan  \nThe program should consider assurance when creating and maintain ing the test plan.  \nThe test plan and processes should at least:  \n Provide a  mechanism to verify all requirements  captured in the FPGA \nspecification .  \n Explicitly list code coverage metrics, the type of testing that will be performed, \nand acceptable testing guidelines.  Code coverage should state how much code \nis checked by the test bench, providing information about d ead code in the \ndesign and holes in the test suites. Document the  decision to use/not use other \ntypes of testing , such as directed test, constrained random stimulus, and \nassertion.  \n Ensure code coverage includes statement coverage, branch coverage, Finite \nState Machine (FSM), condition, expression, and toggle coverage. Document \nany code that will not be covered and why. Ensure untested code is documented \nand reviewed through the review process. Use functional test s to verify the FPGA \ndoes what it is suppose d to do. Any deviations should  be documented and \napproved.  \n Specify the verification environment which describes the tools, the software, and \nthe equipment needed to perform the reviews, analysis, and tests. Each of these \nitems should be maintained under re vision control.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  25 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \n Document and analyze unexpected  behavior and final implementation \nconclusions.  \n Ensure all test discrepancies, bugs, etc. , are resolved via a change process.  \nValidate and verify  test processes  \nThe program should take care to ensure test pr ocesses consider assurance needs.  This \nincludes design/test team separation, peer reviews, and use of automated tools where \napplicable . All test discrepancies, bugs, etc. , should be res olved via a change process  \nutilizing a change management system.  The es tablished processes should be \ndocumented, enforced , and audited.  \nEnsure the t est environment is maintained via configuration management  \nThe test environment should be treated as a critical system and maintain ed similarly to \nthe production environment.  \nUse a revision  control /version management  system  \nRevision control/version management systems should meet the following requirements:  \n Allow only authorized system administrators to make changes to the underlying \nrevision control tool and underlying server.  \n Implement a backup system that mimics the primary system and is maintained by \na separate administrator. Separate system administrators should manage each \nsystem.  \n Enforce administrative restrictions;  restrict privileged access to only an \nauthorized set  of ad ministrators;  limit what users can do to the database; ensure \nall users are verified; encrypt database information both in transit and at rest; \nenforce secure passwords; introduce role -based access control and privileges; \nand remove unused accounts.  \n Remove  any components or functions that are not needed; for example, remove \nall sample files and default passwords.  \n Ensure the system provides a complete and immutable long -term change history \nof every file. The system should  log every change made by individual s. This \nincludes creation and deletion of files and content edits. The history should  \ninclude the person who made the change, what was changed, the date, and \nwritten notes on the purpose of each change.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  26 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \n Ensure the system stores a reliable copy of assets t hat are currently in \nproduction.  \n Ensure the system stores reliable copies of previous production versions of \nassets, allowing for the complete retrieval of those versions.  \n Enforce password best practices (password rotation, length, etc.). In lieu of a \npassword, two -factor authentication can be used.  \nTD 3.3: Mitigating the introduction of a Trojan in to the application \ndesign during develop ment  \nIn this scenario, malicious functionality is introduced into the application design during \nthe development phase.  \nMitigations  \n Ensure all design artifacts have a direct bi-directional link to approved \nrequirements . Tracing to design decisions is permitted in support of derived \nrequirements.  \n Enforce peer review  best practices.  \n Create and execute a documented test plan .  \n Implement, v alidate , and verify test processes  which include design/test team \nseparation, peer reviews, and use of automated tools where applicable . \n Select  a formal proof process  that can validate the equivalency of the  hardware \ndescriptor language  (HDL) and final configuration file. For more information on \nproof tools, contact JFAC.  \nDescriptions  \nEnsure all design artifacts have a direct bi -directional link to approved \nrequirements  \nAll requirements should be documented and traced.  Functionality that i s not associated \nwith a requirement should not be allowed.   \nEnforce peer review  \nEstablish and enforce peer review practices with the following:   \n The author and the reviewer should be different people.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  27 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \n Ensure the design process has time allocated for code reviews.  \n Code review should be done in parallel with development, reviewing small \nchunks at a time.  \n Anyone reviewing the code should already be familiar with the agreed upon \narchitecture.  \n All black box portions of the design should be identified, justif ied, and approved.  \n All scripts that produce design artifacts (HDL, netlist, etc.) should be reviewed \nand approved. Ensure there are no unexpected paths, filenames , or suppressed \noutputs.  \n Ensure the code reviews, at a minimum, verify:  \n The code does what it is intended to do.  \n The code can be traced to requirements.  \n The code is not needlessly complex.  \n Coding standards are being utilized.  \n No extraneous code exists , the developer is not implementing unapproved \nitems that may have future utility.  \n The code has appropriate unit tests.  \n Tests are well designed.  \n The code uses clear names for everything.  \n Comments are clear and useful , and mostly explain why instead of \nwhat.  \nExecute a documented test plan  \nThe program should consider assurance when creating and maintain ing the test plan.  \nThe test plan and processes should at least:  \n Provide a  mechanism to verify all requirements  captured in the FPGA \nspecification .  \n Explicitly list code coverage metrics, the type of testing that will be performed, \nand acceptable testing guidelines.  Code coverage should state how much code \nis checked by the test bench, providing information about dead code in the \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  28 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \ndesign and holes in the test suites. Document the  decision to use/not use other \ntypes of testing , such as directed test, constrained random stimulus, and \nassertion.  \n Ensure code coverage includes statement coverage, branch coverage, Finite \nState Machine (FSM), condition, expressio n, and toggle coverage. Document \nany code that will not be covered and why. Ensure untested code is documented \nand reviewed through the review process. Use functional test s to verify the FPGA \ndoes what it is supposed to do. Any deviations should  be documen ted and \napproved.  \n Specify the verification environment which describes the tools, the software, and \nthe equipment needed to perform the reviews, analysis, and tests. Each of these \nitems should be maintained under revision control.  \n Document and analyze une xpected  behavior and final implementation \nconclusions.  \n Ensure all test discrepancies, bugs, etc. , are resolved via a change process.  \n All modifications made to the application should undergo full testing.  Do not allow \nuntested modifications to go to releas e. \nImplement, validate , and verify test processes  \nThe program should take care to ensure test processes consider assurance needs.  This \nincludes design/test team separation, peer reviews, and use of automated tools where \napplicable.  All test discrepancies, bugs, etc. , should be resolved via a change process \nutilizing a change management system.  The established processes should be \ndocumented, enforced , and audited  \nSelect a f ormal proof process  \nUse logical equivalency checking to the greatest extent possible.  Equivalency checking \nis used to prove the tools did not modify the logic or configuration settings.  To do this , \nthe final bitstream  is compared to the originating application HDL  to demonstrate they \nare logically equivalent with no extraneous log ic in the final format. This approach \nconfirms Trojans were not inserted during the implementation steps.  This check also \nconfirms configuration settings ar e maintained and not altered. Configuration settings \nare those parameters included in the configurat ion file that affect the behavior of the \nFPGA device itself but are not a part of the program application. Examples would \ninclude tamper settings, JTAG settings , and key storage.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  29 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nThere are technical challenges associated with performing Logic Equivalence C heck \n(LEC) on FPGA data . Contact JFAC for information on  emerging industry tools that can \nassist in identifying configuration data in the FPGA formats  or automate the creation of \nhints files.  \nTD 3.4: Mitigating the i ntroduction of compromised tooling  or \nsoftware into the e nvironment  \nIn this scenario, the adversary introduces compromised tooling or software into the \nenvironment. This can be accomplished by an insider or through network intrusion.  \nMitigations  \n Accept only digita lly signed software deliveries . \n Validate cryptographic hashes  against vendor  provided hash es. \n *Research vulnerabilities  affecting  tools/platforms using commercial and JFAC -\nprovided resources. If vulnerabilities are found, use an alternate or newer version \nthat does not have the vulnerabi lity. Alternatively, perform a risk assessment and \ncoordinate findings with JFAC.  \n Use a  revision control/version management  system  that includes document/data \ncontrol, document/data release, backups and archives, refresh of backup media, \nretention of tools  and software, test equipment , and test environment.  \n Utilize a  reproducible build  process  to generate any deployable configuration \nfiles. The program should independently validate this reproducibility for each \ndeployable version on a distinct computer syst em, with an independently \nacquired version of the same EDA tools.  \n Select  a formal proof process  that can validate the equivalency of the  hardware \ndescriptor language  (HDL) and final configuration file. For more information on \nproof tools, contact JFAC.  \nDescriptions  \nAccept only digitally signed software deliveries  \nVerify that the received software has a valid digital signature  on all executables and \nscripts. The signature is used to confirm the software s author and guarantee that the \ncode has not been altered or corrupted since it was signed.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  30 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nValidat e cryptographic hashes  \nAll parts of the software delivery should be authenticated by comparing the \ncryptographic hash of all received software against the hash signed by the vendor. This \nincludes  install macros and other support functions. Only accept certificates validated \nby reputable third parties. Only accept publicly released software and document the \nsource of the hash s ignature and the hash itself.  \nResearch vulnerabilities  \nSoftware and tooling vulnerabilities can be exploited for nefarious purposes. The \nprogram should actively monitor for vulnerabilities and perform risk assessment for any \nsoftware or tools selected.  Platform  and tool vulnerabilities can be found in databases \nsuch as:  \n Common Vulnerabilities and Exposures (CVE)  https://cve.mitre.org   \n Common Weakness Enumeration (CW E)  https://cwe.mitre.org   \n NIST National Vulnerabilities Database (NVD)  https://nvd.nist.gov   \n Government Industry Data Exchange Program (GIDEP)  \nhttps://www.gidep.org/products/products.htm   \n DISA  Security Technical Implementation Guides (STIGs)   \nhttps://public.cyber.mil/stigs/  \n Searches for vendor advisories , errata bulletins , publications , and academic \npapers detailing vulnerabilities in the device in question.  \nIf vulnerabilities are found in the software or tools , choose one  of the following options:  \nOption 1 : Select a different FPGA platform device or software that does not have \npublished  vulnerabilities and that meet s the program requirements . \nOption 2:  Use standard formal processes and procedures to work with the vendor to \nresolve the vulnerability. Once a fix is identified, only accept formal releases, do not \naccept custom beta fixes, c ustom patches, etc. for incorporation . \nOption 3 : The program can internally determine the vulnerability poses no significant \nrisk to their product . JFAC is available to provide assistance in assess ing the risk that \nthe vulnerability poses to the system and acquire recommended mitigations for a \nparticular vulnerability.   \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  31 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nNote: If a vulnerability is identified, it is recommended to report it to Government \nIndustry Data Exchange Program (GIDEP) and to contact the vendor so they may \ncorrect it . \nUse a r evision c ontrol/version management  system  \nThese systems should fulfill the following requirements:  \n Allow only authorized system administrators to make changes to the underlying \nrevision control tool and underlying server.  \n Use a backup system that syncs to the prima ry and is maintained by a separate \nadministrator. Each system should be managed by separate system \nadministrators.  \n Enforce administrative restrictions;  restrict privileged access to only an \nauthorized set  of administrators;  limit what users can do to the database; ensure \nall users are verified; encrypt database information both in transit and at rest; \nenforce secure passwords; introduce role -based access control and privileges; \nand remove unused accounts.  \n Remove any native s ystem  components or functions that are not necessary (for \nexample, remove all sample files and default passwords).  \n Ensure the system provides a complete and immutable , long-term change history \nof every file. The system should  log every change made by indiv iduals. This \nincludes changes such as creating and deleting files and editing content.  The \nhistory should  identify the person who made the change, what was changed, the \ndate of the change, and the purpose of the change.  \n Ensure the system stores a reliable  copy of assets that are currently in \nproduction.  \n Ensure the system stores reliable copies of previous production versions of \nassets, allowing for the complete retrieval of those versions.  \n Ensure password best practices (password rotation, length, etc.) ar e enforced. In \nlieu of a password, two -factor authentication can be utilized.  \nUtilize a  reproducible build  process  \nA reproducible build process is a methodology to verify the integrity of the FPGA \nsynthesis and build software. Reproducible build performs t he synthesis process taking \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  32 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nin human readable HDL, and other human readable inputs, and consistently generates \nthe same final configuration file (bitstream) . At LoA1  reproducible builds should be \nperformed using in dependently acquired software and installe d independently on two \ndistinct computers. It is expected that this process will, in most cases, require the use of \nthe same version of the EDA tools, and in some cases the same operating system \nversion. This process will highlight the possession of modifi ed software where there is a \nmismatch. Contact the FPGA software vendors for more information on how to perform \nreproducible builds.  \nSelect a f ormal proof process  \nUse logical equivalency checking to the greatest extent possible. Equivalency checking \nis used to prove the tools did not modify the logic or configuration settings. To do this, \nthe final placed netlist  is compared to the originating application HDL  to dem onstrate \nthey are logically equivalent with no extraneous logic in the final format. In cases where \ncommercial tools exist to extend the comparison to the bitstream  itself, that comparison  \nis recommended. This approach confirms Trojans were not inserted during the \nimplementation steps.  This check also confirms configuration settings ar e maintained \nand not altered. Configuration settings are those parameters included in the \nconfiguration file that affect the behavior of the FPGA device itself , but are not a part of \nthe program application. Examples would include tamper settings, JTAG settings , and \nkey storage.  \nThere are technical challenges associated with performing Logic Equivalence Check \n(LEC) on FPGA data . Contact JFAC for information on  emerging indust ry tools that can \nassist in identifying configuration data in the FPGA formats  or automate the creation of \nhints files.  \nTD 3.5: Mitigating  intrusion into the internal network  \nIn this scenario, an adversary gains access to the internal network. With this ac cess, the \nadversary can employ multiple methods in which they can act on their nefarious \nintention s, such as modif ying tools, swap files, etc.  \nMitigations  \n Use a revision control/version management  system  that includes document/data \ncontrol, document/data r elease, backups and archives, refresh of backup media, \nretention of tools and software, test equipment , and test environment .  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  33 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \n Assign privileges and accesses based on role s. \n Based on job requirement s control and monitor access , including physical \nrestrictions.  \n Periodically research  vulnerabilities  using commercial and JFAC -provided \ninformation. If vulnerabilities are found, use an alternate or newer version that \ndoes not have the vulnerability. Alternatively, perform a risk as sessment and \ncoordinate findings with JFAC.  \n For DoD system owners, purchase from DoD authorized vendor s and \ndistributors . The D oD program acquisition grou p can provide this information.  \n Use protected  computing environments  to protect from remote attack .  \nDescriptions  \nUse a r evision control/version management  system  \nThese systems should fulfill the following requirements:  \n Allow only authorized system administrators to make changes to the underlying \nrevision control tool and underlying server.  \n Use a backup sy stem that syncs to the primary and is maintained by a separate \nadministrator. Each system should be managed by separate system \nadministrators.  \n Enforce administrative restrictions;  restrict privileged access to only an \nauthorized set  of administrators;  limit what users can do to the database; ensure \nall users are verified; encrypt database information both in transit and at rest; \nenforce secure passwords; introduce role -based access control and privileges; \nand remove unused accounts.  \n Remove any components or  functions that are not necessary (for example, \nremove all sample files and default passwords).  \n Ensure the system provides a complete and immutable , long-term change history \nof every file. The system should  log every change made by individuals. This \ninclud es changes such as creating and deleting files and editing content.  The \nhistory should  identify the person who made the change, what was changed, the \ndate of the change, and the purpose of the change.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  34 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \n Ensure the system stores a reliable copy of assets that are currently in \nproduction.  \n Ensure the system stores reliable copies of previous production versions of \nassets, allowing for the complete retrieval of those versions.  \n Ensure password best practices (password rotation, length, etc.) are enforced. In \nlieu of a password, two -factor authentication can be utilized.  \nAssign privileges and accesses based on roles   \nEmployees should be assigned a specified role with associated accesses and privileges \nbased on the role. At a minimum, these roles should include design, test, network \nadministration , and system administration. Roles should also be defined and \ndocumented with no overlap. Users should not have multiple roles.  \nNote : In many real -world flows, designers and testers will require elevated privileges . \nSome  of these elevated privileges may be shared with system administrators. Some \nmay have names (\"local admin,\" \"root,\" etc.) that imply system administration. For \nexample, a member of the de sign team working on a software/ hardware interface may \nrequire local administrati ve privileges to install and debug their work. A member of the \ntest team for an FPGA -based device connected to an IP network might require the \nability to configure multiple network devices in the test environment, as well as to \nconnect a comput er in promiscuous mode to that same test environment. Those \naccesses represent a part of the design or test role. However, these should be based \non the needs of the design or test process.  \nElevated privileges on computers should be granted only as needed, and kept local to \nspecific computers. Elevated privileges should never include administrative access to \nrevision control servers, software installation, or other corporate infrastructure.  \nElevated privileges on networks should be limited to distinct test n etworks, properly \nisolated from the design environment and the corporate network.  \nControl and monitor access  \nEmployees should only have physical access to areas, equipment, data, and \ninformation necessary to meet the requirements of their assigned job. En try/access to \nappropriate areas should be recorded, monitored, and logged for auditability.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  35 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nResearch vulnerabilities  \nSoftware and tooling vulnerabilities can be exploited for nefarious purposes. The \nprogram should actively monitor for vulnerabilities and perform risk assessment for any \nsoftware or tools selected. Platforms and tool vulnerabilities can be found in databases \nsuch as:  \n Common Vulnerabilities and Exposures (CVE)  https://cve.mitre.org   \n Common Weakness Enum eration (CW E)  https://cwe.mitre.org   \n NIST National Vulnerabilities Database (NVD)  https://nvd.nist.gov   \n Government Industry Data Exchange Program (GIDEP)  \nhttps://www.gidep.org/products/products.htm   \n DISA  Security Technical Implementation Guides (STIGs)   \nhttps://public.cyber.mil/stigs/  \n Searches for vend or advisories , errata bulletins , publications, and academic \npapers detailing vulnerabilities in the device in question.  \nPurchase from D oD authorized vendors and distributors  \nFor DoD system owners, u tilize DoD  authorized vendors and distributors for all \npurchases.  Authorized vendors can be identified through the acquisition organization.  \nUse protected  computing environment s \nPrograms should select one of the protected computing platform options below, to \nprotect from remote attack :  \nOption 1: A computer and network classified at the Defense Security Cooperation \nAgency  (DSCA ) Secret level or above.  \nOption 2:  A computer and network certified for use in a Trust Category 1 facility as \ndefined by Defense Microelectronics Activity (DMEA ).  \nOption 3 : A network -isolated computer enclave with limited and controlled access. This \nis a computer with the vendor software installed by a network administrator. This \nadministrator should not be a designer working on the application design.   \nOption 4:  An infrastr ucture minimally compliant with NIST SP 800 -171 and NIST SP  \n800-172, preferably compliant with Cybersecurity Maturity Model Certification (CMMC).   \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  36 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nTD 3.6: Mitigating risk from a c ompromised hire or employee  \nThis scenario involves the compromise of an emplo yee with access to the design, tools, \nor network being used for design or test.  \nMitigations  \n Enforce auditability  of the requirements, architecture, design , code, test s, bugs, \nand fixes. At a minimum, audit data includes what decisions were made, by \nwhom, f or what reason, and on what date.  \n Track critical data in revision control . \n Adopt  a structured  application design process  that is  document ed, adhere d to, \nand organizationally approved . \n Identify, document , and review critical activities . These items should be reviewed \nby a cleared individual that is different than the o riginal designer.  \n Enforce reviewer criteria . \nDescriptions  \nEnforce auditability  \nThe program should maintain audit logs on all design data to include, requirements, \narchitecture, design , code, t ests, bugs and fixes. The audit data minimally should \ndocument who requested the change with date timestamp, what decision was made \nregarding the change, who made the decision with date timestamp, why the change \nwas requested, and who made the change with date timestamp . \nTrack critical data in revision control  \nThe program should identify and document all data that is considered critical.  Each \ncritical data item should be stored and tracked in the revision control system.  Minimally, \nthe following documents, data artifacts and tool configurations should be managed in \nthe revision control system : \n Third -party IP (3PIP)  \n Utilized libraries  \n Development files, code, software used for development, synthesis scripts, and \ntools  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  37 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \n Test benches, test plans, test procedures, and test reports  \n Tool configuration settings  \n Design documents  \nAdopt a structured application design process   \nThe design process should contain clear entry and exit criteria. Entry and exit criteria  \nshould  incorporate peer reviews and technical reviews with management approval to \nexit a phase.  \nReview critical activities  \nEnsure all critical activities are identified, documented, and the entire design is peer \nreviewed . Reviewers should assess all critical act ivities . Specific considerations include:  \n Design source files in conjunction with behavioral simulations   \n Design synthesis in conjunction with functional verification   \n Design i mplementation in conjunction with static timing analysis   \n Bitstream generation w ith reproducible build results   \n Programming in conjunction with in -circuit verification  \nEnsure that the review teams do not include the original designers .  \nEnforce reviewer criteria  \nEnforce a formal review process using one of the following options : \nOptio n 1: All critical activities are identified and documented. Independent third -party \nreviewers should assess all critical activities.  \nOption 2 : Ensure reviews are performed by independent teams comprised of \nindividuals who hold a Secret -level clearance.   \nOption 3 : Perform all of the reviews  with cleared personnel in an environment certified \nto handle classified information at the Secret level or higher by DSCA. This would also \ninclude design  reviews . \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  38 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nTD 4: Adversary compromises system assembly , keying , or \nprovisioning  \nIn this threat, an adversary has carried out an attack on the system during PCB \nassembly, key injection , or flash provisioning. This attack could include the assembly \nhouse acquiring counterfeit parts on behalf of the end customer, swapping out  authentic  \nFPGA parts for counterfeit ones, stealing  or compromising configuration data, or \nstealing or modifying keys. Multiple parties can be involved during th e system assembly \nphase. The following areas of the supply chain are included in this threat:  \n Shipping devices to the PCB assembly facility.  \n Transmitting keys, configuration data , and FPGA part numbers to the assembly \nfacility . \n Injecting keys into the FPGA devices . \n Provisioning the configuration storage devices . \n Attaching the FPGA devices to the PCB. \n Testing the PCBs. \n Shipping the PCBs to the next manufacturing stage.  \nAs such, there are multiple scenarios in which mitigations need to be applied. These \nscenarios are as follows:  \n Scenario #1 : The program performs product assembly and keying/provisio ning \nin a facility/process certified at a classified Secret level or higher.  \n Scenario #2 : The program performs one or more of the seven steps above \noutside of a classified facility.   \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  39 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \n \nFigure 1: Secure and non -secure facility scenarios  \nTD 4 mitigations  \nRegardless of where the work is performed, th e program should  implement the following \nlist of mitigations in the assembly, keying , and provisioning process:  \n For DoD system owners, purchase from DoD and vendor authorized distribut ors. \nThe D oD program acquisition grou p can provide this information.  \n Follow storage and shipping  guidance  for classified Secret or Trust Category I \nmaterials when storing and transferring FPGA devices between locations .  \n Provide keys and configuration data  to the provisioning house in digitally signed \npackages and with hashes . The assembly house should utilize these to verify the \nintegrity of the contents.  \n Prior to provisioning, clear  memory devices  that store configuration data . \n Provision private keys  into the FPGA devices in a DSCA Classified Secret or \nTrust Category I certified facility after the assembly process.  \n *Protect the FPGA from attack  during assembly and provisioning . \n *Authenticate the FPGA device  after being ou t of the control of the program . \n\n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  40 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nTD 4 mitigation descriptions  \nPurchase from D oD and vendor  authorized distributors  \nFor DoD system owners, utilize distributors that are authorized by both DoD and the \nvendor for all purchases.  Authorized vendors can be located through the acquisition \norganization .  \nFollow storage and s hipping  guidance  \nThe program should document, maintain, and enforce both device storage  and shipping \nprocedures. Minimally, the p rocedures  should enforce the verification and acceptance \ntestin g of all devices upon receipt . In addition to device verification, storage  and \nshipping processes should focus on preventing and detecting  adversary access to the \nparts. Access controls  should be designed to thwart those seeking to alter or rep lace \nthe devices while in storage or ship ping.   \nStorage  protections  should include the following characteristics:   \n Production  devices should be stored and maintained in a restricted area separate \nfrom non -production devices ( i.e., ones for design, test, etc.).  \n The restricted area should enforce access control s that limit  access to only those \npersonnel who require access to suppo rt direct job responsibilities.  This \nshould  exclude all members of the design team.   \n All accesses to the restricted area should be recorded and audited.   \n The FPGA devices should be audited on a regular cadence to ensure they have not \nbeen removed or modified.   \n Production devices should be continuously tracked to include arrival of the device by \nunique identifier, interaction anyone has with the device, and exit of the devic e from \ninventory.  \n The restricted area should have a clearly defined perimeter, but physical barriers are \nnot required.   \n Personnel within the area should be responsible for challenging all persons who may \nlack appropriate access. Accesses to the restricted  area should be audited to \ninclude data containing who entered/exited the area, with a timestamp, and reason \nfor entry.   \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  41 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \n When possible, the FPGA devices should be stored within a tamper detection seal \nor tape to detect unauthorized access.   \nShipping protec tions  should include the following characteristics:   \n Following acceptance testing, non -configured and non -keyed devices should be \nusing a commercial carrier that has been approved by the appropriate Cognizant \nSecurity Agency  (CSA) to transport Secret shipments , although the material is not \nSecret. Commercial carriers may be used only within and between the 48 \ncontiguous States and the District of Columbia or wholly within Alaska, Hawaii, \nPuerto Rico, or a U.S. possession or trust t erritory.   \n Utilize protections against pilferage, theft, and compromise , including:  \n Hardened containers   \n Tamper detection seals and tapes.  The seals should be numbered,  and the \nnumbers indicated on all copies of the bill of lading (BL). When seals are \nused , the BL should  be annotated substantially as follows:  \nDO NOT BREAK SEALS EXCEPT IN CASE OF EMERGENCY OR UPON \nPRIOR AUTHORITY OF THE CONSIGNOR OR CONSIGNEE. IF FOUND \nBROKEN OR IF BROKEN FOR EMERGENCY REASONS, APPLY \nCARRIER'S SEALS AS SOON AS POSSIBLE AND IMMEDIATELY NOTIFY \nBOTH THE CONSIGNOR AND THE CONSIGNEE.   \n Closely track s hipment times and depot stops.  Packages that are lat e should be \nconsidered suspect.   \nProvide k eys and configuration data  \nProvide keys and configuration data to the provisioning house in digitally signed \npackages and with hashes. JFAC recommends that these data packages be encrypted \nusing the AES algorithm with a key of at least 256 -bit length. The assembly house \nshould utilize these to verify the integrity of the contents . \nClear  memory  devices  \nPrior to provisioning, zeroize  memory devices  that store configuration data . This \nprevent s an adversary from storing malicious configuration data in non -used areas of \nthe memory device . These memory devices could include a discrete PCB component \nlike a Flash or the on -chip FPGA non -volatile storage available on certain devices.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  42 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nProvision private keys  \nProvision private keys  into the FPGA devices in a DSCA Classified Secret or Trust \nCategory I certified facility after the assembly process.  \nProtect th e FPGA from attack  during assembly and provisioning  \nTo protect the FPGA from attack during assembly and provisioning , the program can \nselect from one of t wo options : \nOption 1:  Assemble and provision the system in a DSCA Classified Secret or Trust \nCategory I certified facility. This can be a single secure facility or multiple secure \nfacilities.  The assembly work, as well as the keying and provisioning functions should be \nexecuted using equipment, processes , and personnel approved for Secret or Trust \nCategory  level efforts. This in and of itself is a mitigation against attacks focused on \nthese portions of the manufacturing flow . \nOption 2:  Assembl e and provision in an external unclassified facility.  Programs that \nhave chosen to perform some piece or all of the assembly, keying , and configuration file \nprovisioning in an external unclassified facility should perform the following mitigations:  \nThose performing this validation should : \n Verify  the PCB traces related to the FPGA device, the configuration memory \ndevices , and any other devices related to the authentication of the configuration \ndata. The program should rely on guidance from the JFAC PCB Executive Agent \nto perform this verification.  \n Verify  the authenticity of the configuration data loaded on the FPGA memory \ndevice following provisioning and assembly. The verification can be executed by \na bit comparison or a hash. This verification should  be performed by a team \nindependent of the assembly an d provisioning process. The verification should \ncover the entire contents of the memory device and not just the addresses \ncontaining the configuration data . It is recommended to program the entire \nmemory space to disallow unused memory for nefarious purpos es. \n Verify  that the proper post -assembly keys have been loaded into the FPGA key \nstorage elements. This verification should  be performed by a team independent \nof the assembly and provisioning process. Some FPGA devices allow a hash of \nthe keys to be read o ut for confirmation. Additionally, the program can create test \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  43 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nbitfiles to verify that the devices can properly utilize the keys and can reject \nactions utilizing the wrong keys.  \n Verify  the authenticity of the FPGA device to rule out the introduction of a \ncounterfeit part during assembly.  \nAuthenticate the FPGA device  \nWhen the FPGA has been out of positive control of the program it should be \nauthenticated.  The following guidance is intended to detect if the authentic verified \nFPGA devices were maliciously rep laced by counterfeit or modified parts . The program \nshould select one of the options below:  \nOption 1: Verify the device on the PCB is an authentic and authorized device by \nvalidating that each device has a unique cryptographic ID signed by the vendor. Each \ndevice should  contain a unique private asymmetric key for which no read function \nexists, and validati on should  involve the device signing a nonce. A National Institute of \nStandards and Technology (NIST) -approved asymmetric authentication algorithm  \nshould be used for this. The program should authenticate the FPGA devices utilizing \nthis ID when they have been out of the positive control of the program.  \nOption 2: Verify the device on the PCB is an authentic and authorized device by \nperforming physical counterfeit inspection w ith destructive sampling  as described under \nPhysical analysis  for TD 2.  This is primarily an SAE International AS6171 Test Methods \nStandard; General Requirements, Susp ect/Counterfeit, Electrical, Electronic and \nElectromechanical Parts -based evaluation, with requirements to obtain vendor \ninformation.  \nOption 3: Use a soft physical unclonable function (PUF). Verify the device on the PCB \nis an authentic and authorized devi ce by utilizing a soft PUF to create unique IDs. The \nsoft PUF is used to validate the integrity of the devices when they are outside of the \nprogram's control. The program should generate these IDs when FPGAs are in their \ncontrol by loading the soft PUF int o the FPGA fabric, use it to generate a unique ID for \nthe respective device , and then delete the PUF. Following assembly, the program \nshould repeat this process and ensure the ID matches, authenticating the device.  If the \nsoft PUF will be used to authentic ate the device when it is outside the program control, \nit is recommended that the following be done:  \n Prevent readout of the PUF output to the FPGA s external pins . \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  44 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \n Utilize the PUF to encrypt a nonce that can transmit outside the device.  \n Utilize  a public ke y based on the PUF value to decrypt the nonce and \nauthenticate the device.  \nThis approach can be used to support remote attestation when needed.  \nTD 5: Adversary compromises third-party soft IP  \nIn this threat,  an adversary compromises third -party soft IP intended for integration into \nthe configuration of the FPGA. The compromise can occur during the vendors \ndevelopment cycle, during its delivery , or while at rest at the programs design center. In \nall scenarios, the compromised IP contains a malicious fun ction that was inserted during \nits design and can be triggered through some input to the FPGA, or when a specific \nscenario occurs. In all cases , it is important to remember the purpose of the Trojan is \nunknown, but probable impacts include performance, pow er, or reliability. The \nmitigations to these attacks focus on verifying integrity of the delivery of the IP and \nreviews of its HDL code.  \nThere are cases in which previously developed and/or evaluated IP can be re -used. \nSee Appendix B IP reuse guidance  for additio nal information.   \nTD 5  mitigations  \n For DoD system owners, purchase from DoD authorized vendor s and \ndistributors . The D oD program acquisition grou p can provide this information.  \n Only a ccept IP that is unobfuscated  and distributed as functional code.  \n Validate t he cryptographic hash of the IP  against the hash signed by the vendor.  \n Check IP  into revision control  repository immediately upon receipt with the \nhashes used to authenticate the contents. Protection of the hash will allow for re -\nverification o f the IP at a later date.  \n Examine IP for malicious functions . \nTD 5 mitigation d escriptions  \nPurchase from D oD authorized vendors and distributors  \nFor DoD system owners, utilize DoD  authorized vendors and distributors for all \npurchases.  Authorized vendors ca n be identified through the acquisition organization.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  45 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nOnly accept  IP that is unobfuscated   \nOnly accept IP that is unobfuscated and distributed as source  code. The IP needs to be \nhuman readable and able to be reviewed.  \nValidate the c ryptographic hash  of the IP \nEnsure that the cryptographic  hash of the IP  is validated against the hash signed by the \nvendor.  All parts of the software delivery should be authenticated in this manner \nincluding install macros and other support functions. The program should onl y accept \ncertificates validated by reputable third parties. The program should be limited to \npublicly released software. The program should maintain documentation of the source \nof the hash and the actual software hash.  \nCheck IP into revision control  \nImmed iately upon receipt , the IP with its associated hash should be checked into \nversion control.  The hash of the IP should be verified at various stages to ensure there \nhave been no unauthorized modifications.  \nExamine IP for malicious functions  \nTo examine  the IP for malicious functions,  follow the guidance provided in  Third -Party \nIP Review Process for Level of Assurance 1 at https://www.nsa.gov/Press -Room/DoD -\nMicroelectronics -Guidance/ .  \nTD 6: Adversary swaps configuration file on target  \nIn this threat, an adversary obtains access to the system during or after assembly and \ncan compromise the FPGA devices operation via the con figuration data.  \nFor assurance purposes, these guidelines are not concerned with the exposure of the \nconfiguration data or the confidentiality of the public keys, as they do not compromise \nthe assurance  of the operation of the hardware . However, programs w ith security \nrequirements may need to protect this information and can choose to implement \nadditional protections.  \nTechnological mitigations exist publicly for this threat , such as configuration data \nauthentication. Mitigations should  involve authenticatin g the configuration file for both \nintegrity and authorization . JFAC encourages programs to use device families that \nsupport configuration data authentication . For legacy devices that do not have \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  46 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nauthentication functions, these guidelines will lean on exter nal authentication functions \nor the use of symmetrically encrypted data files . \nPrograms are discouraged from using devices that do not support configuration data \nauthentication . In this scenario, authentication practices apply to all configuration file \nloads, including local  loads , remote updates, multi -boot scenarios , configuration via \nsoftware , and configuration via protocol where the configuration file is loaded into the \nFPGA. For devices that store the data internally in non-volatile memory (NVM), this \nrequirement only applies to the initial loading.  \nAs of this publication ( December 2022 ), all the major U.S. FPGA vendors provide built -\nin functionality to authenticate configuration files either at load into an internal memory \nor at conf iguration  for at least one device family . The specifics of this authentication vary \nextremely . The exact details of key management and storage vary from device to device  \ntoo. Some offer facilities to store many authentication keys, some use fuses, others u se \nindependently powered random access memory (RAM). Further, there are public \ntechniques to subvert the authentication , which have complex implications for the \nsecurity of built -in authentication1. \nThe result is that the exact security of each method is n ot apparent without a detailed \nevaluation. This report communicates  the specific mechanisms that meet JFAC \nexpectations, as well as caveats for their use.  As a rule, NSS prog rams must use  the \nlatest CNSS guidance. Non -NSS program s should  use the latest  NIST -approved \nasymmetric cryptographic algorithms at LoA1 . \nTo achieve LoA1 , the source of  all boot/configuration images should  be authenticated \nand the integrity  of the data should be verified . That is, the device should  validate that \nthe file comes from a n authorized provider and that the data has not been modified prior \nto loading. For LoA1 , the recommended method for authenticating the data source is to \nuse an asymmetric algorithm recommended by NIST.  All of the FPGA commercial \nvendors have some device f amilies with authentication schemes built in. Asymmetric \nalgorithms are preferred because they do not require the protection of a secret key  in \nthe device , as implemented by the various vendors . For data integrity, a hashing \nalgorithm such as secure hashin g algorithm  (SHA) is recommended . \n                                                \n1 The Unpatchable Silicon: A Full Break of the Bitstream Encryption of Xilinx 7 -Series FPGAs. Usenix Security 20. Maik Ender, Amir Moradi, Christof Paar.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  47 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nMany of the existing FPGA devices provide these functions for the user. However, less \nmodern device families rely on symmetric encryption and the protection of a  secret key \nby the device to provide source and data  authenti cation. In general, LoA1 can be met \nwith the following  configuration authentication mechanisms , in order of \nrecommendation : \n RSA/ECDSA with SHA -2 or SHA -3 \n AES-GCM with SHA -2 or SHA -3 \n Limited implementations of AES with HMAC with SHA -2 or SHA -3 \nTD 6  mitigations  \n Incorporate cryptographic authentication  of all loaded configuration data as part \nof the system containing the FPGA.  \n Design the system to authenticate configuration data each time the data is \nloaded  into the FPGA device.  \n Configure all production devices in a way to prevent direct read back  of the \nprivate keys through electrical means.  \n Use a CNSS/ NIST -approved algorithm and key length , as described in the latest \napproved version of FIPS 186 , Digital Signature Standard , or FIPS 198 , The \nKeyed -Hash Message Authentication Code (HMAC) . \n Disable operation or use of test access pins  in fielded products.  \n When the program selects mechanisms that allow application modifications , \nensure authentication is  enabled for application modifications  following the \nrequired  NIST  standards.  \n Use a FIPS 140 -2 compliant, Level 2 H ardware Security Module (HSM)  that is \nprogram -controlled to g enerate and store all authentication keys . \nTD 6 m itigation descriptions  \nIncorporate c ryptographic authent ication  \nThe program should  enforce cryptographic authentication.  In addition, the program \nshould  maintain documentation including the authentication methodology, its \narchitecture , and compliance with appropriate NIST standards.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  48 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nAuthenticate configuration data each time the data is loaded  \nDesign the system to authenticate configuration data each time the data is loaded  into \nthe FPGA device.  \nPrevent direct read back  \nConfigure all production devices in a way that prevents direct read back  of the private \nkeys through electrical means.  \nUse a CNSS/ NIST -approved algorithm and key length  \nNSS programs must use the latest approved  CNSS  algorithm and key length.  Non-NSS \nprograms should use  a NIST  approved algorithm and key length, as described in the \nlatest approved version of FIPS 186, Digital Signature Standard, or FIPS 198, The \nKeyed -Hash Message Authentication Code (HMAC ). \nDisable operation or use of test access pins  \nAll modern FPGA family devices have hardware test interfaces to support fabrication \ntesting of the  device and testing of the user product.  These interfaces usually include \nJoint Test Action Group ( JTAG ) pins and dedicated test pins.  \nJFAC recommends disabling  operation or use of these test access pins in fielded \nproducts. It is a common practice to disa ble these access points prior to fielding the \ndevice. In the case of JTAG, JFAC recommends disabling JTAG in full or limiting it to \nbound ary scan for board testing. No access to the internals of the device should be left \nactive. The program should verify t hat no unauthorized JTAG instructions remain active.  \nJFAC recommends  disabling this in non -volatile fuses when available.  \nEnsure authentication is enabled for application modifications  \nMany FPGA platforms contain mechanisms that allow the application to c hange itself. \nSome allow for true in -flight reprogramming, where some portion of the FPGA continues \nnormal operation while another portion changes its behavior. Others allow for \nreprogramming via external storage.  Verify that the built -in application chang e technique \napplies authentication to all the reconfiguration data.   \nThe names of these operations are system specific and include terms like dynamic \nreconfiguration,  partial reconfiguration,  in-application programming,  etc. In practice, \nthese mechanisms do not  provide the same degree of authentication that the primary \nprogramming mechanisms provide.  Under these best practices, an application designer \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  49 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nusing these techniques should  either validate that the technique they use applies the \nauthentication scheme described below to all the configuration data or perform \nauthentication of this data in the application itself.  \nAuthenticating reconfiguration data in the application  itself  \nIn this case, the program incorporates functions in the application to perform \nauthentication on configuration data when the FPGA device cannot. When utilizing this \noption, the program should pay attention to the following considerations.  \nSystem -on-a-chip FPGAs (SoC FPGAs) incorporate central processing units  (CPU) as \na component of a reconfigurable platform. The JFAC FPGA Best Practices do not seek \nto provide software assurance to the application running in the CPUs of a SoC FPGA. \nHowever, the best practic es listed here will provide the same degree of assurance to \nthe initial user code (sometimes called a bootloader) executed by the CPU.  \nFrom there , it is possible for a designer to extend the same authenticity to the user code \nif their system requires it. In cases where the program uses an interface between the \nFPGA fabric and the SoC in order to have one function load the other, it is vital that no \npath exists from this interface to the input/output (I/O). It is up to the program to ensure \nthat only the ap plication has access to it.  \nIn some platforms, security settings can be programmed into both non -volatile storage \nin the device itself and as a setting in the configuration file loaded into the device. \nSetting s should always be programmed in the non -volat ile storage of the device. In \nthose cases where use of security settings within the configuration file is acceptable, it \nshould  be explicitly noted.  \nSome platforms provide support for remotely updating the boot or configuration data on \nthe FPGA device. Thi s update is sent via a network, stored local ly on  the FPGA device, \nand then loaded into the device by the application. Under these best practices, an \napplication designer using this update technique should  either validate that the \ntechnique in use applies the same authentication scheme described  or perform \nauthentication of this data in the application itself.  \nMany platforms support the ability to load different boot or configuration files from local \nmemory. This methodology involves the current application instructing the device to \npoint to a new memory location for the boot/configuration information. In these cases, \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  50 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nthe device maintains a pointer to the original data if there  is a load error with new file. It \nis necessary to ensure that all boot/configurations can be authenticated with respect to \nits source and data integrity in the same manner as the base load . Many devices leave \nthis to the application to perform.  \nUse a FIPS 140 -2 compliant, Level 2 HSM  \nGenerate and store all authentication keys on a program -controlled, FIPS 140 -2 \ncompl iant, Level 2 Hardware Security Module (HSM) with the HSM c onfigured to \nenforce role -based restrictions on the use of the keys . Maintain an approved list of \nindividuals who can access the keys.  \nIt is worth noting  that there are additional protections that can be applied to the FPGA \nconfiguration data when its fielded location is physically unguarded. These include : \n Configuration file encryption using a NIST - or DoD-approved algorithm.  \n The use of split decryption keys to make key theft more difficult. This involves \nstoring multiple keys throughout the system, concatenating them , and then using \nthe hash of the concatenation as the decryption key.  \n The use of PUFs for key generation or a combination of PUF output and stored \nkey. \n Utilize any additional key protection mechanisms provided by the vendors.  \n Utilize good physical access prote ctions for the PCB.  \nTD 7: Adversary  substitutes modified FPGA software design \nsuite \nIn this threat , an adversary replaces the design suite an  application designer uses  with \none modified to subvert the application during synthesis, place and route , or \nconfiguration data generation. This is a n LoA1 threat because it targets  a specific \nprogram with a compromised executable. The adversary would have access \ncommercially to the vendor software and , by reverse engineering, could  modify the \nprogram to:  \n Subvert the security features of an FPGA during configuration data generation.  \n Insert a malicious function into the device during synthesis, place and route , or \nconfiguration data generation.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  51 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \n Insert a data leak or backdoor into the synthesized device during s ynthesis , place \nand route , or configuration data generation.  \nThis subverted tool would then be entered into the programs design environment by a \nvendor insider, an adversary -in-the-middle  technique,  or through a network intrusion. \nThis threat does not include the scenario where an FPGA vendor insider modifies the \nauthorized software for maliciou s purposes. That is covered by another threat and set of \nmitigations that applies at LoA2 . \nTD 7  mitigations  \n For DoD system owners, purchase from DoD authorized vend ors and \ndistributors . The D oD program acquisition grou p can provide this information.  \n Prevent automatic tool updates  by using a n installation  and update process  that \ndoes not require I nternet connectivity.  \n Use a protected  computing  environment . \n *Validate the cryptographic hash  against the hash signed by the vendor.  \nTD 7 mitigation d escriptions  \nPurchase from D oD authorized vendors and distributors  \nFor DoD system owners, u tilize DoD  authorized vendors and distributors for all \npurchases.  Authorized vendors can be identified through the acquisition organization.  \nPrevent a utomatic tool updates  \nPrevent automatic tool updates  by using an installation and update process that does \nnot require Internet connectivity.  \nUse a protected  computing  environment  \nThe program can select the protected environment from the following options:  \nOption 1:  Install and execute this software using  one of the following computing \nplatforms to protect from remote attack:  \n A computer and network classified at the DSCA Secret level o r above.  \n A computer and network certified for use in a Trust Category 1 facility as defined \nby DMEA.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  52 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \n A network -isolated computer enclave with limited and controlled access. This is a \ncomputer with the vendor software installed by a network administrator. T his \nadmin should not be a designer working on the application design.  \nOption 2:  Enforce compliance with Cybersecurity Maturity Model  Certification , level 3  \n(https://www.acq.osd.mil/cmmc/ ). \nValidate the c ryptographic hash  \nEnsure the cryptographic hash  is validated against the hash signed by the vendor. All \nparts of the software delivery should be authenticated in this manner including install \nmacros and other support functions. The program should only accept certificates \nvalidated by reputable third parties. The program should be limited to publicly released \nsoftware. The program should maintain documentation  with the source of the vendor -\nprovided has h and the actual software hash.  \nIn the event the hash is not provided or does not match the authorized  version, the \nprogram can choose from the options below : \nOption 1: Perform logical equivalency checking between the application HDL and the \nfinal configuration data. This effort should attempt t o verify that the final bitstream and \noriginating application HDL are logically equivalent with no extraneous logic in the final \nformat. This action will confirm that no logical modifications  were made  during the \nimplementation steps.  \nOption 2:  Use a reproducible build process to validate the software.   \nWhen using reproducible builds to validate software , enlist a third party to mirror the \nFPGAs synthesis, place and route, and configuration file generation. If the mirroring is \nexecuted properly and independently, the outputs can be compared to verify that the \nvendo r software package is unmodified or modified in a way that does not affect the \napplication design. To ensure proper execution of this mitigation, the following should  \nbe observed:  \n The software used to mirror the programs synthesis effort should  be procured in \na manner to make it independent from the procurement of the original version.  \n The reproducible build software should be loaded/installed by a different \nadministrator than the administrator that performed the original install.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  53 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \n This mitigation re quires independent duplicative activities since  the \nadversary could have knowledge about the project and how it obtains, \nloads , and controls its tools.  \n The mirrored effort should utilize the same version of the software on the same \noperating system and ver sion.  \n The application development teams software and the mirroring software should \npossess matching hashes and size values.  \n The mirrored effort should  utilize the same HDL code, IP , and synthesis scripts.  \n The mirrored effort should  utilize the same vendo r tool settings.  \n The output of the effort is an unencrypted, uncompressed configuration data file.  \nContact  the FPGA software vendor for more detailed guidance on creating reproducible \nbuilds. They have already performed work in this area and can assist wit h documented \ninstructions.  \nBoth the development effort and the mirror effort should execute the FPGA \ndevelopment flow from synthesis to configuration file output and then perform the \nfollowing steps:  \n Throughout the flow, output any intermediary files that can be used to compare \nresults at various stages. This can include primitive netlists, synthesized netlists, \nphysical netlists , and final configuration data files.  \n Compare the final configuration files for size and content. They should match in \nall respec ts except for header information that may include timestamps and other \nproperty information.  \n If the files are encrypted, take steps to ensure that any nonces, such as \nthe initialization vector, used by both efforts are the same.  \nIf discrepancies are found  in the comparison, contact  the software vendors for \nassistance.  \nIf a software version does  not match what was expected, JFAC recommends  report ing it \nto the vendor for further analysis and correction.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  54 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nTD 9: Adversary compromises single -board computing \nsystem (SBCS)  \nIn this threat, an adversary compromises a single -board computing system (SBCS) \npurchased by a program for use in a system. An SBCS is a commercial off -the-shelf \nproduct consisting of a PCB with FPGAs and computer processing resources. These \nboards are common throughout D oD systems as they are readily available in the \nmarketplace. Under this threat, the program does not have control of the manufacturing \nprocess of the SBCS , forcing the prog ram to rely upon a verification -heavy approach to \nmitig ating attacks.  Of primary concern in this scenario are threats to : \n Authenticity of the FPGA devices  \n PCB connections to the FPGA  \n The configuration methodology  \n Test interfaces  \nThe following mitigations only address the hardware assurance concerns related to the \nmanufacturing and operation of the FPGA device and do not consider other \ncomponents of the SBCS.  \nTD 9  mitigations  \n For DoD system owners, purchase from DoD authorized vendor s and \ndistributors . The D oD program acquisition grou p can provide this information.  \n Authenticate the FPGA devices .  \n Populate and inspect the SB CS. \n Document the steps taken to demonstrate compl iance  with TD 9.  \nTD 9 mitigation d escriptions  \nPurchase fro m DoD authorized vendors and distributors  \nFor DoD system owners, utilize DoD  authorized vendors and distributors for all \npurchases.  Authorized vendors can be identified through the acquisition organization.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  55 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nAuthenticate the FPGA devices  \nAuthenticate the FPGA devices. In this mitigation, the program can either order SBCSs \nwith unpopulated FPGA locations and perform the FPGA attachment, or the program \ncan physically inspect a sampled set of pre -populated boards.  \nPopulate and inspect the SBC S \nThe program should select one of the options below for the SBCS : \nOption 1:  Populate SBCSs in a classified or Trust Category 1 facility. If the program \nchooses to populate the SBCS boards with their own FPGA devices, they should \nadhere to the following ca veats:   \n Purchase an SBCS with empty FPGA location/bond -outs that can be populated \nby the program.  \n Purchase the FPGA devices and  perform the mitigations from TD 2: Adversary \ninserts malicious  counterfeit  for LoA1 on the FPGA devices to authenticate them.  \n Populate  the SBCS with the FPGA devices.  \n The work should be conducted in a classified setting or Trust Category 1 facility . \nOption 2:  Populate SBCSs in an external facility with authentication in a classified \nfacility. If the program chooses to populate the SBCS boards with FPGA devices in an \nexternal facility, they should adhere to the following caveats:  \n Purchase an SBCS with empty FPGA location/bond -outs that can be populated \nby the program.  \n Purchase the FPGA devices and  perform the mitigations from TD 2: Adversary \ninserts malicious  counterfeit  for LoA1 on FPGA devices to authenticate them.  \n Populate the SBCS with the FPGA devices.  \nFollowing SBCS population and return to the program, the FPGA devices should be \nauthenticated in a classified facility prior to moving to the next stage of system \nintegration. Authentication should  be performed according to the guidelines found in the \nmitigations for TD 2: Adversary inserts malicious  counterfeit . \nOption 3: Purchase complete SBCS devices and physically inspect the F PGA devices \ncontained on them.  To perform physical counterfeit inspection with destructive \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  56 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nsampling , see guidance provided under TD 2: Adversary inserts malicious counterfeit . In \naddition v erify the PCB connections  by: \n Obtain and review the SBCS schematics  for functional correctness, \nvulnerabilities , and security concerns as they relate to the FPGA configuration \nprocess and security connections.  \n Post assembly , verify the PCB traces related to the FPGA device, the \nconfiguration memory devices, and any other devices related to the \nauthentication of the configuration data. The program should rely on guidance \nfrom the JFAC PCB Executive Agent to perform this verifi cation.  \nVerify the SBCSs  FPGA configuration process  by using  SBCSs whose configuration  \nprocess and board -level connections comply with the LoA1 mitigation requirements for \nTD 6: Adversary swaps configuration file on target . This includes , but is not limit ed to , \nrequirements for:  \n Strong authentication algorithms , \n Differential power analysis (DPA) resistant authentication , \n Strongly protected key storage , \n Strong anti -tamper detection and response , \n Freedom from  known vulnerabilities in the configuration and s ecurity functions , \n Encryption and authentication key lengths  compliant with the requirements \noutlined NIST SP 800 -57, and  \n The ability to disable FPGA test pins , such as JTAG.  \nDocument the steps taken to demonstrate compliance  \nDocument all steps taken to d emonstrate compliance with TD 9.  These steps and \nassociated data artifacts should be auditable.  \n5. JFAC Survey Requ est \nIn support of these  mitigation s, JFAC asks all DoD programs seeking LoA compliance \nat any level to provide JFAC with information regarding the FPGA devices they are \nusing along with a brief summary of the ir use. This information will  assist in identifying \nadditional assurance gaps and mitigations.  Refer to Appendix C: JFAC FPGA Reporting \nTemplate  for the information a program should include i n the email.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  57 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nFor more information on how this data will be used , contact \nJFAC_HWA@radium.nscs.mil  \n6. Summary  \nThe mitigations in this report are intended to protect against adversarial  threats to  \nassurance on FPGA-based systems. Once a program incorporates the mitigations for \nthese 8 threat descriptions,  it can consider its FPGAs to have achieved LoA1 .  \nIf a program has developed alternate solutions for mitigating these threats , it can \nconsult with JFAC to determ ine if the alternative mitigation s are sufficient .  \nFinally, if a program has questions regarding this report  or requires assistance, it should \ncontact  JFAC at JFAC_HWA@radium.ncsc.mil  for assistance.  \n  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  58 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nAppendix  A: Standardized terminology  \nThe following terms are used in the Joint Federated Assurance Center Field \nProgrammable Gate Array Best Practices documents. These terms are  modified from \nDefense Acquisition University definitions to support common understanding.  \nApplication design   The collection of schematics, constraints, hardware description \nlanguage (HDL) , and other implementation files developed to generate an FPGA \nconfiguration file for use on one or many FPGA platforms.  \nApplication domain   This is the area of technology of the s ystem itself, or a directly \nassociated area of technology. For instance, the system technology domain of a radar \nsystem implemented using FPGAs would be \"radar\" or \"electronic warfare.\"  \nConfiguration file  The set of all data produced by the application d esign team and \nloaded into an FPGA to personalize it. Referred to by some designers as a bitstream, \nthe configuration file includes that information, as well as additional configuration \nsettings and firmware, which some designers may not consider part of  their bitstream.  \nControllable effect   Program -specific , triggerable function allowing the adversary to \nattack a specific target.  \nDevice/FPGA device   A specific physical instantiation of an FPGA.  \nExternal facility   An unclassified facility that is out  of the control of the program or \ncontractor.  \nField  programmable gate array (FPGA)   In this context  FPGA includes the full range \nof devices containing substantial reprogrammable digital logic. This includes devices \nmarketed as FPGAs, complex programmable logic device s (CPLD), system -on-a-chip \n(SoC) FPGAs, as well as devices marketed as SoCs and containing reprogrammable \ndigital logic capable of representing arbitrary functions. In addition, some FPGAs \nincorporate analog/mixed signal elements alongside subs tantial amounts of \nreprogrammable logic.  \nFPGA platform   An FPGA platform refers to a specific device type or family of devices \nfrom a vendor.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  59 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nHard IP   Hard IP is a hardware design captured as a physical layout, intended to be \nintegrated into a hardware design in the layout process. Hard IP is most typically \ndistributed as Graphic Design System II (GDSII). In some cases, Hard IP is provided by \na fabrication company and the user of the IP does not have access to the full layout, but \nsimply a size and the i nformation needed to connect to it. Hard IP may be distributed \nwith simulation hardware description language (HDL) and other soft components, but is \ndefined by the fact that the portion that ends up in the final hardware was defined by a \nphysical layout by  the IP vendor.  \nLevel of assurance (LoA)   A Level of Assurance is an established guideline that \ndetails the appropriate mitigations necessary for the implementation given the impact to \nnational security associated with subversion of a specific system, wit hout the need for \nsystem -by-system custom evaluation.  \nPhysical unclonable function (PUF)   This function provides a random string of bits of \na predetermined length . In the context of FPGAs, the randomness of the bitstring is \nbased upon  variations in the si licon of the device due to manufacturing. These bitstrings \ncan be used for device IDs or keys.   \nPlatform design   The platform design is the set of design information that specifies \nthe FPGA platform, including physical layouts, code, etc.  \nSoft IP   Soft I P is a hardware design captured in hardware description language \n(HDL), intended to be integrated into a complete hardware design through a synthesis \nprocess. Soft IP can be distributed in a number of ways, as functional HDL or a netlist \nspecified in HDL, encrypted or unencrypted.  \nSystem   An aggregation of system elements and enabling system elements to achieve \na given purpose or provide a needed capability.  \nSystem design  System design is the set of information that defines the \nmanufacturing, behavior, a nd programming of a system. It may include board designs, \nfirmware, software, FPGA configuration files, etc.  \nTarget  A target refers to a specific deployed instance of a given system, or a specific \nset of systems with a common design and function.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  60 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nTargeta bility  The degree to which an attack may have an effect that only shows up in \ncircumstances the adversary chooses. An attack that is poorly targetable would be more \nlikely to be discovered accidentally, have unintended consequences, or be found in \nstanda rd testing.  \nThird -party intellectual property (3PIP)   Functions whose development are not \nunder the control of the designer. Use of the phrase intellectual property, IP, or 3PIP in \noutlining this methodology of design review does not refer to property r ights, such as, \nfor example, copyrights, patents, or trade secrets. It is the responsibility of the party \nseeking review and/or the reviewer to ensure that any rights needed to perform the \nreview in accordance with the methodology outlined are obtained.  \nThreat category  A threat category refers to a part of the supply chain with a specific \nattack surface and set of common vulnerabilities against which many specific attacks \nmay be possible.  \nUtility  The utility of an attack is the degree to which an effect  has value to an \nadversarial operation. Higher utility effects may subvert a system or provide major \ndenial of service effects. Lower utility attacks might degrade a capability to a limited \nextent.  \nVulnerability  A flaw in a software, firmware, hardware,  or service component \nresulting from a weakness that can be exploited, causing a negative impact to the \nconfidentiality, integrity, or availability of an impacted component or components.    \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  61 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nAppendix B : IP reuse guidance  \nThere are several situations in which a program  or organization would like to reuse \npreviously generated IP. This IP can be generated internally (i.e.,  from an authorized \nDoD program , but for a different program th an originally developed ) or externally (i.e., \npurchased IP). In either cas e, there are situations in which this IP will not need \nadditional review. This appendix describes situations in which the IP should be \nevaluated and situations in which its use without an evaluation is permitted.  \nIP that was not generated or previously evaluated by a DoD program in conjunction with \nthe DoD Microelectronics FPGA LoA1 Best Practices should be evaluated by the \nprogram before use . This includes cases in which vendors have had the IP evaluated by \na thir d party. That review is not an acceptable alternative in accordance with FPGA \nLevels of Assurance Best Practices. Programs have the responsibility to perform  or \noversee all reviews.  \nReuse Conditions  \nIn general, at LoA1 , IP reuse  is acceptable as long as th e follo wing conditions have all \nbeen met:  \n The IP has been  developed internally for  an LoA1  or higher compliant  program  or \nthe IP was successfully internal ly evaluated at LoA1  or higher . \n All documentation associated with development or previous evaluation is \ncryptographically signed and  stored within the configuration management system \ncompliant with th is DoD Microelectronics FPGA LoA1 Best Practices do cument.  The \ndocumentation is provided to the new program in totality. The documentation \nremain s unchanged since the time the evaluation was performed.  \n The program cannot accept any IP in which the report has discrepancies from the \nversion received. The name of the IP, version information, hash, and all other \ninformation  match es exactly.   \n After the initial dev elopment or evaluation, the IP remain s unchanged. The IP should \nbe stored in a configuration mana gement system compliant with this  DoD \nMicroelectronics FPGA LoA1 Best Practices document.  The hash of the IP is \ncryptographically  signed and maintained separat e from the actual IP. The hash was \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  62 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nmaintained from the time of original usage through the time o f reuse.  The program \nverifies that the hash of the IP matches the stored hash value . \n In the event the IP was previously evaluated and assurance concerns were \nidentified, the se concerns  should be documented and provided to the program that \nwould like to reuse the IP. The program has the responsibility to accept or mitigate \nthe risk s based on individual program needs.  \nReuse Scenarios  \nThe following s ection describ es several use cases that provide additional details of \nwhen IP can or cannot be reused at LoA1.  \nScenarios in which LoA1 IP reuse is app ropriate : \na. The program  would like to reuse internally developed IP that was developed to be \ncompliant with LoA1, LoA2 , or LoA3 , but not previously evaluated outside of the \nprogram use.  \nIn this scenario, the IP was developed internally using the processes described  in \nthe Levels of Assurance Best Practice documents . Therefore, the IP was previously \nproven compliant. To reuse the IP , the program should  demonstrate compliance with \nthe conditions outlined in the Reuse Conditions section.  \nb. The program would like to reuse internally developed IP that was developed to be \ncompliant with LoA1, LoA2 , or LoA3 and previously successfully evaluated to be \ncompliant with LoA1, LoA2 , or LoA3.  \nIn this scenario, the IP was developed and evaluated internally using the processes \noutlined in the Levels of Assurance Best Practice documents . Therefore, the IP was \npreviously proven compliant. To reuse the IP , the program should  demonstrate \ncompliance with the conditions outlined in the Reuse Conditions section.  \nc. In this scenario the IP was developed by an external vendor. The 3PIP was \npreviously verified internally at LoA1 , LoA2 , or LoA3 . A differe nt program would like \nto now use this IP  for LoA1 purposes . \nIn this scenario, the IP was evaluated internally using the processes outlined in the \nLevels of Assurance Best Practice documents . Therefore, the IP was previously \nproven compliant. To reuse the I P, the program should  demonstrate compliance with \nthe conditions outlined in the Reuse Conditions section.   \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  63 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nUse Cases in which a n LoA1 IP  evaluation in accordance with Third -Party IP  Review  \nProcess for Level of Assurance 1  document would be required:  \na. The p rogram would like to use internally developed IP that was not developed or \nevaluated to satisfy any level of assurance. The program would like to use this IP at \nLoA1.  \nIn this scenario, the program should treat the IP the same as unevaluated externally \ndeveloped 3PIP. The program should follow the guidance provided within DoD \nMicroelectronics FPGA LoA 1 Best Practices  to mitigate  Threat Description 5 : \nAdversary compromises third-party soft IP.  \nb. The program would like to use externally developed 3PIP version X.1. Version X .0 \nwas previously verified to be LoA1 compliant.  \nIn this scenario, the IP has been modified. Due to the modification , the program \nshould treat the IP the same as unevaluated externally developed 3PIP. The \nprogram should follow the guidance provided within DoD Microelectronics FPGA \nLoA1 Best Practices to mitigate  Threat Description 5 : Adversary compromises third-\nparty soft IP.  \nc. At LoA1, the program would like to use externally developed 3PIP version X .0, which \nwas verified by an independent th ird party at LoA1, LoA2 , or LoA3 . \nThe program should treat the IP the same as not previously reviewed IP. The \nprogram should follow the guidance provided within DoD Microelectronics FPGA \nLoA1 Best Practices  to mitigate  Threat Description 5 : Adversary compr omises third-\nparty soft IP.  \nd. At LoA1, the program would like to use 3PIP version X .1. Version X .0 was internally \nverified at LoA2 or LoA3 .  \nIn this scenario, the IP to be used has been modified. Given the threats and level of \naccess at LoA1, the program should treat the IP the same as unevaluated externally \ndeveloped 3PIP. The program should follow the guidance provided within DoD \nMicroelectronics FPGA LoA 1 Best Practices  to miti gate Threat Description 5 : \nAdversary compromises third-party soft IP.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  64 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nAppendix C: JFAC FPGA reporting template  \nEach DoD program is requested to provide the following information to JFAC.  Multiple \nemail addresses are provided to support a variety of classification levels; only o ne email \nto any of these is required. Please contact  JFAC to obtain the appropriate email \naddress at JFAC_HWA@radium.ncsc .mil. \nThe template and informati on to be included in the email are as follows:  \n=========================== ==================  \n*** Please Portion Mark Appropriately ***  \n(U) POC Contact Info   \n(U) Name:  \n(U) Organization/Company:  \n(U) Email:  \n(U) Phone:  \n(U) Address:  \n  \n(U) Program Info   \n(U) Program Name (top -level program, i.e. F35, M1 tank, etc.):  \n(U) US Govt  Sponsor: (Air Force, Army, Marines, Navy, DOE, other)  \n(U) Do you want to be included in any future JFAC FPGA Assurance related bulletins in \nthe future?  \n(U) Estimated Number of Systems to be Built:  \n(U) Program Description (1 -3 sentences describing the t op-level program in which the \nsubsystem listed below is included):  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  65 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \n  \n(U) FPGA Info (for each FPGA part number used)  \n(U) FPGA Vendor: (Intel, Lattice, MicroChip, Xilinx, other)  \n(U) FPGA Device Family:  \n(U) FPGA Device Part Number:  \n(U) FPGA Design Software  Used and Version #:  \n(U) Description of Subsystem Containing FPGA Device:  \n(U) Total Estimated Number of Subsystems to be Built:  \n(U) Operating Environment: (mil, ind, com, radiation, cryo)  \n(U) Source/seller of the FPGA devices:  \n(U) Date purchased:  \n(U) Anticipated Fielding date:  \n(U) LoA Level:  \n(U) Description of FPGA Role in Subsystem.  If multiple instances of FPGA devices, \nnumber and describe the role of each.  \n1.  \n2.  \n3.  \n===============================================  \n \n  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  66 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nExample  \n=============== ==============================  \n*** Please Portion Mark Appropriately ***  \n(U) POC Contact Info   \n(U) Name:  Jack Jackson  \n(U) Organization/Company: Army Research Lab  \n(U) Email : jjackson@army_email.mil  \n(U) Phone:  555-555-5555  \n(U) Address:  10 Main St, Fort Murphy, Illinois 55555  \n  \n(U) Program Info   \n(U) Program Name (top -level program, i.e. F35, M1 tank, etc.):  Next Generation \nCombat Vehicle (NGCV)  \n(U) US Govt Sponsor: (Air Force, Army, Marines, Navy, DOE, other) Army  \n(U) Do you want to be included in any fut ure JFAC FPGA Assurance related bulletins in \nthe future? :  Yes \n(U) Estimated Number of Systems to be Built: 1400  \n(U) Program Description (1 -3 sentences describing the top -level program in which the \nsubsystem listed below is included ): \nThe Next Generation Combat Vehicle  Future Decisive Lethality (NGCV -FDL) \nwill have capabilities that are enabled by assured position, navigation , and \ntiming  and resilient networks . This will enable future maneuver formations to \nexecute semi -independent operations while condu cting cross -domain \nmaneuver against a peer adversary . \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  67 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \n  \n(U) FPGA Info (for each FPGA part number used)  \n(U) FPGA Vendor: (Xilinx, Intel, MicroChip, Lattice, other):  Acme MicroElectronics  \n(U) FPGA Device Family:  Big Blue Iceberg  \n(U) FPGA Device Part Number:  BBI-624L100K  \n(U) FPGA Design Software Used and Version #:  IceBreaker V2021.15  \n(U) Description of Subsystem Containing FPGA Device:  image processing for data \noriginating from the cannon targeting sensor  \n(U) Total Estimated Number of Subsystems to be Built: 3000  \n(U) Operating Environment: (mil, ind, com, radiation, cryo):  mil \n(U) Source/seller of the FPGA devices : Digikey, online  \n(U) Date purchased: 2/25/2020  \n(U) Anticipated Fielding date:  5/1/2022  \n(U) LoA  Level : 1 \n(U) Description of FPGA Role in Sub system.  If there are  multiple instances of FPGA \ndevices, number and describe the role of each  one.  \n1. FPGA #1  is used to perform  signal processing on raw image data coming in \nfrom the externally mounted canno n. \n2. FPGA #2  is used to perform signal processing on raw image data coming \nfrom the scout drone through the external antennae #2 and synchronized with \nGPS positioning data.  \n================ ===============================  \n  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  68 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nAppendix D: Guidance for embedded FPGA IP  \nLoA1 Introduction  \nWhile an embedded field programmable gate array (eFPGA) has many of the same \nfunctions and features of commercial FPGA devices, from an assurance stand point, it \nshould be viewed as an application -specific integrated circ uit (ASIC) third -party IP block. \nAssurance guidance for ASIC IP can be found in the JFAC ASIC/CIC  Best Practices \nGuides and should be followed when considering  the use of this macro in a custom \ndesign. However, there are some FPGA related assurance concern s that are inherent in \nthis IP block and can addressed with guidance from this document. For any eFPGA \nuse, the program should apply ASIC assurance best practices with the following FPGA \nadditions.  \nThe eFPGA is susceptible to a combination of ASIC and FPG A related threats. The \ntable below illustrates what threat categories are relevant to eFPGA at LoA1 and which \nguidance to follow.  \n# FPGA t hreat description (TD)  ASIC  FPGA  \nTD 3   Adversary compromises application design cycle   Yes Yes \nTD 4   Adversary compromises system assembly and keying \nand provisioning   NA Yes \nTD 5   Adversary compromises third -party soft IP   Yes No \nTD 6   Adversary swaps configuration file on target   No Yes \nTD 7   Adversary substitutes modified EDA software design \nsuite   Yes Not LoA1  \nTD 10   Adversary inserts a compromise during the ASIC \nfabrication process   Yes No \nTD 11   Adversary modifies FPGA software design suite   Not LoA1  Not LoA1  \neFPGA Guidance  \nWhat follows is guidance provided to mitigate eFPGA -specific threats. This should be \nadded to the steps already taken to assure the ASIC design and development flows.  \nTD 3: Adversary compromises application design cycle   \nThe eFPGA IP block shares all the ASIC design cycle concerns with some additions \nfrom the FPGA space. The user  is encouraged to reference the ASIC/CIC Best Practice \nGuidance for design cycle protections with the following FPGA enhancements. The \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  69 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nprogram should follow the FPGA guidance for the generation and protection of the \nconfiguration file and any associated cr yptographic keys. The concerns here include an \nadversary compromising or stealing keys and modifying the configuration file while in \nstorage. For this threat, it is recommended that the user follow the guidance found \nunder TD 3 in this document.  \nTD 4: Advers ary compromises system assembly, keying , or provisioning   \nThe eFPGA macro shares the same requirement for the loading of cryptographic keys \nand configuration data as an FPGA. All of the FPGA threat surfaces in this area apply \nequally to the use of eFPGA IP. This includes the modific ation of the configuration file or \nkeys, or their complete replacement during assembly and provisioning. The user should \nutilize the full guidance found under TD 4  of this document for the loading of their \neFPGA data onto their system.  \nTD 5: Adversary compromises third -party soft IP   \nThe eFPGA is a 3PIP hard macro and should be protected using guidance from the \nASIC/CIC Best Practice guides for 3PIP. As this IP is delivered as a hard embedded \nlayout macro, it is not able to be evaluated in the same  manner as a soft HDL function.  \nThus , it should be evaluated as a hard macro according to the guidance provided by the \nCIC Best Practice Guide obtainable from JFAC.  \nTD 6: Adversary swaps configuration file on target   \nThe eFPGA macro shares the same threat space as the FPGA with respect to the \ncompromise of the configuration file after provisioning. The user should apply the full \nguidance found in TD 6  in this document. Unlike FPGAs, the function to perform \nauthentication and store keys has to be designed and integrated into the ASIC by the \nprogram. This is not an easy task and does add cost and time to a development \nschedule. There are several means to achieve this protection and they are as follows:  \n Integrate an authentication mechanism in the ASIC with key s torage that follows the \nconstraints for this type of cryptographic protections found under TD 6  of this \ndocument.  \n Store the authenticated configuration file in memory on the ASIC itself. This protects \nthe configuration f ile from being modified at rest. The ability to update or replace the \nconfiguration file memory space should be protected  by authentication, using digital \nsignatures or passwords . \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  70 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \n Store the authenticated configuration file in a stacked memory device in the  ASIC \npackage. This protects the configuration file from being modified at rest. The ability \nto update or replace the configuration file memory space should be cryptographically \nprotected.  \nTD 7: Adversary substitutes modified FPGA software design suite   \nUnlike a commercial FPGA,  the details of the interface between the  eFPGA fabric and \nthe ASIC logic is not available to the eFPGA vendor. Additionally, the eFPGA is custom \nbuilt per the customers required size and layout. Finally, the eFPGA does not have the  \nplethora of additional functions embedded in a commercial FPGA , such as security, \ntamper, configurable IO, and SOC features , to compromise. An adversary would have \nto have specific knowledge of the programs application, its pinout , and the ASIC logic \ninterfacing to the application. This would have to be combined with detailed knowledge \nof the vendor tools and an ability to swap a modified version for the authorized version. \nThese facts would make it very difficult to coordinate an attack on the ASIC throu gh the \neFPGA software. Additionally, t he access requirements would push this threat out of \nLoA1 into a higher level and therefore is not addressed here from an FPGA perspective.  \nTD 10: Adversary modifies FPGA software design suite   \nThis threat does not meet the requirements for LoA1 for the same reasons as TD7. \nASIC guidance should be followed for this threat at LoA1 as applicable . \n  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  71 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nAppendix E: Checklist s and data/documentation  \nrequirements  \nChecklist for TD  1: Adversary utilizes a k nown FPGA platform \nvulnerability  \nTD 1 mitigation s Data/ Documentation requirement  \nUse caution when \nselecting tools or \nplatforms  The program should document the name of the person \nperforming the research  on tools/platforms , the date timestamp of \nthe research, the research results, and the vendor -provided end -\nof-life plan or release notes (if available). If a beta/initial release is \nselected , the program should document the rationale behind the \nselection and contain the signature of the programmatic ap proval \nauthority.  \nResearch vulnerabilities  The program should document each publication that was \nsearched (minimally those identified in this guidance should be \nsearched) , the search results, the name of the person performing \nthe search , and the date timestamp of when the search was \nperformed.  \nIf vulnerabilities are found when researching vulnerabilities, choose one:  \nOption 1: Select a \ndifferent  tool For the different tool, t he program should document each \npublication that was searched (minimall y those identified in this \nguidance should be searched) , the search results, the name of \nthe person performing the search , and the date timestamp of \nwhen the search was performed.  \nOption 2: Work with \nvendor  The program should maintain documentation regarding the \nidentified vulnerability, log communication with the vendor, and \ndocument the source and method of the received fix.  \nOption 3 : Risk analysis  The program should maintain documentation identifying the risk, \nany mitigations, and the approval authority for accepting the \nresidual risk . \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  72 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nTD 1 mitigation s Data/ Documentation requirement  \nUse a revision \ncontrol/version \nmanagement system  The program should document and enforce a configuration \nmanagement (CM) plan that is compliant with CMMC L evel 3 or \nNIST SP 800-171 Protecting Controlled Unclassified Information \nin Nonfederal Systems and Organizations  and NIST SP 800-172 \nEnhanced Security Requirements for Protecting Controlled \nUnclassified Information . The program should  document how the \nCM plan is compliant with  the requirements.  \nThe conf iguration management plan  should  include details on \nhow configuration data will be maintained for control and audit \npurposes.  It should include management of document/data, \nreleases, backups and archives, refresh of backup media, \nretention of tools and sof tware, test equipment , and the test \nenvironment.  \nAudit logs should be reviewed with the results recorded.  \nEnforce auditability  The program should maintain audit logs on all design data to \ninclude requirements, architecture, design , code, test s, bugs , and \nfixes. The audit data minimally should document who requested \nthe change with date  timestamp, what  decision was made \nregarding the change, who made the decision with date \ntimestamp, why the change was requested, and who made the \nchange with date times tamp . \nPerform a v ulnerability \ndata review   The program should ensure vulnerability  data is reviewed  by two \ndistinctly independent individuals.  The program should obtain the \nresults of independent reviews to include:  \n  Type and extent of verification performed, includ ing evaluation \nobjective, data used, sources, etc. \n  Findings, both positive and negative, for all evaluations \nperformed  \n  Risks identified by the review team (e.g., quality issues, \nvulnerability to thr eats, etc.)  \n  Recommendations , if any  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  73 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nTD 1 mitigation s Data/ Documentation requirement  \nPerform r outine \nemployment monitoring  Maintain employment records to include application, background \nchecks, etc.  Audit employee assessments, and discipline logs.  \nPrevent a compromised insider  from hiding a vulnerability  by choosing one of the following \noptions:  \nOption 1: Perform \nindependent reviews  The program should obtain the results of independent reviews to \ninclude:  \n  Type and extent of verification performed, to include evaluation \nobjective, methodolo gy, and tools  \n  Findings, both positive and negative, for all evaluations \nperformed  \n  Risks identified by the review team (e.g., quality issues, \nvulnerability to threats, etc.)  \n  Recommendations to mitigate identified risks  \n  Identification and credentials of each reviewer,  showing that the \nreviewers are i ndependent from the team doing the design  \n  Time/date stamp of when the review was performed  \nOption 2: Use personnel \nwith Secret -level \nclearance  The program should keep a  log of personnel assigned along with \ntheir clearance level.  \nChecklist for TD  2: Adversary inserts malicious counterfeit  \nTD 2 mitigation s Documentation requirements  \nPurchase from DoD \nauthorized vendors \nand distributors  The program should document the name and location of the \nauthorized vendor along with documentation demonstrating the \nvendor is authorized.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  74 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nTD 2 mitigation s Documentation requirements  \nFollow storage and \nshipping guidance  The program should document, maintain , and enforce a \ntransportation plan which supports the movement of bulky classified \nmaterial. Minimally the plan should include:  \n Title of Plan  \n Date of movement  \n Authorization/Approval  \n Purpose  \n Description of consignment, to include unique ID when available  \n Identification of responsible government and/or company \nrepresentatives  \n Identification of commercial entities to be involved in each \nshipment  \n Packaging of the consignment  \n Routing of the consignment  \n Couriers/escorts  \n Recipient  responsibilities  \n Return of material procedures  \n Other information as required  \nValidate the authenticity of the FPGA device . Choose one:  \nOption 1:  \nCryptographically \nsecure I D The program should document and store the ID of each FPGA , the \nID that was provided directly by the vendor , the date timestamp of \nwhen the ID was validated cryptographically, and who performed \nthe validation . \nOption 2:  Physical \nanalysis  The program should document the results of the physical analysis \ntest with e ach FPGA unique ID the test was performed on , the date \ntimestamp of the analysis, and who performed the analysis . \nThe program should maintain documentation on the sample \nselection process. This could be from the use of a non -human \nselection process, cleared personnel, or independent party. The \nprogram should m aintain the list of devices used for samples . \nDocument th e process to secure the device and the results , as well \nas, documentation of all parties that touched the device with the \nreason for the interaction.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  75 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nChecklist  for TD  3: Adversary compromises application design cycle  \nTD 3 mitigation s Documentation requirements  \nUse c leared \npersonnel  in a \ncleared e nvironment  In writing, t he program should designate work that mus t be done \nby cleared Individuals . The program should keep a log of \npersonnel assigned to that work along with their clearance level.  \nThe program should maintain a list of the members comprising \neach team, along with their clearance level. The program should  \nmaintain audit logs  stating what each team member accessed.  \nThe program  should maintain SSP documentation . \nTrack critical data in \na revision control  \nsystem  The program should ensure the following data items are tracked \nin revision control:  \n Third -party IP (3PIP)  \n Utilized  libraries  \n Development files, code, software used for development, \nsynthesis scripts, and tools  \n Test benches, test plans, test procedures, and test reports  \n Tool configuration settings  \n Design documents  to include:  \n Critical documents, to minimall y include requirements, \ndesign artifacts, test reports, test plans, and discrepancy \nreports.  \n Documentation with approval to proceed from \norganizationally defined reviews: code reviews, architecture \nreviews, technical design reviews, and verification and \nvalidation reviews.  \nEach of the artifacts should be identified in the program s auditing \nstrategy and the audit logs should minimally include  decisions \nthat were made, by whom, for what reason, and on what date.  \nEnforce auditability  The program should maintain audit logs on all design data to \ninclude requirements, architecture, design , code, test s, bugs , and \nfixes. The audit data minimally should document who requested \nthe change with date  timestamp, what  decision was made \nregarding the change, who made  the decision with date \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  76 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nTD 3 mitigation s Documentation requirements  \ntimestamp, why the change was requested, and who made the \nchange with date timestamp . \nUse a r evision \ncontrol /version \nmanagement  system  The program should document and enforce a configuration \nmanagement (CM) plan that is compliant with CMMC L evel 3 or \nNIST SP 800-171 Protecting Controlled Unclassified Information \nin Nonfederal Systems and Organizations  and NIST  SP 800-172 \nEnhanced Security Requirements for Protecting Controlled \nUnclassified Information . The program should  document h ow the \nCM plan is compliant with  the requirements.  \nThe configuration management plan  should  include details on \nhow configuration data will be maintained for control and audit \npurposes.  It should include management of document/data, \nreleases, backups and ar chives, refresh of backup media, \nretention of tools and software, test equipment , and the test \nenvironment.  \nAudit logs should be reviewed with the results recorded.  \n3.1 Mitigating the i ntroduction of a compromised design  into the application  \nIsolate and store the \napplication design  The program should document the hash of the final configuration \nafter the final design and verify the hash prior to provisioning. The \nprogram should maintain the configuration management audit \nlogs.  \nPerform a \nreproducible build  Document the reproducible build process and results validating \nthat the separate build s produce the same binary and hash.  \n3.2 Mitigating the modification of test benches/plan to reduce coverage or hide Trojan \ncode  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  77 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nTD 3 mitigation s Documentation requirements  \nExecute a \ndocumented  test \nplan  The program should document  and maintain a test plan that \nincludes a mechanism to verify all requirements.  \n The test plan should explicitly list code coverage metrics, the \ntype of testing that will be performed, and acceptable testing \nguidelines.  \n Code coverage should state how much code is checked by the \ntest bench, providing information about dead code in the design \nand holes in test suites. Ensure code coverage includes \nstatement coverage, branch coverage, Finite State Machine \n(FSM), condition, expression, and toggle coverage. Document \nany code that will not be covered and why. Ensure untested code \nis docu mented and reviewed through the review process. Use \nfunctional test s to verify the FPGA does what it is supposed to do. \nAny deviations should  be documented and approved.  \n The decision to use/not use other types of testing such as \ndirected test, constrain ed random stimulus, and assertion should \nbe documented.  \n Unexpected behavior should be documented and analyzed, with \nfinal implementation conclusions documented.  \n The test plan should specify the verification environment which \ndescribes the tools, the software, and the equipment needed to \nperform the reviews, analysis, and tests. Each of these items \nshould be maintained under revision control.  \n Ensure all test discrepancies, bugs, etc. are resolved via a \nchange process.  \nValidate and verify \ntest proc esses  The program should document, review, maintain, enforce , and \narchive the test plan. The test plan should include which tools will \nbe used with names, version numbers, and the various test \nreviews that will take place, type of testing to be performed, and \nthe methods used to accomplish the test.  \nThe program should maintain documentation of all testing \nperformed, including members of each team and role, all \ndocumentation associated with peer reviews , configuration logs \nindicating all actions taken, by wh om and when , and use of \nautomated tools where applicable.  All test discrepancies, bugs, \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  78 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nTD 3 mitigation s Documentation requirements  \netc. should be resolved via a change process utilize a change \nmanagement system.  The established processes should be \ndocumented, enforced , and audited . \nEnsure the t est \nenvironment is \nmaintained via \nconfiguration \nmanagement  The program should maintain the test environment based on the \nconfiguration management plan in accord ance with requirements \nof CMMC L evel 3 or NIST SP 800-171 Protecting Controlled \nUnclassified Inform ation in Nonfederal Systems and \nOrganizations  and NIST SP 800-172 Enhanced Security \nRequirements for Protecting Controlled Unclassified Information . \nThe program should maintain the CMMC audit results or NIST SP \n800-171 self -assessments.  \nUse a r evision  \ncontrol/version \nmanagement  system  The program should document and enforce a configuration \nmanagement (CM) plan that is compliant with CMMC L evel 3 or \nNIST SP 800-171 Protecting Controlled Unclassified Information \nin Nonfederal Systems and Organizations  and NIST SP 800-172 \nEnhanced Security Requirements for Protecting Controlled \nUnclassified Information . The program should  document how the \nCM plan is compliant with  the requirements.  \nThe configuration management plan  should  include details on \nhow configuration data will be maintained for control and audit \npurposes.  It should include management of document/data, \nreleases, backups and archives, refresh of backup media, \nretention of tools and software, test equipment , and the test \nenvironment.  \nAudit logs should be reviewed with the results recorded.  \n3.3 Mitigating the  introduc tion of Trojans into the application design during \ndevelopment  \nEnsure all design \nartifacts have a \ndirect bi -directional \nlink to approved \nrequirements  The program should document bi -directional traceability for all \ndevice requirements, including derived requirements.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  79 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nTD 3 mitigation s Documentation requirements  \nEnforce peer review  The program should document the results of each peer review to \ninclude:  \n  Entry criteria and status  \n  Roles and responsibilities with associated names  \n  Attendees  \n  Findings, including deviations or waivers and associated \nrationale and approval  \n  Exit criteria and status  \nExecute a \ndocumented test \nplan  The program should document  and maintain a test plan that \nincludes a mechanism to verify all requirements.  \n The test plan should explicitly list code coverage metrics, the \ntype of testing that will be performed, and acceptable testing \nguidelines.  \n Code coverage should state how much code is checked by the \ntest bench, providing information about dead code in the design \nand holes in test suites. Ensure code coverage includes \nstatement coverage, branch coverage, Finite State Machine \n(FSM), condition, expression, and toggle coverage. Docu ment \nany code that will not be covered and why. Ensure untested code \nis documented and reviewed through the review process. Use \nfunctional test s to verify the FPGA does what it is supposed to do. \nAny deviations should  be documented and approved.  \n The dec ision to use/not use other types of testing such as \ndirected test, constrained random stimulus, and assertion should \nbe documented.  \n Unexpected behavior should be documented and analyzed, with \nfinal implementation conclusions documented.  \n The test plan  should specify the verification environment which \ndescribes the tools, the software, and the equipment needed to \nperform the reviews, analysis, and tests. Each of these items \nshould be maintained under revision control.  \n Ensure all test discrepancies, b ugs, etc. are resolved via a \nchange process.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  80 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nTD 3 mitigation s Documentation requirements  \nImplement, validate, \nand verify test \nprocesses  The program should document, review, maintain, enforce , and \narchive the test plan. The test plan should include which tools will \nbe used with names, version numbers, and the various test \nreviews that will take place, type of testing to be performed, and \nthe methods used to accomplish the test.  \nThe program shou ld maintain documentation of all testing \nperformed, including members of each team  and role, all \ndocumentation associated with peer reviews , configuration logs \nindicating all actions taken, by whom and when , and use of \nautomated tools where applicable.  All test discrepancies, bugs, \netc. should be resolved via a change process utilize a change \nmanagement system.  The established processes should be \ndocumented, enforced , and audited . \nSelect a formal \nproof  process  Document  all code that was review ed using LE C. Document any \nfunctional discrepancies and how those discrepancies were \nresolved.  \n3.4 Mitigating the introduction of compromised tooling/software into the environment  \nAccept only digitally \nsigned software \ndeliveries  The program should document the software author  listed in the \ndigital signature, the timestamp of the digital signature, and the \ntimestamp of when the digital signature was validated.  \nValidate \ncryptographic \nhash es  The program should document the value of the calculated \ncryptographic ha sh and the signed hash provided by the vendor \nalong with the software name, version , and release number.  \nResearch \nvulnerabilities  The program should document each publication that was \nsearched (minimally those identified in this guidance should be \nsearched) , the search results, the name of the person performing \nthe search , and the date timestamp of when the search was \nperformed.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  81 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nTD 3 mitigation s Documentation requirements  \nIf vulnerabilities are found when researching vulnerabilities or tools, choose one:  \nOption 1: Select a \ndifferent  tool For the different tool, t he program should document each \npublication that was searched (minimally those identified in this \nguidance should be searched) , the search results, the name of \nthe person performing the search , and the date timestamp of \nwhen the se arch was performed.  \nOption 2: Work with \nvendor  The program should maintain documentation regarding the \nidentified vulnerability, log communication with the vendor, and \ndocument the source and method of the received fix.  \nOption 3 : Risk \nanalysis  The program should maintain documentation identifying the risk, \nany mitigations, and the approval authority for accepting the \nresidual risk .  \nUse a r evision \ncontrol/version  \nmanagement system  The program should document and enforce a configuration \nmanagement (CM) plan that is compliant with CMMC L evel 3 or \nNIST SP 800-171 Protecting Controlled Unclassified Information \nin Nonfederal Systems and Organizations  and NIST SP 800-172 \nEnhanced Security  Requirements for Protecting Controlled \nUnclassified Information . The program should  document how the \nCM plan is compliant with  the requirements.  \nThe configuration management plan  should  include details on \nhow configuration data will be maintained for cont rol and audit \npurposes.  It should include management of document/data, \nreleases, backups and archives, refresh of backup media, \nretention of tools and software, test equipment , and the test \nenvironment.  \nAudit logs should be reviewed with the results recor ded.  \nUtilize a  reproducible \nbuild  process  Document the reproducible build process and results validating \nthat the separate builds produce the same binary and hash.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  82 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nTD 3 mitigation s Documentation requirements  \nSelect a formal \nproof  process  Document  all code that was review ed using LEC . Document any \nfunctional discrepancies and how those discrepancies were \nresolved.  \n3.5 Mitigating intrusion into the internal network  \nUse a r evision  \ncontrol/version \nmanagement  system  The program should document and enforce a configuration \nmanagement (CM) plan that is compliant with CMMC L evel 3 or \nNIST  SP 800-171 Protecting Controlled Unclassified Information \nin Nonfederal Systems and Organizations  and NIST SP 800-172 \nEnhanced Security Requirements for Protecting Controlled \nUnclassified Information . The program shou ld document how the \nCM plan is compliant with  the requirements.  \nThe configuration management plan  should  include details on \nhow configuration data will be maintained for control and audit \npurposes.  It should include management of document/data, \nreleases, b ackups and archives, refresh of backup media, \nretention of tools and software, test equipment , and the test \nenvironment.  \nAudit logs should be reviewed with the results recorded.  \nAssign privileges \nand accesses based \non roles  The program should approve, do cument , and maintain all \nindividuals, the roles they perform and the access allowed by that \nrole. At a minimum, these roles should include design, test, \nnetwork administration , and system administration.  \nControl and monitor \naccess  Entry/access to appropriate areas should be recorded, monitored, \nand logged for auditability.  \nResearch  \nvulnerabilities  The program should document each publication that was \nsearched (minimally those identified in this guidance should be \nsearched) , the search results, the name of the person performing \nthe search , and the date timestamp of when the search was \nperformed.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  83 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nTD 3 mitigation s Documentation requirements  \nPurchase from DoD \nauthorized vendors \nand distributors  The program should document the name and location of the \nauthorized vendor along with documentation demonstrating the \nvendor is authorized.  \nChoose one protected  computing environment  option:  \nOption 1:  DSCA \nSecret Level network  Maintain log of personnel with clearance information . \nMaintain all records in accordance with maintaining a DSCA \nSecret network as well as a  documented and enforced SSP.  \nOption 2:  DMEA \nCertified network  The program should maintain proof of DMEA certification . \nOption 3:  Network -\nIsolated Computer \nEnclave  The program should maintain documentation with the enclave  \nlayout, open ports, etc.  \nOption 4:  Enforce \nCMMC level three \nrequirements or \nimplement the latest \napproved version of \nNIST SP 800 -171 \nand NIST SP 800 -\n172. The program should maintain Cybersecurity Maturity Model \nCertification (CMMC) audit data.  Until CMMC is the program \nrequirement, the program should maintain a self -assessment \ndemonstrating compliance with NIST SP 800 -171 Protecting \nControlled Unclassified Information in Nonfederal Syst ems and \nOrganizations  and NIST SP 800 -172 Enhanced Security \nRequirements for Protecting Controlled Unclassified Information . \n3.6 Mitigating risk from compromised hire or employee  \nEnforce auditability  The program should maintain audit logs on all design data to \ninclude requirements, architecture, design , code, test s, bugs , and \nfixes. The audit data minimally should document who requested \nthe change with date  timestamp, what  decision was made \nregarding the change, who made the decision with date \ntimestamp, why the change was requested, and who made the \nchange with date timestamp . \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  84 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nTD 3 mitigation s Documentation requirements  \nTrack critical data  in \nrevision control  The program should ensure the following data items are tracked \nin revision control:  \n Third -party IP (3PIP)  \n Utilized libraries  \n Development files, code, software used for development, \nsynthesis scripts, and tools  \n Test benches, test plans, test procedures, and test reports  \n Tool configuration settings  \n Design documents  to include:  \n Critical documents, to minimally include requirements, \ndesign artifacts, test reports, test plans, a nd discrepancy \nreports.  \n Documentation with approval to proceed from \norganizationally defined reviews: code reviews, architecture \nreviews, technical design reviews, and verification and \nvalidation reviews.  \nEach of the artifacts should be identified in the program s auditing \nstrategy and the audit logs should minimally include  decisions \nthat were made, by whom, for what reason, and on what date.  \nAdopt a s tructured  \napplication  design \nprocess  The program should document and utilize the entry and exit \ncriteria of each stage of the design process. This includes \ndocumentation for each peer review and design review with roles \nand responsibilities with associated names, attendees, and \nfindings , including deviations or waivers and associated rationale \nand approvals.  \nAll design changes should be documented and approved, and \ntesting should adhere to organizationally approved test \nstandards.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  85 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nTD 3 mitigation s Documentation requirements  \nReview c ritical  \nactivitie s The program should obtain the resu lts of independent reviews to \ninclude:  \n  Type and extent of verification performed, to include evaluation \nobjective, methodology, and tools  \n  Findings, both positive and negative, for all evaluations \nperformed  \n  Risks identified by the review team (e.g. , quality issues, \nvulnerability to threats, etc.)  \n  Recommendations to mitigate identified risks  \n  Independent team should be separate from the team doing the \ndesign  \n  Identification and credentials of each reviewer  \n  Time/date stamp of when the review  was performed  \nEnforce reviewer criteria with one of the following options:  \nOption 1:  Perform \nindependent third -\nparty reviews  In writing, t he program should designate work that mus t be done \nby independent third -party reviewers or cleared personnel . The \nprogram should keep a log of personnel assigned to that work \nwith their clearance level, the data reviewed, date timestamp and \nduration of the review, and the results of the review, in addition to  \naudit logs demonstrating what each team member access ed. \nOption 2:  Perform \nindependent reviews \nby Secret -cleared \nindividuals  In writing, t he program should designate work that mus t be done \nby independent third -party reviewers or cleared personnel . The \nprogram should keep a log of personnel assigned to that work \nwith their clearance level, the data reviewed, date timestamp and \nduration of the review, and the results of the review, in addition to  \naudit logs demonstrating what each team member accessed.  \nOption 3: Perform \nindependent reviews \nby cleared \nindividu als in a \nDSCA Secret -level \nfacility  In writing, t he program should designate work that mus t be done \nby independent third -party reviewers or cleared personnel . The \nprogram should keep a log of personnel assigned to that work \nwith their clearance level, the data reviewed, date timestamp and \nduration of the review, and the results of the review, in addition to  \naudit logs demonstrating what each team member accessed.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  86 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nChecklist for TD  4: Adversary compromises system assembly, keying, \nor provisioning  \nTD 4 mitigation s Documentation requirements  \nPurchase from DoD and \nvendor  authorized \ndistributors  The program should document the name and location of the \nauthorized vendor along with documentation demonstrating the \nvendor is authorized.  \nFollow s torage and \nshipping  guidance   The program should document, maintain , and enforce a \ntransportation plan which supports the movement of bulky \nclassified material. This plan should specifically address DoD \n5220.22 -M, Chap ter 5 Sec tion 3. Minimally the plan should \ninclud e:  \n Title of Plan  \n Date of movement  \n Authorization/Approval  \n Purpose  \n Description of consignment, to include unique ID when available  \n Identification of responsible government and/or company \nrepresentatives  \n Identification of commercial entitie s to be involved in each \nshipment  \n Packaging of the consignment  \n Routing of the consignment  \n Couriers/escorts  \n Recipient responsibilities  \n Return of material procedures  \n Other information as required  \nProvide k eys and \nconfiguration data  The program should document assembly house receipt of data \npackages and the hash value of the packages.  \nClear memory devices  The program should document the company, location, individual , \nand method for clearing the contents along with the contents \nbefore and after clearing.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  87 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nTD 4 mitigation s Documentation requirements  \nProvision private keys  The program sh ould document:  \n The company name, location , and date of provisioning  \n The number of provisioned devices and number of unique keys \nused  \n Proof of DSCA facility classification  \n Proof of DMEA Trust Cat egory  I certification  \nSelect an option and verify to protect the FPGA from attack during assembly and \nprovisioning : \nOption 1:  Assemble and \nprovision the system in a \nDSCA Classified Secret \nor Trust Category I \ncertified facility  The program sh ould document:  \n The company name, location and date of provisioning  \n The number of provisioned devices and number of unique keys \nused  \n Proof of DSCA facility classification  \n Proof of DMEA Trust Cat I certification  \nOption 2:  Assembl e and \nprovision in an external \nunclassified facility  The program sh ould document:  \n The company name, location and date of provisioning  \n The number of provisioned devices and number of unique keys \nused  \n Proof of CMMC audit  \nVerify assembly and \nprovisioning a ctivities  The program should maintain documentation including the \nprocedures used to verify the PCB traces, where the work was \nperformed, when it was performed , and the results of the \nverification.  \nThe program should maintain documentation including the \nprocedures used to authenticate the configuration data, where \nthe work was performed, who performed it, when it was \nperformed , and the results of the verification.  \nThe program should maintain document ation including the \nauthentication methodology, its architecture , and compliance with \nappropriate NIST standards.  \nThe program should maintain documentation including the \nmethodology used to verify the proper keys were loaded, where \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  88 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nTD 4 mitigation s Documentation requirements  \nthe work was performed, when it was performed , and who \nperformed the work.  \nThe program should maintain documentation including the \nprocedure s used to authenticate the post -assembly FPGA \ndevice, where the authentication was performed, by whom, when , \nand the results of the verifica tion. \nChoose one option to authenticate the FPGA device : \nOption 1: Verify the \nunique cryptographic ID  The program should document:  \n The authenticity verification method  \n The verification outcomes  \n The individual name or reference ID who performed the \nverification  \nOption 2:  Verify the \ndevice on the PCB  The program should document:  \n The authenticity verification method  \n The verification outcomes  \n The individual name or reference ID who performed the \nverification  \nOption 3:  Use a soft \nPUF  The program should document:   \n The authenticity verification method  \n The verification outcomes  \n The individual name or reference ID who performed the \nverification  \nChecklist for TD  5: Adversary compromises third -party soft IP  \nTD 5 mitigation s Documentation requirements  \nPurchase from DoD \nauthorized vendors and \ndistributors  The program should document the name and location of the \nauthorized vendor along with documentation demonstrating the \nvendor is authorized.  \nOnly a ccept IP that is \nunobfuscated   The program should keep a copy of the clean unobfuscated code, \nalong with the name and or ID of the person who received it.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  89 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nTD 5 mitigation s Documentation requirements  \nValidate the \ncryptographic hash of the \nIP  The program should document the value of the calculated \ncryptographic hash and the signed hash provided by the vendor \nalong with the software name, version , and release number.  \nCheck IP into revision \ncontrol  The program should include the initial IP and hash check -in within \nthe system.  \nExamine IP for malicious \nfunctions  The program should document all results in accordance with \nThird -Party IP Review Process for Level of Assurance 1.  \nAll interaction with JFAC regarding examining IP for malicious \nfunctions should be documented.  \nChecklist for TD  6: Adversary swaps configu ration file on target  \nTD 6 mitigation s Documentation requirements  \nIncorporate \ncryptographic \nauthentication  The program should document:  \n The method used to authenticate the configuration file on load  \n The verification process used to test the authentication method  \nAuthenticate \nconfiguration data each \ntime the data  is loaded  For each configuration load method used , the program should \ndocument :  \n The method used to authenticate the configuration file on load  \n The verification process used to test the authentication method  \nPrevent direct read back  The program should document the steps taken to prevent direct \nread back  of private keys . \nUse a CNSS/ NIST -\napproved algorithm and \nkey length  The program  should document the key length being used along \nwith the version number of the latest FIPS guidance and the \napproved key length in accordance with the guidance.  \nDisable operation or use \nof test access pins  The program should maintain documentation includ ing the means \nby which the JTAG test pins were disabled.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  90 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nTD 6 mitigation s Documentation requirements  \nEnsure authentication is \nenabled for application \nmodifications  Document if the FPGA allows application change s, how the \nvendor states  authentication will apply to all re configuration data,  \nand test results indicating how authentication was actually \napplied to all reconfiguration data.  \nUse a  FIPS 140 -2 \ncompliant, Level 2 H SM Document how the program utilizes FIPS 140 -2. Document the \nHSM that is being used and the spec sheet showing  FIPS \ncomplian ce. \nChecklist for TD  7: Adversary substitutes modified FPGA software \ndesign suite  \nTD 7 mitigation s Documentation requirement  \nPurchase from DoD \nauthorized vendors and \ndistributors  The program should document the name and location of the \nauthorized vendor along with documentation demonstrating the \nvendor is authorized.  \nPrevent a utomatic tool \nupdates  The program should document, maintain , and follow the SSP.  \nUse a protected \ncomputing  environment  The program should maintain documentation demonstrating one \nof the following computing platforms were utilized:  \n A computer and network classified at the DSCA Secret level or \nabove. The documentation should include all records in \naccordance with  maintain ing a DSCA Secret network .  \n A computer and network certified for use in a Trust Category 1 \nfacility as defined by DMEA.  \n A network -isolated computer enclave with limited and controlled \naccess.  \nThe documentation should include a log of personnel along with \ntheir clearance level  and a documented SSP  for all networks . \nValidate the \ncryptographic hash  The program should maintain the value of the calculated hash, \nand the hash that is provided by the vendor along with the \nversion/release number and date  timestamp.  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  91 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nTD 7 mitigation s Documentation requirement  \nIf the hash is not provided or does not match, validate the tool output using one of the \nfollowing options:  \nOption 1:  Perform \nlogical equivalency check  Document  all code that was reviewed using LEC. Document any \nfunctional discrepancies and how those discrepancies were \nresolved.  \nOption 2:  Use a \nreproducible b uild \nprocess to validate the \nsoftware  Document the reproducible build process and results validating \nthat the separate build s produce the same binary and hash.  \nChecklist for TD  9: Adversary compromises single -board computing \nsystem (SBCS)  \nTD 9 mitigation s Documentation requirement  \nPurchase from DoD \nauthorized vendors and \ndistributors  The program should document the name and location of the \nauthorized vendor along with documentation demonstrating the \nvendor is authorized.  \nAuthenticate the FPGA \ndevice s  The program should document the physical inspection results for \neach slash sheet and UID for the device  inspected.  \nChoose an option to populate and inspect the SBCS:  \nOption 1: Populate \nSBCSs in a classified or \nTrust Category 1 facility . Document the company name, responsible division , and physical \naddres s of the verification location, along with the associated \nwork order, quote, and purchase order ( PO).  \nOption 2:  Populate \nSBCSs in an external \nfacility with \nAuthentication in \nClassified Facility.  Document the company name, responsible division , and physical \naddres s of the verification location, along with the associated \nwork order, quote, and purchase order ( PO).  \n\n \n \nU/OO/ 156781 -24 | PP-24-1960 | May 2024 Ver. 1.1  92 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Level of Assurance 1 Best Practices  \nTD 9 mitigation s Documentation requirement  \nOption 3 : Purchase \ncomplete SBCS devices \nand physically inspect \nthe FPGA devices \ncontained on them.   Document the company name, responsible division , and physical \naddres s of the verification location, along with the associated \nwork order, quote, and purchase order ( PO).  \nDocum ent the steps  \ntaken to demonstrate \ncompliance  Document the steps take n to comply with these requirements. \nThis includes hardware and software featu res. \n \n\n",
  "cves": [],
  "techniques": [],
  "advisory": "cybersecurity-alerts",
  "title": "ctr_dod_microelectronics-fpga_loa1_best_practices",
  "source": "nsa",
  "id": "2bbe2e233d628535fecf9034dc8b04588cec4244e29768f4977f97a9d7001bd8"
}