{
  "markdown": " \n \n \n \nNational Security Agency  \nCybersecurity and  Infrastructure Security Agency  \n \nCybersecurity  Technical Report  \n \n \n \n \n \nKubernetes  Hardening Guid e \n \n \n \n \n \n \n \nAugust  2022  \n \nU/OO/168286 -21 \nPP-22-0324  \nVersion 1. 2 \n \n\n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 i \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nNotices and history  \nDocument change history  \nDate  Version  Description  \nAugust  2021  1.0 Initial publication  \nMarch  2022 1.1 Updated guidance based on industry feedback  \nAugust 2022  1.2 Corrected  automountServiceAccountToken (Authentication and \nAuthorization), clarified ClusterRoleBinding (Appendix K)  \nDisclaimer of warranties and endorsement  \nThe information and opinions contained in this document are provided \"as is\" and \nwithout any warranties or  guarantees. Reference herein to any specific commercial \nproducts, process, or service by trade name, trademark, manufacturer, or otherwise, \ndoes not necessarily constitute or imply its endorsement, recommendation, or favoring \nby the United States Governme nt, and this guid e shall not be used for advertising or \nproduct endorsement purposes.  \nTrademark recognition  \nKubernetes is a registered trademark of The Linux Foundation.   SELinux is a registered \ntrademark of the National Security Agency.   AppArmor is a registered trademark of \nSUSE LLC.   Windows and Hyper -V are registered trademarks of Microsoft Corporation.  \n ETCD is a registered trademark of CoreOS, Inc.   Syslog -ng is a registered trademark \nof One Identity Software International Designated Activity Co mpany.   Prometheus is a \nregistered trademark of The Linux Foundation.   Grafana  is a registered trademark of \nRaintank, Inc. dba Grafana Labs   Elasticsearch and ELK S tack are registered \ntrademarks of Elasticsearch B.V.  \nCopyright  recognition  \nInformation, examples, and figures in this document are based on Kubernetes \nDocumentation  by The Kubernetes Authors , published  under a Creative Commons \nAttributi on 4.0 l icense .  \nAcknowledgements  \nNSA and CISA acknowledge the feedback received from numerous partners and the \ncybersecurity community on the previous version of this report, and thank them for their \nhelp in making it better. Changes have been incorporated where appropriate.  \n  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 ii \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nPublication information  \nAuthor(s)   \nNational Security Agency  (NSA)   \nCybersecurity Directorate  \nEndpoint Security  \nCybersecurity and Infrastructure Security Agency (CISA)  \nContact information  \nClient Requirements / General Cybersecurity Inquiries:  \nCybersecurity Requirements Center, 410 -854-4200, Cybersecurity_Requests@nsa.gov  \nMedia inquiries / Press Desk:  \nMedia Relations, 443 -634-0721, MediaRelations@nsa.gov  \nFor incident response resources, contact CISA at CISAServiceDesk@cisa.dhs.gov . \nPurpose  \nNSA and CISA developed this  document in furtherance of their respective  cybersecurity \nmissions , including their  responsibilities to develop and issue cybersecurity \nspecifications and mitigations. This information may be shared broadly to reach all \nappropriate stakeholders.   \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 iii \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nExecutive summary  \nKubernetes is an open -source system that automates the deployment, scaling, and \nmanagement of applications run in containers , and is often hosted in a cloud \nenvironment . Using this type of virtualized infrastructure can provide several flexibility \nand security benefits  compared to traditional, monolithic software platforms.  However, \nsecurely  managing everything from microservices to the underlying infrastructure \nintroduces other complexities . This report is designed to help organizations handle \nKubernetes -associated risks and enjoy the benefits of using this technology.  \nThree common sources of compromise in Kubernetes are  supply chain risk s, malicious \nthreat actors , and i nsider threats . Supply chain risks are often challenging to mitigate \nand can arise in the containe r build cycle or infrastructure acquisition. Malicious threat \nactors can exploit vulnerabilities and misconfigurations in components of  the \nKubernetes architecture , such as the control plane, worker nodes, or containerized \napplications. Insider threats can  be administrators, users, or cloud service providers. \nInsiders with special access to an organizations Kubernetes infrastructure may be able \nto abuse these privileges.  \nThis guide  describes the security challenges associated with setting up and securing a  \nKubernetes cluster. It includes strategies for system administrators and developers of \nNational Security Systems , helping them  avoid common misconfigurations and \nimplement recommended hardening measures and mitigations when deploying \nKubernetes . This guide  details the following mitigations:  \n Scan containers and Pods  for vulnerabilities or misconfigurations . \n Run containers and Pods with the least privileges possible.  \n Use network separation to control the amount of damage a compromise  can \ncause .  \n Use firewalls to limit unneeded network connectivity  and use encryption to \nprotect confidentiality .  \n Use strong authentication and authorization to limit user and administrator \naccess as well as to limit the attack surface.  \n Capture  and monitor  audit  logs so th at administrators can be alerted to potential \nmalicious activity.  \n Periodic ally review all Kubernetes settings and use vulnerability scans to ensure \nrisks are appropriate ly accounted for and security patches are applied . \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 iv \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nFor additional security hardening guidance, see  the Center for Internet Security \nKubernetes benchmarks, the Docker and Kubernetes Security Technical \nImplementation Guides, the Cybersecurity and Infrastructure Security Agency (CISA) \nanalysis report, and Kubernetes documentation  [1], [2], [3], [6]. \n  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 v \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nContents  \nKubernetes Hardening Guide  ................................ ................................ .............................  i \nExecutive summary  ................................ ................................ ................................ .................  iii \nContents  ................................ ................................ ................................ ................................ .... v \nIntroduction  ................................ ................................ ................................ ...............................  1 \nRecommendations  ................................ ................................ ................................ ...................  2 \nArchitectural overview  ................................ ................................ ................................ .............  4 \nThreat model  ................................ ................................ ................................ .............................  6 \nKubernetes Pod securit y ................................ ................................ ................................ ..........  8 \nNon -root containers and rootless container engines  ................................ ...........................  9 \nImmutable container file systems  ................................ ................................ ...........................  10 \nBuilding secure container images  ................................ ................................ ..........................  10 \nPod security enforcement  ................................ ................................ ................................ ...... 12 \nProtecting Pod service account tokens  ................................ ................................ ..................  12 \nHardening container environments  ................................ ................................ ........................  13 \nNetwork separation and hardening  ................................ ................................ .......................  14 \nNamespaces  ................................ ................................ ................................ .........................  14 \nNetwork policies  ................................ ................................ ................................ ....................  15 \nResource policies  ................................ ................................ ................................ ..................  17 \nControl plane hardening  ................................ ................................ ................................ ........  18 \nEtcd ................................ ................................ ................................ ................................ ... 19 \nKubeconfig Files  ................................ ................................ ................................ ................  19 \nWorker node segmentation  ................................ ................................ ................................ .... 19 \nEncryption  ................................ ................................ ................................ .............................  20 \nSecrets  ................................ ................................ ................................ ................................ .. 20 \nProtecting sensitive cloud  infrastructure  ................................ ................................ ................  21 \nAuthentication and authorization  ................................ ................................ ..........................  22 \nAuthentication  ................................ ................................ ................................ ........................  22 \nRole-based access control  ................................ ................................ ................................ .... 23 \nAudit Lo gging and Threat Detection  ................................ ................................ .....................  27 \nLogging  ................................ ................................ ................................ ................................ . 27 \nKubernetes native audit logging configuration  ................................ ................................ .... 29 \nWorker node and container logging  ................................ ................................ ...................  30 \nSeccomp: audit mode  ................................ ................................ ................................ ........  32 \nSyslog ................................ ................................ ................................ ................................  32 \nSIEM platforms  ................................ ................................ ................................ ..................  33 \nService meshes  ................................ ................................ ................................ .................  34 \nFault tolerance  ................................ ................................ ................................ ...................  35 \nThreat Detection  ................................ ................................ ................................ ....................  36 \nAlerting  ................................ ................................ ................................ ..............................  37 \nTools  ................................ ................................ ................................ ................................ ..... 38 \nUpgrading and application security practices  ................................ ................................ ...... 40 \nWorks cited  ................................ ................................ ................................ .............................  41 \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 vi \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nAppendix A: Example Dockerfile for non -root application  ................................ ..................  42 \nAppendix B: Example deployment template for read -only file system  ...............................  43 \nAppendix C: Pod Security Policies (deprecated)  ................................ ................................ .. 44 \nAppendix D: Example Pod Security Policy  ................................ ................................ ...........  46 \nAppendix E: Example namespace  ................................ ................................ .........................  48 \nAppendix F: Example network policy  ................................ ................................ ....................  49 \nAppendix G: Example LimitRange  ................................ ................................ .........................  50 \nAppendix H: Example ResourceQuota  ................................ ................................ ..................  51 \nAppendix I: Example encryption  ................................ ................................ ............................  52 \nAppendix J: Example KMS configuration  ................................ ................................ .............  53 \nAppendix K: Example pod -reader RBAC Role  ................................ ................................ ...... 54 \nAppendix L: Example RB AC RoleBinding and ClusterRoleBinding ................................ .... 55 \nAppendix M: Audit Policy  ................................ ................................ ................................ ....... 57 \nAppendix N: Example Flags to Enable Audit Logging  ................................ .........................  59 \n \nFigures  \nFigure 1: High -level view of Kubernetes cluster components  ................................ ..............  1 \nFigure 2: Kubern etes architecture  ................................ ................................ ..........................  4 \nFigure 3: Example of container supply chain dependencies introducing malicious code \ninto a cluster  ................................ ................................ ................................ ............................  7 \nFigure 4: Pod components with sidecar proxy as logging container  ................................ ... 9 \nFigure 5: A hardened container build workflow  ................................ ................................ ....11 \nFigure 6: Possible Role, ClusterRole, RoleBinding, and ClusterRoleBinding combinations \nto assign access  ................................ ................................ ................................ ..................... 25 \nFigure 7: Cluster leveraging service mesh to integrate logging with network security  .....35 \nTables \nTable I: Control plane ports  ................................ ................................ ................................ ....18 \nTable II: Worker node ports  ................................ ................................ ................................ ....20 \nTable III: Remote logging configuration  ................................ ................................ ................ 31 \nTable IV: Detection recommendations  ................................ ................................ .................. 36 \nTable V: Pod Security Policy components  ................................ ................................ ............ 44 \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 1 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nIntroduction  \nKubernetes, frequently abbreviated K8s  because there are 8 letters between K and S , \nis an open -source container -orchestration system used to automate deploying, scaling, \nand managing containerized applications. As illustrated in the following figure, it \nmanages all elements that make up a cluster, from each microservice in an application \nto entire clusters.  Using containerized applicati ons as microservices provides  more \nflexibility and security benefits  compared to monolithic software platforms , but also can \nintroduce other complexities .  \n \nFigure 1: High -level view of Kubernetes cluster  components  \nThis guide  focuses on  security challenges and suggests  hardening strategies for \nadministrators of National Security Systems  and critical infrastructure . Although this \nguide  is tailored to National Security Systems  and critical infrastructure  organizations , \nNSA and CI SA also encourage administrators of federal and state, local, tribal, and \nterritorial (SLTT) government networks to implement the recommendations in this guide . \nKubernetes clusters can be complex to secure and are often abused in compromises  \n\n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 2 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nthat exploit their misconfigurations. Th is guide offers specific security configurations that \ncan help build more secure Kubernetes clusters.  \nRecommendations  \nA summary of the key  recommendations  from each section  are: \n Kubernetes Pod security  \n Use containers built to run  applications as non -root users . \n Where possible, run containers with immutable file systems . \n Scan container images for possible vulnerabilities or misconfigurations . \n Use a technical control  to enforce a minimum level of security including : \n Preventing privi leged containers . \n Denying container features frequently exploited to breakout, such \nas hostPID, hostIPC, hostNetwork, allowedHostPath . \n Rejecting containers that execute as the root user or allow \nelevation to root . \n Hardening applications against exploitatio n using security services \nsuch as SELinux, AppArmor, and secure computing mode \n(seccomp ). \n Network separation and hardening  \n Lock down access to control plane nodes using a firewal l and role -based \naccess control (RBAC) . Use separate networks for the contro l plane \ncomponents and nodes . \n Further limit access to the Kubernetes etcd server . \n Configure control plane components to use authenticated, encrypted \ncommunications using Transport Layer Security (TLS) certificates . \n Encrypt etcd at rest and use a separate T LS certificate for communication . \n Set up network policies to isolate resources. Pods and services in different \nnamespaces can still communicate with each other unless additional \nseparation is enforced . \n Create  an explicit den y network policy . \n Place all cred entials and sensitive information  encrypted  in Kubernetes \nSecrets rather than in configuration files. Encrypt Secrets using a strong \nencryption method . Secrets are not encrypted by default.  \n Authentication and authorization  \n Disable anonymous login  (enabled by default) . \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 3 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \n Use strong user authentication . \n Create RBAC  policies with unique roles for users, administrators, \ndevelopers, service accounts, and infrastructure team . \n Audit Logging and Threat Detection  \n Enable audit logging (disabled by default) . \n Persist logs to ensure availability in the case of node, Pod, or container -\nlevel failure . \n Configure logging throughout the environment ( e.g., cluster application \nprogram interface (API) audit event logs, cluster metric logs, application \nlogs, P od seccomp lo gs, repository audit logs, etc.). \n Aggregate l ogs external to the cluster . \n Implement a log monitoring and alerting system tailored to the \norganizations cluster . \n Upgrading and application security practices  \n Promptly apply security  patches and updates . \n Perform periodic vulnerability scans and penetration tests . \n Uninstall and delete unused components from the environment . \n  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 4 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nArchitectural overvie w \nKubernetes uses a cluster architecture. A Kubernetes cluster comprises many  control \nplane s and one or more  physical or virtual machines called worker  nodes.  The worker \nnodes host Pods, which contain one or more containers .  \nA container is a runtime environment  contain ing a software package and all its \ndependencies. Container images are standalone collections  of the executable code and \ncontent that are used to populate a container environment  as illustrated in the following \nfigure:  \n \nFigure 2: Kubernetes architecture  \nThe control plane makes decisions about the cluster . This includes scheduling  \ncontainers to run , detecting /responding to failures , and starting new Pods when the \nnumber of replicas listed in a D eployment file is unsatisfied. The following logical \ncomponents are all part of the control plane:  \n Controller manager  Monitors the Kubernetes cluster to detect and maintain \nseveral aspects of the Kubernetes environment including joining Pods to \nservices, maintaining the correct number of Pods in a set, and responding to the \nloss of nodes.  \n\n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 5 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \n Cloud controller manager   An optional component used for cloud -based \ndeployments. The cloud controller interfaces with the cloud service provider \n(CSP) to manage load balancers and virt ual networking for the cluster.  \n Kubernetes application progra mming interface ( API) server   The interface \nthrough which administrators direct Kubernetes. As such, the API server is \ntypically exposed outside of the control plane. It is designed to scale and may \nexist on multiple control plane nodes.  \n Etcd  The persistent backing store where all infor mation regarding the state of \nthe cluster is kept. Etcd is not intended to be manipulated directly but should be \nmanaged through the API server.  \n Scheduler  Tracks the status of worker nodes and determines where to run \nPods. Kube -scheduler is intended to be accessible only from within the control \nplane.   \nKubernetes worker nodes are physical or virtual machines dedicate d to running \ncontainerized applications for the cluster. In addition to running a container engine, \nworker nodes host the following two serv ices that allow orchestration from the control \nplane :  \n Kubelet   Runs on each worker node to orchestrate and verify Pod execution .  \n Kube -proxy   A network proxy that uses the host s packet filtering capability to \nensure correct packet routing in the Kubernetes cluster.  \nClusters are commonly hosted using a CSP Kubernetes service  or an on-premises  \nKubernetes service; CSPs often provide additional features.  They administer most \naspects  of managed Kubernetes services; however, organizations may need to handle \nsome Kubernetes service aspects, such as authentication and authorization , because \ndefault CSP configurations are typically not secure. When designing  a Kubernetes \nenvironment , organizations should understand their responsibilities  in securely \nmaintaining the cluster .  \nReturn to Contents  \n  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 6 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nThreat model  \nKubernetes can be  a valuable target for data or compute  power  theft. While data theft is \ntraditionally the primary motivation, cyber actors  seeking computational power (often for \ncryptocurrency  mining) are also drawn to Kubernetes to harness the underlying  \ninfrastructure. In addition to resource theft, cyber actors  may target Kubernetes to cause \na denial of service. The following threats represent some of the most likely sources of \ncompromise for a Kube rnetes cluster:  \n Supply Chain   Attack vectors to the supply chain are diverse and challenging \nto mitigate. The risk  is that an adversary may subvert any element that makes up \na system . This includes product components, services, or personnel that help \nsupply the end product. Additional supply chain risks can include third -party \nsoftware and vendors used to create and m anage the Kubernetes cluster. Supply \nchain compromises  can affect Kubernete s at multiple levels including:  \n Container/ application level  The security of applications  and their third -\nparty dependencies  running in Kubernetes rely on the trustworthiness of \nthe developers and the defense of the development infrastructure. A \nmalicious container or application from a third party  could  provide cyber \nactors  with a foothold in the cluster.  \n Container runtime  Each node has a container runtime installed to load \nconta iner image s from the repository . It monitors local system resources, \nisolate s system resources for each container, and manage s container \nlifecycle. A vulnerability in the container runtime could lead to insufficient \nseparation between containers.  \n Infrastructure  The underlying systems hosting Kubernetes have their \nown software and hardware dependencies. Any compromise of systems \nused as worker nodes or as part of the control plane could provide cyber \nactors  with a foothold in the cluster.  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 7 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \n \nFigure 3: Example of container supply chain dependencies introducing malicious code  into a cluster  \n Malicious Threat Actor   Malicious  actors often exploit vulnerabilities  or stolen \ncredentials from social engineering  to gain access  from a remote location. \nKubernetes architecture exposes several APIs that cyber actors  could potentially \nleverage for remote exploitation  including:  \n Control plane  The Kubernetes control plane has many  components that \ncommunicate to track and manage the cluster . Cyber actors  frequently \ntake advantage of exposed control plane components lacking appropriate \naccess controls.  \n Worker nodes  In addition to running a container engine, worker nodes \nhost the kubelet  and kube -proxy  service, which are potentially exploitable \nby cyber actors . Additionally, worker nodes exist outside of the locked -\ndown control plane and may be more accessible to cyber actors .  \n Containerized application s  Applications running inside the cluster are \ncommon targets . They are frequently  accessible outside of the cluster , \nmaking them reachable by remote cyber actors . An actor  can then pivot \nfrom an already compromised Pod or escalate privileges within the cluster  \nusing an exposed applications internally ac cessible resources .  \n Insider Threat  Threat actors can exploit vulnerabilities or use privileges given \nto the individual while working within the organization. Individuals from within the \norganization have  special knowledge and privileges that can be used  against \nKubernetes clusters.  \n Administrator  Kubernetes administrators have  control over running  \ncontainers , including executing arbitrary commands inside containerized \nenvironments . Kubernetes -enforced RBAC authorization can reduce the \nrisk by restricting access to sensitive capabilities . However, because \n\n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 8 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nKubernetes lacks two-person integrity controls , at least one  administrative \naccount must be capable of gaining control of the cluster . Administrators \noften have physical access to the systems o r hypervisors, which could \nalso be used to compromise the Kubernetes environment.   \n User  Containerized application users may know and have credentials to \naccess containerized services in the Kubernetes cluster. This level of \naccess could provide sufficien t means to exploit either the application itself \nor other cluster components.  \n Cloud service or infrastructure provider  Access to physical systems or \nhypervisor s managing Kubernetes nodes could be used to compromise a  \nKubernetes environment. CSPs often ha ve layers of technical and \nadministrative controls to protect systems from privileged administrators.  \n \nReturn to Contents  \nKubernetes Pod security  \nPods are the smallest deployable Kubernetes unit and consist of one or more \ncontainers. Pods are often a cyber actor s initial execution environment  upon exploiting \na container . For this reason, Pods should be hardened to make exploitation more \ndifficult and to limit the impa ct of a successful compromise.  The following figure \nillustra tes the components of a Pod and possible attack surface.  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 9 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \n \nFigure 4: Pod components with sidecar proxy as logging container  \nNon-root containers and  rootless container engines  \nBy default, many container services run as the privileged root user, and applications \nexecute inside the container as root despite not requiring privileged execution. \nPreventing root execution by using non -root containers or a rootless container engine  \nlimits the impact of a container compromise . Both methods affect the runtime \nenvironment significantly, so applications should be thoroughly tested to ensure \ncompatibility.  \nNon-root containers   Container engines allow containers to run applications as a \nnon-root user with non -root group membership. Typically, this non -default setting is \nconfigured when the container image is built. For an example Dockerfile that runs an \napplication as a non -root user, r efer to Appendix A : Example Dockerfile for non-\nroot application . Alternatively, Kubernetes can load containers into a  Pod with \nSecurityContext:runAsUser  specifying  a non -zero user. Whi le the runAsUser  \ndirective effectively force s non-root execution at deployment, NSA and CISA \nencourage developers to build container applications to execute as a non -root user. \nHaving non -root execution integrated at build time provides better assurance that \napplications will function correctly without root privileges .  \n\n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 10 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nRootless container engines   Some container engines can run in an unprivileged \ncontext rather than using a daemon running as root . In this scenario, execution \nwould appear to  use the root user  from the containerized applications perspective, \nbut execution is remapped to the engines user con text on the host . While rootless \ncontainer engines add an effective layer of security, many are currently released as \nexperimental and should not be used in a  production environment. Administrators \nshould be aware of this emerging technology and adopt rootless container engines \nwhen vendors release a stable version compatible with Kubernetes.  \nImmutable container file systems  \nBy default, containers are permitted  mostly unrestricted  execution within their own \ncontext. A cyber actor  who has gained execution i n a container can create files, \ndownload scripts, and modify the application  within the container . Kubernetes can lock \ndown a containers file  system , thereby preventing many post -exploitation activities . \nHowever,  these limitations  also affect  legitimate container applications and can \npotentially result in crashes or anomalous behavior . \nTo prevent damaging legitimate applications , Kubernetes administrators can mount \nsecondary read/write file  systems for specific directories where applications require  \nwrite access. For an example immutable container with a writable directory, refer to  \nAppendix B : Example deployment template for read-only filesystem . \nBuilding secure container images  \nContainer images are usually created by either building a contain er from scratch or by \nbuilding on top of an existing image pulled from a repository. Repository controls within \nthe developer environment can be used to restrict developers to using only trusted \nrepositories . Specific controls vary depending on the environ ment but may include both \nplatform -level restrictions, such as admission controls, and network -level restrictions. \nKubernetes admission controllers , third -party tools, and some CSP-native solutions  can \nrestrict entry so that only digitally signed images ca n execute in the cluster.   \nIn addition to using trusted repositories to build containers, image scanning is key to \nensuring deployed containers are secure. Throughout the container build workflow, \nimages should be scanned to identify outdated libraries, kn own vulnerabilities, or \nmisconfigurations, such as insecure ports or permission s. Scanning should also provide \nthe flexibility to disregard false positives  for vulnerability detection where knowledgeable \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 11 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \ncybersecurity professionals have deemed alerts to be inaccurate . As illustrated in the \nfollowing figure, one approach to implementing image scanning is to use an admission \ncontroller. An admission controller  is a Kubernetes -native feature that can intercept and \nprocess requests to the Kubernetes API prior to persistence of the object, but after the \nrequest is authenticated and authorized. A custom or proprietary webhook can be \nimplemented to scan any image be fore it is deployed in the cluster. This admission \ncontroller can block deployments if the image does not comply with the organizations \nsecurity policies defined in the webhook configuration [4]. \n \nFigure 5: A hardened container build workflow  \n  \n\n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 12 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nPod security  enforcement  \nEnforcing security requirements on Pods can be accomplished natively in Kubernetes \nthrough  two mechanisms:  \n1. A beta1 release feature called Pod  Security  Admission   Production Kubernetes \nadministrators should adopt Pod Security Admission , as the feature is enabled by \ndefault in Kubernetes version 1.23. Pod Security Admission is based around \ncategorizing pods as privileged, baseline, and restricted and provides a more \nstraightforward implementation than PSPs. More information about Pod Secu rity \nAdmission is available in the online documentation2. \n2. A deprecated feature called Pod Security Policies (PSPs)   Administrators using \nPSPs while transitioning to Pod Security Admission can use information in \nAppendix C: Pod Security Policies  to enhance  their PSPs.  \nIn addition to native Kubernetes solutions, third -party solutions often implemented as \nKubernetes admission controllers can provide additional fine -grained policy control. \nWhile these solutions are beyond the scope of this document, administra tors may \nexplore the products available for their environment to determine the best solution for \ntheir needs.  \nProtecting Pod service account tok ens \nBy default, Kubernetes automatically provisions a service account when creating a Pod \nand mounts the accoun ts secret token within the Pod at runtime. Many containerized \napplications do not require direct access to the service account as Kubernetes \norchestration occurs transparently in the background. If an application is compromised, \naccount tokens in Pods can  be gleaned by cyber actor s and used to further compromise \nthe cluster. When an application does not need to access the service account  directly , \nKubernetes administrators should ensure that Pod specifications disable the secret \ntoken being mounted. This c an be accomplished using the \nautomountServiceAccountToken: false  directive in the Pods YAML \nspecification . \nIn some cases, containerized applications use provisioned service account tokens to \nauthenticate to external services, such as cloud platforms. In  these cases, it can be \n                                                \n1 Beta releases of software have generally passed some level of quality assurance and contain most planned functionality  \n2 https://kubernetes.io/docs/concepts/security/pod -security -admission/  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 13 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \ninfeasible to disable the account token. Instead, cluster administrators should ensure \nthat RBAC  is implemented to restrict Pod privileges within the cluster. For more \ninformation on RBAC , refer to the  section on  authentication and authorization . \nHardening container environments  \nSome platforms and container engines provide additional options or tools to harden \ncontainerized environments.  For example:   \n Hypervisor -backed containerization   Hypervisors rely on hardware to enforce \nthe vi rtualization boundary rather than the operating system. Hypervisor isolation \nis more secure than traditional container isolation. Container engines running on \nthe Windows operating system can be configured to use the built -in Windows \nhypervisor, Hyper -V, to enhance security. Additionally, some security -focused \ncontainer engines natively deploy each container within a lightweight hypervisor \nfor defense -in-depth. Hypervisor -backed container s mitigate container breakouts.  \n Kernel -based solutions   The seccomp  tool, which is disabled by default, can \nbe used to limit a containers system call abilities, thereby lowering the kernels \nattack surface. Seccomp can be enforced through a previously described Pod \npolicy . For more information on Seccomp , refer to Audit Logging and Threat \nDetection.   \n Application sandboxes   Some container engine solutions offer the option to \nadd a layer of isolation between the containerized application and the host \nkernel. This isolation boundary forces the application to operate within a virtual \nsandbox thereby protecting the host operating system from malicious or \ndestructive operations.  \nReturn to Contents  \n  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 14 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nNetwork separation and h ardening  \nCluster networking is a central concept of Kubernetes. Communication among \ncontainers, Pods, services, and external services must be taken into consideration. By \ndefault,  Kubernetes resources  are n ot isolated and do not prevent lateral movement or \nescalati on if a cluster is compromised. Resource separation and encryption can be an \neffective way to limit a cyber actor s movement and escalation within a cluster.  \n \nNamespaces  \nKubernetes n amespaces are one way to partition cluster resources among  multiple \nindividuals, teams, or applications within  the same  cluster. By default, namespaces are  \nnot automatically isolated . However, namespaces do assign a label to a scope, which \ncan be used to specify authorization rules via RBAC  and networking policies. In addition \nto policies that limit access to resources by namespace, resource policies can  limit \nstorage an d compute resources to provide better control over Pods at the namespace \nlevel.  \nThere are three namespaces by default , and they cannot be deleted:  \n kube -system (for Kubernetes components)  \n kube -public (for public resources)  \n default (for user re sources)  \nUser Pods should not be placed in kube -system or kube -public, as these are reserved \nfor cluster services. A YAML file, shown in Appendix E: Example namespace , can be \nused to create new namespaces. Pods and services in different namespaces can stil l \ncommunicate with each other unless additional separation is enforced .  Key points  \n Use network policies and firewalls to separate and isolate resources.  \n Secure the control plane.  \n Encrypt traffic and sensitive data (such as Secrets) at rest.  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 15 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nNetwork p olicies  \nEvery Pod gets its own  cluster -private  IP address  and can be treated similarly to virtual \nmachines ( VMs) or physical hosts with  regard to port allocation, naming, service \ndiscovery, and load balancing.  Kubernetes can shift Pods to other nodes and recreate \nPods in a Deployment that have died. When that happens, the Pod IP addresses can \nchange , which means  applications should not de pend on the Pod IP being static .  \nA Kubernetes Service is used to solve the issue  of changing IP addresses.  A Service is  \nan abstract way to  assign  a unique IP address to a logical set of Pods  selected using a \nlabel in the Pod configuration.  The address is tied to the lifespan of the Service  and will \nnot change while the Service is alive.  The communication to a Service is automatically \nload-balanced among the Pods that are members of the Service.   \nServices can be exposed externally using NodePorts or LoadBal ancers, and internally. \nTo expose a Service externally, configure the Service  to use TLS certificates to encrypt \ntraffic . Once TLS is configured, Kubernetes supports two ways to expose the Service to \nthe Internet: NodePorts and LoadBalancers.  \nAdding type: NodePort  to the Service specification file will assign a random port to \nbe exposed to the Internet using the clusters public IP address. The NodePort can also \nbe assigned manually if desired. Changing the type to LoadBalancer can be used in \nconjunction wi th an external load balancer. Ingress and egress traffic can be controlled \nwith network policies. Although Services cannot be selected by name in a network \npolicy, the Pods can be selected using the label that is used in the configuration to \nselect the Pods for the Service.  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 16 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nNetwork policies control traffic flow between Pods, \nnamespaces, and external IP addresses. By defaul t, \nno network policies  are applied to P ods or \nnamespaces , resulting in  unrestricted  ingress and \negress traffic  within the Pod network . Pods become \nisolated through a network policy that applies to the \nPod or the Pods namespace. Once a Pod is \nselect ed in a  network policy, it reject s any \nconnections that are not specifically allowed by any \napplicable policy object.  \nTo create network policies, a  container network \ninterface (CNI) plugin  that supports the \nNetworkPolicy API is required. Pods are selected \nusing t he podSelector  and/or the \nnamespaceSelector  options. For an example network policy, refer to Appendix F: \nExample network policy .  \nNetwork policy formatting may differ depending on the CNI plugin used for the cluster. \nAdministrators should use a default policy selecting all Pods to deny all ingress and \negress traffic and ensure any unselected Pods are isolated. Additional policies could \nthen relax these restrictions for permissible connections.  \nExternal IP addresses can be used in ingress and egress polic ies using ipBlock , but \ndifferent CNI plugins, cloud providers, or service implementations may affect the order \nof NetworkPolicy processing and the rewriting of addresses within the cluster.  \nNetwork policies can also be used in conjunction with firewalls an d other external tools \nto create network segmentation. S plitting the network into separate sub -networks or \nsecurity zones helps isolate public -facing applications from sensitive internal resources . \nOne of the major benefits to network segmentation is limit ing the attack surface and \nopportunity for lateral movement. In Kubernetes, network segmentation can be used to \nseparate applications or types of resources to limit the attack surface.  Network Policies Che cklist  \n Use a CNI plugin that supports \nNetworkPolicy API  \n Create policies that select P ods using \npodSelector and/or the \nnamespaceSelector  \n Use a default policy to deny all ingress \nand egress traffic . Ensure s unselected \nPods are isolated to all namespaces \nexcept kube -system  \n Use LimitRange and ResourceQuota \npolicies to limit resources on a \nnamespace or Pod level  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 17 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nResource p olicies  \nLimitRanges, ResourceQuotas, and Process ID Limits re strict resource usage for \nnamespaces, nodes, or Pods. These policies are important to reserve compute and \nstorage space for a resource and avoid resource exhaustion.  \nA LimitRange policy constrains individual resources per Pod or container within a \nparticu lar namespace, e.g., by enforcing maximum compute and storage resources. \nOnly one LimitRange constraint can be created per namespace . For an example YAML \nfile, refer to Appendix G: Example LimitRange . \nUnlike LimitRange policies that apply to each Pod or co ntainer individually, \nResourceQuotas are restrictions placed on the aggregate resource usage for an entire \nnamespace, such as limits placed on total CPU and memory usage. For an example \nResourceQuota policy , refer to Appendix H: Example ResourceQuota . If a user tries \nto create a Pod that  violates a LimitRange or ResourceQuota policy, the Pod creation \nfails. \nProcess IDs (PIDs) are a fundamental resource on nodes  and can be e xhausted  without \nviolating  other resource limits. PID exhaustion prevents host daemons (such as \nkubelet and kube-proxy ) from running. Administrators can use node PID limits to \nreserve a specified number of PIDs for system use and Kubernetes system daemons. \nPod PID limits  are used  to limit the number of processes running on each Pod. Eviction \npolicies can be used to terminate a Pod that is misbehaving and consuming abnormal \nresources. However, e viction policies are calculated  and enforced  periodically and do \nnot enforce the limit.  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 18 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nControl plane hardening  \nThe control plane is the core of Kubernetes and allows \nusers to view containers, schedule new Pods, read \nSecrets , and execute commands in the cluster. Because \nof these sensitive capabilities, the control plane should \nbe highly protected. In addition to secure configurations \nsuch as TLS encryption, RBAC, and a strong \nauthentication method, network separation can help \nprevent unauthorized users from accessing the con trol \nplane. The Kubernetes API server runs on port 6443 , \nwhich should be protected by a firewall to accept only \nexpected traffic. The Kubernetes API serve r should not \nbe exposed to the Internet or an untrusted network. \nNetwork policies can be applied to th e kube -system namespace to limit internet access \nto the kube -system. If a default deny policy is implemented to all namespaces, the \nkube -system namespace must still be able to communicate with other control plane \nsegments and worker nodes.  \nThe following ta ble lists the control plane ports and services:  \nTable I: Control plane ports  \nProtocol  Direction  Port Range  Purpose  \nTCP Inbound  6443  Kubernetes API server  \nTCP Inbound  2379 -2380  etcd server client API  \nTCP Inbound  10250  kubelet API  \nTCP Inbound  10259  kube -scheduler  \nTCP Inbound  1025 7 kube -controller -manager  Steps to secure the control plane  \n1. Set up TLS encryption  \n2. Set up strong authentication \nmethod s \n3. Disable access to internet and  \nunnecessary, or untrusted networks  \n4. Use RBAC policies to restrict \naccess  \n5. Secure the etcd datastore with \nauthentication and RBAC policies  \n6. Protect kubeconfig files from \nunauthorized modifications  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 19 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nEtcd  \nThe etcd backend database stores state information and cluster Secrets . It is a critical \ncontrol plane component, and gaining write access to etcd could give a cyber actor  root \naccess to the entire cluster. The etcd serv er should be configured to trust only \ncertificates assigned to the API server. Etcd can be run on a separate control plane \nnode , allowing a firewall to limit access to only the API servers. This limits the attack \nsurface when the API server  is protected \nwith the clusters authentication meth od \nand RBAC policies to restrict users. \nAdministrators should set up TLS \ncertificates to enforce Hypertext Transfer \nProtocol  Secure  (HTTPS ) communication \nbetween the etcd server and API servers. \nUsing a separate  certificate authority  (CA) \nfor etcd may also  be beneficial, as it trusts \nall certificates issued by the root CA by \ndefault.  \nKubeconfig Files  \nThe kubeconfig  files contain sensitive information about clusters, users, namespaces, \nand authentication mechanisms. Kubectl  uses the configuration files store d in the \n$HOME/.kube  directory on the worker node and control plane local machines . Cyber \nactors  can exploit access to  this configuration directory  to gain access to and modify \nconfigurations or credentials to further compromise the cluster. The configuration files \nshould be protected from unintended changes, and unauthenticated non -root users \nshould be blocked from accessing the files.  \nWorker node s egmentation  \nA worker node can be a virtual or physical machine, depending on the clusters \nimplementation. Because nodes run the microservices and host the web applications for \nthe cluster, they are often the target of exploits. If a node becomes compromis ed, an \nadministrator should proactively limit the attack surface by separating the worker nodes \nfrom other network segments that do not need to communicate with the worker nodes or \nKubernetes services.  The etcd backend database \nis a critical control plane \ncomponent and the most \nimportant piece to secure \nwithin the control plane . \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 20 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nDepending on the network, a  firewall can be used to s eparate internal net work \nsegments from the external -facing worker nodes or the entire Kubernetes service . \nExamples of services that may need to be separated from the possible attack surface of \nthe worker nodes are confidential databases or internal service s that would not need to \nbe internet accessible.  \nThe following table lists the worker node ports and services:  \nTable II: Worker node ports  \nProtocol  Direction  Port Range  Purpose  \nTCP Inbound  10250  kubelet API  \nTCP Inbound  30000 -32767  NodePort Services  \nEncryption  \nAdministrators should configure all traffic in the Kubernetes cluster including between \ncomponents, nodes , and the control plane to use TLS 1.2 or 1.3 encryption. \nEncryption can be set up during installation or afterward usin g TLS bootstrapping , \ndetailed in the Kubernetes documentation , to cre ate and distribute certificates to nodes. \nFor all methods, certificates must be distributed among nodes to communicate securely.  \nSecrets  \nKubernetes Secrets maintain sensitive information, such as passwords, OAuth tokens, \nand Secure S hell ( SSH) keys. Storing sensitive information in Secrets provides greater \naccess control than storing passwords or token s in YAML files, container images, or \nenvironment variables. By default, Kubernetes stores Secrets as unencrypted base64 -\nencoded strings that can be retrieved by anyone with API access.  Access can be \nrestricted by applying RBAC policies to the secrets  resource.  \nSecrets can be encrypted by configuring data -\nat-rest encryption on the API server or  by \nusing an external key management service \n(KMS), which may be available through a \ncloud provider. To enable Secret data -at-rest \nencryption using the API server, \nadministrators should change the kube-\napiserver  manifest file to execute using the  \n--encryption -provider -config  \nargument.  By default, Secrets are \nstored as unencrypted \nbase64 -encoded strings and \ncan be retrieved by anyone \nwith API access.  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 21 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nFor an example encryption -provider -config  file, refer to  Appendix I: Example \nencryption . Using a KMS provider prevent s the raw encryption key from being stored \non the local disk. To encrypt Secrets with a KMS provider, the encryption -\nprovider -config  file should s pecify the KMS provider . For an example, refer to  \nAppendix J: Example KMS configuration .  \nAfter applying the encryption -provider -config  file, administrators should run the \nfollowing command to read and encrypt all Secrets:  \nkubectl get secrets --all-namespaces -o json | kubectl replace -f - \nProtecting sensitive cloud infrastructure  \nKubernetes is often deployed on VMs in a cloud environment. As such, administrators \nshould carefully consider the attack surface of the VMs on which the Kubernetes worker \nnodes a re running. In many cases, Pods running on these VMs have access to \nsensitive cloud metadata services on a non -routable address. These metadata services \nprovide cyber actors  with information about the cloud infrastructure and possibly even \nshort -lived credentials for cloud resources.  \nCyber actors  abuse these metadata services for privilege escalation [5]. Kubernetes \nadministrators should prevent Pods from accessing cloud meta data services by using \nnetwork policies or through the cloud configuration policy. Because these services vary \nbased on the cloud provider, administrators should follow vendor guidance to harden \nthese access vectors.  \nReturn to Contents  \n  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 22 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nAuthentication and a uthorization  \nAuthentication and authorization are the primary mechanisms to restrict access to \ncluster resources. Cyber actors  can scan for well -known Kubernetes ports and access \nthe clusters database or make API calls wi thout being authenticated if the cluster is \nmisconfigured. Several u ser authentication mechanisms are supported  but not enabled \nby default . \nAuthentication  \nKubernetes clusters have two types of users :  \n Service accounts  \n Normal user accounts  \nService accounts handle API requests on behalf of Pods. Authentication is typically \nmanaged automatically by Kubernetes through the ServiceAccount Admission \nController  using bearer tokens. When the admission controller is active, it checks \nwhether Pods have an attached ser vice account. If the Pod definition does not specify  a \nservice account , the admission controller attaches the default service account for the \nnamespace. The admission controller will not attach the default service account if the \nPod definition prohibits  the addition of the  service token  by setting \nautomountServiceAccountToken  or automountServiceAccounttoken  to \nfalse . Service accounts can also be individually created  to grant specific permissions.  \nWhen Kubernetes creates the service account, it creates a ser vice account Secret and \nautomatically modifies the Pod to use the Secret. The service account token Secret \ncontains credentials for accessing the API. If left unsecured or unencrypted, service \naccount tokens could be used from outside of the cluster by att ackers. Because of this  \nrisk, access to Pod Secrets should be restricted to those with a need to view them , \nusing Kubernetes RBAC.  \nFor normal users and admin accounts, there is no t an automatic authentication method . \nAdministrators must implement an authen tication method or delegate  authentication to a \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 23 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nthird-party service.  Kubernetes assumes that a cluster -independent service manages \nuser authentication. The Kubernetes documentation  lists several ways to implement \nuser authentication including X509 client certificates, bootstrap tokens, and OpenID \ntokens. At least one user aut hentication method should be implemented. When multiple \nauthentication methods are implemented, the first module to successfully authenticate \nthe request short -circuits the evaluation. \nAdministrator s should not use weak \nmethods , such as static password fil es, as \nweak  methods could allow cyber actors to \nauthenticate as legitimate users.   \nAnonymous requests are requests that are \nnot rejected by other configured \nauthentication methods and are not tied to \nany individual user or Pod. In a server setup \nfor token authentication with anonymous requests enabled, a request without a token \npresent would be performed as an anonymous request. In Kubernetes 1.6 and newer, \nanonymous requests are enabled by default.  When RBAC is enabled, anonymous \nrequests  require explicit authorization of the system:anonymous  user or \nsystem:unauthenticated  group.  Anonymous requests should be disabled by \npassing the --anonymous -auth=false  option to the API server.  Leaving anonymous \nrequests enabled could allow a cyber actor  to access cluster  resources without \nauthentication.  \nRole -based access c ontrol  \nRBAC , enabled by default,  is one method to control access to cluster resources based \non the roles of individuals within an organization.  RBAC can be used to restrict access \nfor user accounts and service accounts.  To check if RBAC is enabled in a cluster using \nkubectl, execute kubectl api -version . The API version for \n.rbac.authorization.k8s.io/v1  should be listed if RBAC is enabled. Cloud \nKubernetes services may have a different way of checking whether RBAC is enabled for \nthe cluster. If RBAC is not enabled, start the API server with the --authorization -\nmode  flag in the following command:  \nkube-apiserver --authorization -mode=RBAC  Kubernetes assumes that a \ncluster -independent service \nmanages user \nauthentication.  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 24 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nLeaving auth orization -mode flags , such as AlwaysAllow , in place allow s all \nauthorization requests, effectively disabling all authorization and limiting the ability to \nenforce least privilege for access.  \nTwo types of permissions can be set :  \n Roles   Set permiss ions for particular namespaces  \n ClusterRoles  Set permissions across all cluster res ources regardless of \nnamespace  \nBoth Roles and ClusterRoles can only be used to add permissions. There are no deny \nrules. If a cluster is configured to use RBAC and anonymous access is disabled, the \nKubernetes  API server will deny  permission s not explicitly allowed.  For an example \nRBAC Role , refer to Appendix K: Example pod-reader RBAC Role . \nA Role or ClusterRole defines a permission but does not tie the permission to a user. As \nillustrated in the following figure, RoleBindings and Cluster RoleBindings are used to tie \na Role or ClusterRole to a  user, group , or service account. RoleBindings grant \npermissions in Roles or ClusterRoles to users, groups, or service accounts in a defined \nnamespace. ClusterRole s are created independent of namespaces and can be used \nmultiple times in conjunction with  a RoleBinding to limit the namespace scope .  \nThis is useful when users, groups, or service accounts require similar permissions in \nmultiple namespaces. One ClusterRole can be used several times with different \nRoleBindings to limit scope to different individu al users, groups, or service accounts.  \nCluster RoleBindings grant users, groups, or service accounts ClusterRoles across all \ncluster resources.  For an example of RBAC RoleBinding and ClusterRoleBinding, refer \nto Appendix L: Example RBAC RoleBinding and Clus terRoleBinding . \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 25 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \n \nFigure 6: Possible Role, ClusterRole, RoleBinding, and ClusterRoleBinding combinations to assign access  \nTo create or update Roles and ClusterRoles, a user must have the permissions \ncontained in the new role at the same scope or possess explicit permission to perform \nthe escalate verb on the Roles or ClusterRoles resources in the \nrbac.authorization.k8s.io  API group. After a binding  is created, the Role or \nClusterRole  is immutable. The binding must be deleted to change a role.  \nPrivileges assigned to users, groups, and service accounts should follow the principle of \nleast privilege , allowing only required permis sions to complete tasks . User groups can \nmake creating Roles easier to manage. Unique permissions are required for different \n\n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 26 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \ngroups , such as users, administrators, developers, and the infrastructure team. Each \ngroup needs access to different resources and should not have permissions to edit or \nview other groups resources. Users, user groups, and service accounts should be \nlimited to interact and view specific namespaces where required resources reside. \nAccess to the Kubernetes API is limited by creating an  RBAC Role or ClusterRole with \nthe appropriate API request verb and desired resource on which the action can be \napplied.  Tools exist that can help audit RBAC policies by printing users, groups, and \nservice account s with their associated assigned Roles and ClusterRoles . \nReturn to Contents  \n  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 27 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nAudit Logging and Threat Detection  \nAudit l ogs capture attributed activity  in the cluster. An effective logging solution and log \nreviewing are necessary, not only for ensuring that services are operating and \nconfigured as intended, but also for ensuring the security of the system. Systematic \nsecurity audit requirements mandate consistent and thorough checks of security \nsettings to help identify compromises.  Kubernetes is capable of capturing audit logs for \ntracking attributed cluster actions , and monitoring basic CPU and memory usage \ninformation;  however, it does not natively provide full featured  monitoring or alerting \nservices.  \nLogging  \nSystem administrators running applications within Kubernetes should establish an \neffective logging and monitoring system for their environment. Logging Kubernetes \nevents alone is not enough to provide a fu ll picture of the actions occurring on the \nsystem. Logging should be performed at all levels of the environment , including on the \nhost, application, container, container engine, image registry, api-server, and the cloud , \nas applicable. Once captured, t hese  logs should all be aggregated to a single service  to \nprovide security auditors, network def enders, and incident responders  a full view of the \nactions taken throughout the environment.  \nWithin the Kubernetes environment, some events that administrators should monitor/log \ninclude the following:  \n API request history  \n Performance metrics  \n Deployments  \n Resource consumption  Key points  \n Establish Pod baselines at creation to enable anomalous activity identification.  \n Perform logging at all levels of the environment.  \n Integrate existing network security tools for aggregate scans, monitoring, alert s, \nand analysis . \n Set up fault -tolerant policies to prevent log loss in case of a failure.  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 28 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \n Operating system calls  \n Protocols, permission changes  \n Network traffic  \n Pod scaling  \n Volume mount actions  \n Image and container modif ication  \n Privilege changes  \n Scheduled job  (cronjob) creations and modifications  \nWhen administrators create or update a Pod, they should capture detailed logs of the \nnetwork communications, response times, requests, resource consumption, and any \nother relevant metrics to establish a baseline. RBAC policy configurations should also \nbe reviewed periodically and whenever personnel chang es occur in the organizations \nsystem administrators . Doing so  ensure s access controls remain  in compliance with the \nRBAC policy -hardening guidance outlined in the r ole-based access control section  of \nthis guide .  \nRoutine system security a udits should incl ude comparisons of current logs to the \nbaseline measurements of normal activities to identify significant changes in any of the \nlogged metrics and events. System administrators should investigate significant \nchanges  to determine the root cause. For example , a significant increase in resource \nconsumption  could be indicative of a change in application usage or the installation of \nmalicious processes  such as a cryptominer.  \nAudits of internal and external traffic logs should be conducted to ensure all intended \nsecurity constraints on connections have been configured properly and are working as \nintended. Administrators can also use these audits as systems evolve to evaluate where \nexternal access may be restricted.  \nStreaming logs  to an external logging service will help  to ensure availability to security \nprofessionals outside of the cluster, enabling them to  identify abnormalities in as close \nto real time as possible. If using this method, logs should be encrypted during transit \nwith TLS 1.2 or 1.3 to ensure cyber actors  cannot access the logs in transit and gain \nvaluable information about the environment.  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 29 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nAnother precaution to take when utilizing an external log server is to configure the log \nforwarder within Kubernetes with append -only access to the external stor age. This \nprotect s the externally stored logs from being deleted or overwritten from within the \ncluster.  \nKubernetes native audit logging configuration  \nThe kube-apiserver  resides on the Kube rnetes control plane and acts as the front \nend, handling internal and external requests for a cluster. Each request, whether \ngenerated by a user, an application, or the control plane, \nproduces an audit event at each stage in its execution. When \nan audit ev ent registers, the kube-apiserver  check s for an \naudit  policy  file and applicable rule. If such a rule exists, the \nserver log s the event at the level defined by the first matched \nrule. Kubernetes built -in audit  logging capabilities perform no \nlogging  by de fault.  \nCluster administrators must write an audit policy YAML file to establish the rules and \nspecify the desired audit level at which to log each type of audit event. This audit policy \nfile is then passed to the kube-apiserver  with the appropriate flags.  For a rule to be \nconsidered valid, it must specify one of the four audit levels:  \n None  \n Metadata  \n Request  \n RequestResponse  \nLogging all events at the RequestResponse  level will give administrators  the \nmaximum amount of information available for incident responders should a breach \noccur. However, this may  result in capturing  base64 -encoded  Secrets in the logs. NSA \nand CISA recommend reducing  the loggi ng level of requests involving S ecrets to the \nMetadata  level to avoid capturing S ecrets in logs .  \nAdditionally, logging all other events at the highest level will produce a  large quantity of \nlogs, especially in a production cluster. If an organizations constraints require it , the \naudit policy can be tai lored to the environment, reducing the logging level of non -critical, \nroutine events. The specific rules necessary for such an audit policy will vary by \ndeployment. It is vital to log all security -critical events, paying close attention to the Kubernetes audit  \nlogg ing capabilities \nare disabled by \ndefault  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 30 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \norganization s specific cluster configuration and threat model to indicate  where to focus \nlogging. The goal of refining an audit policy should be to remove redundancy, while still \nproviding a clear picture and attribution of the events occurring in the cluster.   \nFor some examples of general critical and non -critical audit event types and stages, as \nwell as an example  of an audit policy file that logs Secrets at the metadata  level, and \nall other events at the RequestResponse  level, refer to Appendix M: Audit Policy   \nFor an example where the kube-apiserver  configuration file is located and an \nexample of the flags by which the audit policy file can be passed to the kube-\napiserver , refer to Appendix N: Example Flags to Enable Audit Logging . For \ndirections on how to mount  the volumes and configure the host path, if necessary, refer \nto Appendix N: Example Flags to Enable Audit Logging . \nThe kube-apiserver  includes configurable logging and webhook backends for audit \nlogging. The logging backend writes the audit events specified to a log file, and the \nwebhook backend can be configured to send the file to an external HTTP API. The  \n--audit-log-path  and --audit-log-maxage  flags , set in the example in \nAppendix N: Example Flags to Enable Audit Logging , are two examples of the flags \nthat can be used to configure the log ging backend, which writes audit events to a file. \nThe log-path  flag is the minimum configuratio n required to enable logging and the \nonly configuration necessary for the logging backend. The default format for these log \nfiles is Java Script Object Notation ( JSON ), though this can also be changed if \nnecessary. Additional configuration options for the logging backend can be found in the \nKubernetes documentation . Kubernetes also provides a webhook backend option that \nadministrators can manually configure via a YAML file sub mitted to the kube-\napiserver  to push logs to an external backend . An exhaustive list of the configuration \noptions, which can be set in the kube-apiserver  for the webhook backend, can be \nfound in the Kubernetes documentation . Further details on how the webhook backend \nworks and how to set it up can also be found in the Kubernetes documentation . There \nare also many external tools available to perform log aggregation, some of which are \ndiscussed briefly in the following sections.  \nWorker node and container logging  \nThere are many w ays for logging capabilities to be configured within a Kubernetes \narchitecture. In the built -in method of log management, the kubelet  on each node is \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 31 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nresponsible for managing logs. It stores and rotates log files locally based on its policies \nfor individua l file length, storage duration, and storage capacity. These logs are \ncontrolled by the kubelet  and can be accessed from the command line. The following \ncommand print s the logs of a container within a Pod:  \nkubectl logs [ -f] [-p] POD [ -c CONTAINER]  \nThe -f flag may be used if the logs are to be streamed, the -p flag may be used if logs \nfrom previous instances of a container exist and are desired, and the -c flag can be \nused to specify a container if there are more than one in the Pod.  If an error occurs that \ncauses a container, Pod, or node to die, the native logging solution in Kubernetes does \nnot provide a method to preserve logs stored in the failed object. NSA and CISA \nrecommend configuring a  remote logging solution to preserve logs should a node fail. \nOptions for remote logging include:  \nTable III: Remote logging configuration  \nRemote logging option  Reason to use  Configuration implementation  \nRun a logging agent on \nevery node to push logs to \na backend  Gives the node the ability to \nexpose logs or push logs to a \nbackend, preserving them \noutside of the node in the \ncase of a failure . Configure an independent container in a \nPod to run as a logging agent, giving it \naccess to the nodes application log \nfiles and configuring it to forward l ogs to \nthe organizations SIEM . \nUse a sidecar container in \neach Pod to push logs to \nan output stream  Used to push logs to separate \noutput streams. This can be a \nuseful option when \napplication containers write \nmultiple log files of different \nformats.  Confi gure a sidecar container for each \nlog type and use them  to redirect these \nlog files to their individual output \nstreams, where the kubelet  can \nmanage  them. The node -level logging \nagent can then forward these logs onto \nthe SIEM or other backend . \nUse a loggi ng agent \nsidecar in each Pod to push \nlogs to a backend  When more flexibility is \nneeded than the node -level \nlogging agent can provide . Configure for each Pod to push logs \ndirectly to the backend. This is a \ncommon method for attaching third -\nparty logging age nts and backends.  \nPush logs directly to a \nbackend from within an \napplication  Allows logs to go directly to \nthe aggregation platform. Can \nbe useful if the organization \nhas separate teams \nresponsible for managing \napplication security vs the \nKubernetes platform security.  Kubernetes does not have built -in \nmechanisms for exposing or pushing \nlogs to a backend directly . \nOrganizations will need to either build \nthis functionality into their application or \nattach a reputable third -party tool to \nenable this.  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 32 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nTo ensure continuity of logging agents across worker nodes , it is common to run them \nas a DaemonSet. Configuring a DaemonSet for this method ensures that there is a copy \nof the logging agent on every node at all times and that any changes made to the \nlogging agent are consistent across the cluster.  \nLarge organizat ions with multiple teams  running their own Kubernetes clusters should \nestablish logging requirements and a standard architecture to ensure that all teams \nhave an effective solution in place.  \nSeccomp: audit m ode \nIn addition to the node and container loggin g previously described , it can be highly \nbeneficial to log system calls. One method for auditing container system calls in \nKubernetes is to use the seccomp tool. This tool is disabled by default but can be used \nto limit a containers system call abilities,  thereby lowering the kernels attack surface. \nSeccomp can also log what calls are being made by using an audit profile.  \nA custom seccomp profile define s which system calls are allowed , denied, or logged,  \nand default actions for calls not specified. To ena ble a custom seccomp profile within a \nPod, Kubernetes admins can write their seccomp profile JSON file to the \n/var/lib/kubelet/seccomp/  directory and add a seccompProfile  to the Pods \nsecurityContext .  \nA custom seccompProfile  should also include two fields : Type: Localhost  and \nlocalhostProfile: myseccomppolicy.json . Logging all system calls can help \nadministrators know what system calls are needed for standard operations allowing \nthem to restrict the seccomp profile  further without losing system functionali ty. It can \nalso help administrators establish a baseline for a Pods standard operation patterns, \nallowing them to identify any major deviances from this pattern that could be indicative \nof malicious activity.  \nSyslog  \nKubernetes , by default , writes kubelet  logs and container runtime logs to journald if \nthe service is available. If organizations wish to utilize syslog utilities or to collect logs \nfrom across the cluster and forward them to a syslog server or other log storage and \naggregation platform they can  configure that capability manually.  Syslog protocol \ndefines a log message -formatting standard. Syslog messages include a header  and a \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 33 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nmessage written in plaintext. Syslog daemons such as syslog -ng and rsyslog are \ncapable of collecting and aggregating log s from across a system in a unified format. \nMany Linux operating systems by default use rsyslog or journald an event logging \ndaemon  that optimizes log storage and output logs in syslog format via journalctl. The \nsyslog utility  logs events , on nodes running certain Linux distributions by default at the \nhost level.  \nContainers running these Linux distributions will , by default , collect logs using syslog as \nwell. Syslog utilities store logs in the local file system on each applicable node or \ncontainer unless a log aggregation platform is configured to collect them.  The syslog \ndaemon or another such tool should be configured to aggregate both these and all other \nlogs being collected across the cluster and forward them to an external backend for  \nstorage and monitoring.  \nSIEM  platforms  \nSecurity information and event management (SIEM) software collects logs from across \nan organizations network . It bring s together firewall logs, application logs, and more , \nparsing them out to provide a centralized p latform from which analysts can monitor \nsystem security. SIEM tools have variations in their capabilities. Generally, these \nplatforms provide log collection, aggregation , threat detection, and alerting capabilities. \nSome include machine -learning capabiliti es, which can better  predict system behavior \nand help to reduce false alerts.  Organizations using these platforms in their environment \nshould integrate them with Kubernetes to better monitor and secure clusters.  Open -\nsource platforms for managing logs from  a Kubernetes environment exist as alternative s \nto SIEM platforms.  \nContainerized environments have many interdependencies between nodes, Pods, \ncontainers, and services. In these environments, Pods and containers are constantly \nbeing deleted  and re deployed  on different nodes. This type of environment presents a n \nextra  challenge for traditional SIEMs , which typically use IP addresses to correlate logs. \nEven next-generation  SIEM platforms may not always be suited to the complex \nKubernetes environment. However , as Kubernetes has emerged as the most widely \nused container orchestration platform, many of the organizations developing SIEM tools  \nhave developed variations of their products specifically designed to work with the \nKubernetes environment , providing full monitoring solutions for these containerized \nenvironments.  Administrators should be aware of their platforms capabilities  and \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 34 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nensure that their logging sufficiently captures the environment to support future incident \nresponses.  \nService meshes  \nService meshes are platforms that streamline micro -service communications within an \napplication by allowing for the logic of these communications to be coded into the \nservice mesh rather than within each micro -service. Coding this communication logic \ninto individu al micro -services is difficult to scale, difficult to debug as failures occur, and \ndifficult to secure. Using a service mesh can simplify this  coding  for developers. Log \ncollection at this level can also give cluster administrators insight into the standar d \nservice -to-service communication flow throughout the cluster. The mesh can :  \n Redirect traffic when a service is down , \n Gather performance metrics  for optimizing communications , \n Allow management of service -to-service communication encryption , \n Collect logs for service -to-service communication ,  \n Collect logs from each service ,  \n Help developers diagnose problems and failures of micro -servic es or \ncommunication mechanisms , and  \n Help with migrating services to hybrid or multi -cloud environments.  \nWhile service meshes are not necessary, they are an option that is highly suitable to the \nKubernetes environment. Their  logging capabilities can also be useful in mapping the \nservice -to-service communications, helping administrators see what their standard  \ncluster operation looks like and identify anomalies easier. Managed Kubernetes \nservices often include their own service mesh; however , several other platforms are \nalso available and , if desired, are highly customizable.  \nAnother major benefit of modern se rvice meshes is encryption of service -to-service \ncommunications. Many service meshes  manage keys and  generate and rotate \ncertificates, allowing for secure TLS authentication between services , without requiring \ndevelopers to set this up for each individual service and manage it themselves . Some \nservice meshes even perform this service -to-service encryption by default. If \nadministrators deploy a service mesh within a Kubernetes cluster, it is important to keep \nup with updates and security alerts for the servi ce mesh  as illustrated in the following \nfigure:  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 35 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \n \nFigure 7: Cluster leveraging service mesh to integrate logging with network security  \nFault tolerance  \nOrganizations should put f ault tolerance policies in place. These policies coul d differ \ndepending on the specific Kubernetes use case. One such policy is to allow new logs to \noverwrite the oldest log files , if absolutely necessary , in the event of storage capacity \nbeing exceeded. Another such policy that can be used i f logs are being  sent to an \nexternal service is to establish a place for logs to be stored locally if a communication \nloss or an external service failure occurs. Once communication to the external service is \nrestored, a policy should be in place for the locally stored log s to be pushed up to the \nexternal server.  \n\n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 36 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nThreat Detection  \nAn effective logging solution comprises two critical components: collecting all necessary \ndata and then actively monitoring the collected data for red flags in as close to real time \nas possible. The best logging solution in the world is useless if the data is never \nexamined. Much of the process of  log examination can be automated ; however, when \neither writing log parsing policies or manually examining logs, it is  vital to know what to \nlook for. When attackers try to exploit the cluster, they will leave traces of their actions in \nthe logs.  \nThe following table contains some of the ways attackers may try to exploit the cluster \nand how that may present in the logs. ( Caveat:  This table lists some  known suspicious \nindicators. Administrators should also be aware of, and alert to, specific concerns and \nemerging threats in their environments. The most effective alerts are tailored to identify \nabnormal activity for a specif ic cluster. ) \nTable IV: Detection recommendations  \nAttacker Action  Log Detection  \nAttackers may try to deploy a Pod or container to \nrun their own malicious software or to use as a \nstaging ground/pivot point for their attack. \nAttackers may try to masquerade their deployment \nas a legitimate image by copying names and \nnaming conventions. They may also try to start a \ncontainer with root privileges to escalate privileges.  Watch for atypical Pod and container deployments. \nUse image IDs and la yer hashes for comparisons \nof suspected image deployments against the valid \nimages. Watch for Pods or application containers \nbeing started with root permissions  \nAttackers may try to import a malicious image into \nthe victim organizations registry, either to give \nthemselves access to their image for deployment, \nor to trick legitimate parties into deploying their \nmalicious image instead of the legitimate ones.  This may be detectable in the container engine or \nimage repository logs. Network defenders should \ninvestigate a ny variations from the standard \ndeployment process. Depending on the specific \ncase this may also be detectible through changes \nin containers behavior after being redeployed \nusing the new image version.  \nIf an attacker manages to exploit an app lication to \nthe point of gaining command execution \ncapabilities on the container , then depending on \nthe configuration of the Pod, they may be able to \nmake API requests from within the Pod, potentially Unusual API requests (from the Kubernetes audit \nlogs) or unusual system calls (from seccomp logs) \noriginating from inside a Pod. This could also show \nas pod creation requests reg istering a Pod IP \naddress as it s source IP . \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 37 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nAttacker Action  Log Detection  \nescalating privileges, moving laterally within the \ncluster, or breaking out onto the host.  \nAttackers who have gained initial access to a \nKubernetes cluster will likely start attempting to \npenetrate further into the cluster , which will require \ninteracting with the kube -apiserver . While they work to determine what initial \npermissions they have,  they may end up making \nseveral failed requests to the API server. Repeated \nfailed API requests and request  patterns that are \natypical  for a given account would be red flag s. \nAttackers may attempt to compromise a cluster in \norder to use the victims resou rces to run their own \ncryptominer (i.e., a cryptojacking attack) . If an attacker were to successfully start a \ncryptojacking attack it would likely show in the logs \nas a sudden spike in resource consumption.  \nAttackers may attempt to use anonymous accounts \nto avoid attribution of their activities in the cluster.  Watch for any anonymous actions in the cluster.  \nAttackers may try to add a volume mount to a \ncontainer they have compromised or are creating , \nto gain access to the host  Volume mount actions should be closely monitored  \nfor abnormalities.  \nAttackers with the abilit y to create scheduled jobs \n(aka Kubernetes CronJobs) may attempt to use \nthis to get Kubernetes to automatically and \nrepetitively run malware on the cluster  [8]. Scheduled job creations and m odifications should \nbe closely monitored . \nThe enormous quantity of logs generated in an environment such as this makes it \ninfeasible for administrators to review all of the logs  manually  and even more important \nfor administrators to know what indicators t o look for. This knowledge can be used to \nconfigure auto mated responses  and refine the criteria for triggering alerts.  \nAlerting  \nKubernetes does not natively support alerting; however, several monitoring tools with \nalerting capabilities are compatible with Kubernetes. If Kubernetes administrators \nchoose to configure an alerting tool to work within a Kubernetes environment, \nadministrators can use  several metrics to monitor and configure alerts.  \nExamples of actionable events that could trigger alerts include but are not limited to:  \n Low disk space on any of the machines in the environment,  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 38 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \n Available storage space on a logging volume running low,  \n External logging service going offline,  \n A Pod or application running with root permissions,  \n Requests being made by an  account for resources they do not have permission \nfor, \n Anonymous requests being submitted to the API server , \n Pod or Worker Node IP addresses being listed as the source ID of a Pod creation \nrequest,  \n Unusual system calls or failed API calls,  \n User/admin behavior that is abnormal (i.e. at unusual times or from an unusual \nlocation), and  \n Significant deviations from the standard operation metrics baseline.  \nIn their 2021 Kubernetes blog post, contributors to the Kubernetes project made the \nfollowing three addi tions to this list [ 7]: \n Changes to a Pods securityContext , \n Updates to admission controller configs , and  \n Accessing certain sensitive files/URLs . \nWhere possible, systems should be configured to take steps to mitigate compromises \nwhile administrators respond  to alerts. In the case of a Pod IP being listed as the source \nID of a Pod creation request, automatically evicting the Pod is one mitigation that could \nbe implemented to keep the application available but temporarily stop any compromises \nof the cluster. D oing so would allow a clean version of the Pod to be rescheduled onto \none of the nodes. Investigators could  examine the logs to determine if a breach \noccurred and, if so, how the malicious actors executed the compromise so that a patch \ncan be deployed. Aut omating such responses can help improve security professionals  \nresponse time to critical events.  \nTools  \nKubernetes does not natively include extensive auditing capabilities. However, the \nsystem  is built to be extensible, allowing users the freedom to deve lop their own custom \nsolution or to choose an existing add -on that suits their needs. Kubernetes cluster \nadministrators common ly connect  additional backend services  to their cluster  to perform \nadditional functions for users , such as extended search paramet ers, data mapping \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 39 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nfeatures, and alerting functionality. Organizations that already use SIEM platforms can \nintegrate Kubernetes with these existing capabilities. Open -source monitoring tools \nsuch as the Cloud Native Computing Foundations Prometheus, Grafa na Labs \nGrafana, and Elasticsearchs Elastic Stack (ELK)are also available . The tools can \nconduct event monitoring, run threat analytics, manage alerting, and collect resource \nisolation parameters, historical usage, and network statistics on running co ntainers. \nScanning tools can  be used when auditing the access control and permission \nconfigurations to identify risky permission configurations in RBAC . \nNSA and CISA encourage  organizations utilizing Intrusion D etection Systems  (IDSs)  on \ntheir existing environment to consider integrating that service into their Kubernetes \nenvironment as well . This integration  would allow an organization to monitor for and \npotentially kill containers showing signs ofunusual behavior so the containers can be \nrestarted from the initial clean image. Many CSPs also provide container monitoring \nservices for those wanting more managed and scalable solutions.  \nReturn to Contents  \n  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 40 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nUpgrading and application security practices  \nFollow ing the hardening guidance outlined in this document is a step toward ensuring \nthe security of applications running on Kubernetes orchestrated containers. However, \nsecurity is an ongoing process , and it is vital to keep up with patches, updates, and \nupgrad es. The specific software components vary depending on the individual \nconfiguration, but each piece of the overall system must be kept as secure as possible. \nThis includes updating Kubernetes, hypervisors, virtualization software, plugins, \noperating system s on which the environment is running, applications running on the \nservers, all elements of the organizations continuous integration/continuous delivery \n(CI/CD) pipeline and any other software hosted in the environment.  Companies who \nneed to maintain 24/7  uptime for their services can consider using high -availability \nclusters, so that services can be off -loaded from physical machines one at a time, \nallowing for firmware, kernel, and operating system updates to be deployed in a timely \nmanner while still mai ntaining service availability.  \nThe Center for Internet Security (CIS) publishes benchmarks for securing software. \nAdministrators should adhere to the CIS benchmarks for Kubernetes and any other \nrelevant system components. Administrators should periodicall y check to ensure their \nsystem 's security  is compliant with the current cyber security best practices. Periodic \nvulnerability scans and penetration tests should be performed on the various system \ncomponents to proactively look for insecure configurations an d zero-day vulnerabilities. \nAny discoveries should be promptly remediated  before potential cyber actors  can \ndiscover and exploit them.  \nAs administrators deploy updates, they should also keep up with uninstalling any old , \nunused components from the environ ment  and deployment pipeline. This practice will \nhelp reduce the attack surface and the risk of unused tools remaining on the system \nand falling out of date . Using a managed Kubernetes service can help to automate \nupgrades and patches for Kubernetes, opera ting systems, and networking protocols. \nHowever, administrators  must still ensure that t heir deployments are up to date and \ndevelopers properly tag new images to avoid accidental deployments of outdated \nimages.  \nReturn to Cont ents   \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 41 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nWorks cited  \n \n \n \n  [1]  Center for Internet Security, \"CIS Benchmarks Securing Kubernetes,\" 2021. [Online]. \nAvailable: https://cisecurity.org/benchmark/kubernetes/.  \n[2]  DISA, \"Kubernetes STIG,\" 2021. [Online]. Available: \nhttps://public.cyber.mil/stigs/downloads/.  \n[3]  The Linux Foundation, \"Kubernetes Documnetation,\" 2021. [Online]. Available: \nhttps://kubernetes.io/docs/ . [Accessed 02 2021].  \n[4]  The Linux Foundation, \"11 Ways (Not) to Get Hacked,\" 18 07 2018. [Online].  Available: \nhttps://kubernetes.io/blog/2018/07/18/11 -ways -not-to-get-hacked/#10 -scan -images -and-run-\nids. [Accessed 03 2021].  \n[5]  MITRE, \"MITRE ATT&CK,\" 2021. [Online]. Available: \nhttps://attack.mitre.org/techniques/T1552/005/. [Accessed 7 May 2021].  \n[6]  CISA, \"Analysis Report (AR21 -013A),\" 14 January 2021. [Online]. Available: \nhttps://www.cisa.gov/uscert/ncas/analysis -reports/ar21 -013a. [Accessed 26 May 2021].  \n[7]  Kubernetes, \"A Closer Look at NSA/CISA Kubernetes Hardening Guidance,\" 5 October \n2021. [ Online]. Available: https://www.kubernetes.io/blog/2021/10/05/nsa -cisa-kubernetes -\nhardening -guidance/ 2021.  \n[8]  MITRE ATT&CK, \"Scheduled Task/Job: Container Orchestration Job,\" 27 7 2021. [Online]. \nAvailable: https://attack.mitre.org/techniques/T1053/007/. [Accessed 9 11 2021].  \n[9]  The Kubernetes Authors, \"Pod Security Admission,\" [Online]. Available: \nhttps://kubernetes.io/docs/concepts/security/pod -security -admission/.  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 42 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nAppendix A:  Example Dockerfile for non -root application  \nThe following example is a Dockerfile that runs an application as a non -root user with \nnon-group membership. The lines highlighted in red below are the portion specific to \nusing non -root.  \nFROM ubuntu:latest  \n \n#Update and  install the make utility  \nRUN apt update && apt install -y make \n \n#Copy the source from a folder called code and build the application with \nthe make utility  \nCOPY . /code  \nRUN make /code  \n \n#Create a new user (user1) and new group (group1); then switch into that \nusers context  \nRUN useradd user1 && groupadd group1  \nUSER user1:group1  \n \n#Set the default entrypoint for the container  \nCMD /code/app  \n  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 43 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nAppendix B:  Example deployment template for read -only file \nsystem  \nThe following example is a Kubernetes deployment template that u ses a read -only root \nfile system. The lines highlighted in red below are the portion specific to making the \ncontainers filesystem read -only. The lines highlighted in blue are the portion showing \nhow to create a writeable volume for applications requiring this capability.  \napiVersion: apps/v1  \nkind: Deployment  \nmetadata:  \n  labels: \n    app: web  \n  name: web  \nspec:  \n  selector:  \n    matchLabels:  \n      app: web  \n  template:  \n    metadata:  \n      labels: \n        app: web  \n      name: web  \n    spec: \n      containers:  \n      - command: [\"sleep\"]  \n        args: [\"999\"]  \n        image: ubuntu:latest  \n        name: web  \n        securityContext:  \n          readOnlyRootFilesystem: true  \n        volumeMounts:  \n          - mountPath: /writeable/location/here  \n            name: volName  \n      volumes:  \n      - emptyDir: {}  \n        name: volName  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 44 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nAppendix C:  Pod Security Policies (deprecated)  \nA Pod Security Policy (PSP) is a cluster -wide policy that specifies security \nrequirements/defaults for Pods to execute within the cluster. While security mechanis ms \nare often specified within Pod/deployment configurations, PSPs establish a minimum \nsecurity threshold to which all Pods must adhere. Some PSP fields provide default \nvalues used when a Pods configuration omits a field. Other PSP fields are used to deny \nthe creation of non -conformant Pods. PSPs are enforced through a Kubernetes \nadmission controller, so PSPs can only enforce requirements during Pod creation. \nPSPs do not affect Pods already running in the cluster.  \nPSPs are useful technical controls to enforce security measures in the cluster. PSPs \nare particularly effective for clusters managed by admins with tiered roles. In these \ncases, top -level admins can impose defaults to enforce requirements on lower -level \nadmins. NSA and CISA encourage organizations to adapt the Kubernetes hardened \nPSP template in Appendix D: Example Pod Security Policy  to their needs. The \nfollowing table describes some widely applicable PSP components.  \nTable V: Pod Sec urity Policy components3 \nField Name(s)  Usage  Recommendations  \nprivileged  Controls whether  Pods  can run \nprivileged containers.  Set to false.  \nhostPID, hostIPC  Controls whether  containers can \nshare host process namespaces.  Set to false.  \nhostNetwork  Controls  whether  containers can \nuse the host network.  Set to false.  \nallowedHostPaths  Limits containers to specific paths \nof the host file system.  Use a dummy path name (such \nas /foo marked as read -only). \nOmitting this field results in no \nadmission restriction s being placed \non containers.  \nreadOnlyRootFilesystem  Requires the use of a read only \nroot file system.  Set to true when possible.  \nrunAsUser, runAsGroup, \nsupplementalGroups, \nfsGroup  Controls whether  container \napplications can run with root \nprivileges or with root group \nmembership.  \n - Set runAsUser to \nMustRunAsNonRoot.  \n- Set runAsGroup to non -zero (See \nthe example in Appendix D: \nExample Pod Security Policy).  \n- Set supplementalGroups to non -\nzero (see example in appendix D). \n                                                \n3 https://kubernetes.io/docs/concepts/policy/pod -security -policy  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 45 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nField Name(s)  Usage  Recommendations  \n- Set fsGroup to non -zero (See the \nexample in Appendix D: Example \nPod Security Policy).  \nallowPrivilegeEscalation  Restricts escalation to root \nprivileges.  Set to false. This measure is \nrequired to effectively enforce \nrunAsUser: MustRunAsNonRoot \nsettings . \nseLinux  Sets the SELinux context of the \ncontainer.  If the environment supports \nSELinux, consider adding SELinux \nlabeling to further harden the \ncontainer.  \nAppArmor annotations  Sets the AppArmor profile used by \ncontainers.  Where possible, harden \nconta inerized applications by \nemploying AppArmor to constrain \nexploitation.  \nseccomp annotations  Sets the seccomp profile used to \nsandbox containers.  Where possible, use a seccomp \nauditing profile to identify required \nsyscalls for running applications; \nthen enable a seccomp profile to \nblock all other syscalls.  \nNote : PSPs do not automatically apply to the entire cluster for the following reasons:  \n First, before PSPs can be applied, the PodSecurityPolicy plugin must be enabled \nfor the Kubernetes admission contr oller, part of kube-apiserver .  \n Second, the policy must be authorized through RBAC. Administrators should \nverify the correct functionality of implemented PSPs from each role within their \nclusters organization.  \nAdministrators should be cautious in environ ments with multiple PSPs as  Pod creation \nadheres to the least restrictive  authorized policy. The following command describes all \nPod Security Policies for the given namespace, which can help to identify problematic \noverlapping policies:  \nkubectl get psp -n <namespace>  \n  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 46 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nAppendix D:  Example Pod Security Policy  \nThe following example is a Kubernetes Pod Security Policy that enforces strong security \nrequirements for containers running in the cluster. This example is based on official \nKubernetes documentation: https://kubernetes.io/docs/concepts/policy/pod -security -\npolicy/ . Administrators are encouraged to tailor the policy to meet their organizations \nrequirements.  \napiVersion: policy/v1beta1  \nkind: PodSecurityPolicy  \nmetadata:  \n  name: restricted  \n  annotations:  \n    seccomp.security.alpha.kubernetes.io/allowedProfileNames: \n'docker/default,runtime/default'  \n    apparmor.security.beta.kubernetes.io/allowedProfileNames: \n'runtime/default'  \n    seccomp.security.alpha.kubernetes.io/defaultProfileName:   \n'runtime/default'  \n    apparmor.security.beta.kubernetes.io/defaultProfileName:   \n'runtime/default'  \nspec: \n  privileged: false # Required to prevent escalations to root.  \n    allowPrivilegeEscalation: f alse   \n  requiredDropCapabilities:  \n    - ALL \n  volumes:  \n    - 'configMap'  \n    - 'emptyDir'  \n    - 'projected'  \n    - 'secret'  \n    - 'downwardAPI'      \n    - 'persistentVolumeClaim' # Assume persistentVolumes set up by admin \nare safe  \n  hostNetwork: false  \n  hostIPC: false  \n  hostPID: false  \n  runAsUser:  \n    rule: 'MustRunAsNonRoot' # Require the container to run without root  \n  seLinux:   \n    rule: 'RunAsAny' # This assumes nodes are using AppArmor rather than \nSELinux \n  supplementalGroups:  \n    rule: 'MustRunAs'  \n    ranges: # Forbid adding the root group.        \n      - min: 1 \n        max: 65535  \n  runAsGroup:  \n    rule: 'MustRunAs'  \n    ranges: # Forbid adding the root group.        \n      - min: 1 \n        max: 65535  \n  fsGroup:  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 47 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \n    rule: 'MustRunAs'  \n    ranges: # Forbid adding the root group.        \n      - min: 1 \n        max: 65535  \n  readOnlyRootFilesystem: true  \n  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 48 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nAppendix E:  Example namespace  \nThe following example is for each team or group of users, a  Kubernetes namespace \ncan be created using  either a kubectl  command or YAML file. Any name with the \nprefix kube- should be avoided as it may conflict with Kubernetes system reserved \nnamespaces.  \nKubectl  command to create a namespace : \nkubectl create namespace <insert -namespace -name-here> \nTo create namespace using YAML file, create a  new file  called my-namespace.yaml  \nwith the contents:  \napiVersion: v1  \nkind: Namespace  \nmetadata:  \n  name: <insert -namespace -name-here> \nApply the namespace using : \nkubectl create f ./my-namespace.yaml  \nTo create new Pods in an existing namespace, switch to the desired namespace using:  \nkubectl config use -context <insert -namespace -here> \nApply new deployment using:  \nkubectl apply -f deployment.yaml  \nAlternatively, the namespace can be added to the kubectl  command using:  \nkubectl apply -f deployment.yaml --namespace=<i nsert-namespace -here> \nor specify namespace: <insert -namespace -here>  under metadata in the YAML \ndeclaration.  \nOnce created, resources cannot be moved between namespaces. The resource must \nbe deleted, then created in the new namespace.   \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 49 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nAppendix F:  Example network policy  \nNetwork policies differ depending on the network plugin used. The following  example is \na network policy to limit access to the nginx service to Pods with the label access  using \nthe Kubernetes documentation : https://kubernetes.io/docs/tasks/administer -\ncluster/declare -network -policy/  \napiVersion: networking.k8s.io/v1  \nkind: NetworkPolicy  \nmetadata:  \n name: example -access-nginx \n namespace: prod   #this can any namespace or be left out if no \nnamespace is used  \nspec: \n    podSelector:  \n        matchLabels:  \n            app: nginx  \n    ingress:  \n    -from: \n        -podSelector:  \n            matchLabels:  \n                access: true  \nThe new NetworkPolicy can be applied using:  \nkubectl apply -f policy.yaml  \nA default deny all ingress policy:  \napiVersion: networking.k8s.io/v1  \nkind: NetworkPolicy  \nmetadata:  \n    name: deny -all-ingress \nspec: \n    podSelector: {}  \n    policyType:  \n    - Ingress \nA default deny all egress policy:  \napiVersion: network ing.k8s.io/v1  \nkind: NetworkPolicy  \nmetadata:  \n    name: deny -all-egress \nspec: \n    podSelector: {}  \n    policyType:  \n    - Egress \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 50 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nAppendix G:  Example LimitRange  \nLimitRange support is enabled by default in Kubernetes 1.10 and newer. The following \nYAML file specifies a LimitRange with a default request and limit , as well as a min and \nmax request , for each  container . \napiVersion: v1  \nkind: LimitRange  \nmetadata:  \n   name: cpu -min-max-demo-lr \nspec: \n   limits  \n   -  default:  \n         cpu: 1 \n      defaultRequest:  \n         cpu: 0.5  \n      max:    \n         cpu: 2 \n      min: \n         cpu 0.5  \n      type: Container  \nA LimitRange can be applied to a namespace with:  \nkubectl apply -f <example -LimitRange>.yaml --namespace=<Enter -Namespace>  \nAfter this example LimitRange configuration is applied , all containers created in the \nnamespace are assigned the default CPU request and limit  if not specified. All \ncontainers in the namespace must have a CPU request greater than or equal to the \nminimum value and less than or equal to the maximum CPU value  or the container will \nnot be instantiated . \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 51 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nAppendix H:  Example ResourceQuota  \nResourceQuota objects to limit aggregate resource usage within a namespace are \ncreated by applying a YAML file to a namespace or specifying requirements in the \nconfigur ation file of Pods. The following example is based on official Kubernetes \ndocumentation: https://kubernetes.io/docs/tasks/administer -cluster/ma nage -\nresources/quota -memory -cpu-namespace/  \nConfiguration file for a namespace:  \napiVersion: v1  \nkind: ResourceQuota  \nmetadata:  \n    name: example -cpu-mem-resourcequota  \nspec: \n    hard: \n        requests.cpu: 1  \n        requests.memory: 1Gi  \n        limits.cpu: 2 \n        limits.memory: 2Gi  \nThis ResourceQuota can be applied with:  \nkubectl apply -f example -cpu-mem-resourcequota.yaml --\nnamespace=<insert -namespace -here> \nThis ResourceQuota places the following constraints on the chosen namespace:  \n Every container mus t have a memory request, memory limit, CPU request, and \nCPU limit , \n Aggregate m emory request for all containers should not exceed 1 GiB , \n Total m emory limit for all containers should not exceed 2 GiB , \n Aggregate CPU request for all containers should not excee d 1 CPU , and  \n Total CPU limit for all containers should not exceed 2 CPUs . \n  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 52 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nAppendix I:  Example encryption  \nTo encrypt Secret data at rest, the  following encryption configuration file provides an \nexample to specify the type of encryption desired and the encryption key. Storing the \nencryption key in the encryption file only slightly improves security. The Secrets  will be \nencrypted, but the key will  be accessible in the EncryptionConfiguration  file. This \nexample is based on official Kubernetes documentation: \nhttps://kubernetes.io/docs/tasks/administer -cluster/encrypt -data/. \napiVersion: apiserver.config.k8s.io/v1  \nkind: EncryptionConfiguration  \nresources:  \n   -  resources:  \n      -  secrets \n      providers:  \n      -  aescbc: \n            keys: \n            -  name: key1  \n               secret: <base 64 encoded secret>  \n      -  identity: {}  \nTo enable encryption at rest with this encryption file, restart the API server with the  --\nencryption -provider -config  flag set with the location to the config uration  file. \n  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 53 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nAppendix J:  Example KMS configuration  \nTo encrypt Secrets  with a key management service (KMS) provider plugin, the following \nexample encryption configuration YAML file can be used to set the properties for the \nprovider. This example is based on official Kubernetes documentation: \nhttps://kubernetes.io/docs/tasks/administer -cluster/kms -provider/ .  \napiVersion: apiserver.config.k8s.io/v1  \nkind: EncryptionConfiguration  \nresources:  \n  -  resources:  \n       -  secrets \n     providers:  \n       -  kms: \n             name: myKMSPlugin  \n             endpoint: unix://tmp/socketfile.sock  \n             cachesize: 100  \n             timeout: 3s  \n       -  identity: {}  \nTo configure the API server to use the KMS provider, set the  --encryption -\nprovider -config  flag with the location of the configuration file and restart the API \nserver.  \nTo switch from a local encryption provider to KMS, add the KMS provider section of the \nEncryptionConfiguration file above the current encryption met hod, as shown below.  \napiVersion: apiserver.config.k8s.io/v1  \nkind: EncryptionConfiguration  \nresources:  \n  -  resources:  \n       -  secrets \n     providers:  \n       -  kms: \n             name: myKMSPlugin  \n             endpoint: unix://tmp/socketfile.sock  \n             cachesize: 100  \n             timeout: 3s  \n       -  aescbc: \n             keys: \n                -  name: key1  \n                   secret: <base64 encoded secret>  \nRestart the API server and run the command below to re -encrypt all Secrets  with the \nKMS provider.  \nkubectl get secrets --all-namespaces -o json | kubectl replace -f - \n  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 54 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nAppendix K:  Example pod -reader RBAC Role  \nTo create the example  pod-reader  Role, create a YAML  file with the following contents:  \napiVersion: rbac.authorization.k8s.io/v1  \nkind: Role \nmetadata:  \n namespace: your -namespace -name \n name: pod -reader \nrules: \n-  apiGroups: []    #  indicates the core API group  \n   resources: [pods]  \n   verbs: [get, watch, list]  \nApply the Role using:  \nkubectl apply --f role.yaml  \nTo create the exampl e global -pod-reader  ClusterRole:  \napiVersion: rbac.authorization.k8s.io/v1  \nkind: ClusterRole  \nmetadata: default  \n # namespace omitted since ClusterRoles are not bound to a \nnamespace  \n name: global -pod-reader \nrules: \n-  apiGroups: [] #  indicates the core API group  \n   resources: [pods]  \n   verbs: [get, watch, list]  \nApply the Role using:  \nkubectl apply --f clusterrole.yaml  \n  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 55 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nAppendix L:  Example RBAC RoleBinding and \nClusterRoleBinding  \nTo create a RoleBinding, create a YAML file with the following contents:  \napiVersion: rbac.authorization.k8s.io/v1  \n# This role binding allows jane to read Pods in the your -\nnamespace -name  \n# namespace.  \n# You need to already have a Role names pod -reader in that \nnamespace.  \nkind: RoleBinding  \nmetadata:  \n    name: read -pods \n    namespace: your -namespace -name \nsubjects:  \n# You can specify more than one subject  \n-  kind: User  \n   name: jane # name is case sensitive  \n   apiGroup: rbac.authorization.k8s.io  \nroleRef:  \n   # roleRef specifies the binding to a Role/ClusterRole  \n   # kind: Role # this must be a Role or ClusterRole  \n # this must match the name of the Role or ClusterRole you wish to \nbind  \n # to \n   name: pod -reader  \n   apiGroup: rbac.authorization.k8s.io  \nApply the RoleBinding using:  \nkubectl apply --f rolebinding.yaml  \nTo create a ClusterRoleBinding, create a YAML file with the following contents:  \napiVersion: rbac.authorization.k8s.io/v1  \n# This cluster role binging allows anyone in the manager group to \nread  \n# Pod information in any namespace.  \nkind: ClusterRoleBinding  \nmetadata: \n    name: global -pod-reader \nsubjects:  \n# You can specify more than one subject  \n-  kind: Group  \n   name: manager # Name is case sensitive  \n   apiGroup: rbac.authorization.k8s.io  \nroleRef:  \n   # roleRef specifies the binding to a Role/ClusterRole  \n   kind: ClusterRole # this must be a Role or ClusterRole  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 56 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \n   name: global -pod-reader # this must match the name of the Role or \nClusterRole you wish to bind to  \n   apiGroup: rbac.authorization.k8s.io  \nApply the Cluster RoleBinding using:  \nkubectl apply --f clusterrolebinding.yaml   \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 57 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nAppendix M:  Audit Policy  \nThe following example is an Audit Policy that logs requests involving Kubern etes \nSecrets at the Metadata  level, and all other audit events at the highest level : \napiVersion: audit.k8s.io/v1  \nkind: Policy  \nrules: \n   -  level: Metadata  \n      resources:  \n- group:  #this refers to the core API group  \nresources: [secrets]  \n    -  level: RequestResponse  \n \n  # This audit policy logs events involving secrets at the metadata \nlevel, and all other audit events at the RequestResponse level  \nIf an organization has the resources available to store, parse, and examine a large \nnumber of logs, then logging all events , other than those involving S ecrets, at the \nhighest level is a good way of ensuring that , when a  breach occurs , all necessary \ncontextual information is present in the logs. If resource consumption and availability \nare a concern, then more logging rules can be established to lower the logging level of \nnon-critical components and routine non -privileged actions, as l ong as audit  logging  \nrequirements for the system are being met. As Kubernetes API events consist of \nmultiple stages, logging rules can also specify stages of the request to omit from the \nlog. By default, Kubernetes captures audit events at all stages of th e request. The four \npossible stage s of Kubernetes API request are:  \n RequestReceived  \n ResponseStarted  \n ResponseComplete  \n Panic  \nBecause clusters in organizations expand to meet growing needs, it is important to \nensure that the audit policy can still meet logging  needs. To ensure that elements of the \nenvironment are not overlooked, the audit policy should end with a catch -all rule to log \nevents that the previous rules did not log . Kubernetes logs audit events based on the \nfirst rule in the audit policy that applie s to the given event; therefore, it is important to be \naware  of the order in which potentially overlapping rules are written . The r ule regarding \nSecrets should be near the top of the policy file to ensure . This ensures  that any \noverlapping rule s do not ina dvertently capture S ecrets due to logging at  a higher level \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 58 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nthan the Metadata  level. Similarly, the catch -all rule should be the last rule in the policy \nto ensure that all other rules are matched first .  \nWhat follows are some examples of critical event types that should be logged at the \nRequest  or RequestResponse  level. In addition are examples of  less critical event \ntypes and stages that can be logged at a lower level if necessary  to reduce redundancy \nin the logs  and increase the organization s ability to effectively review  the logs as close \nto real time as possible . \nCritical:  \n Pod deployments and alterations  \n Authentication request s \n Modifications  to RBAC resources (clusterrolebindings, clusterroles, etc.) \n Scheduled job  creations  \n Edits to Pod Security Admissions or Pod Security Policies  \nNoncritical:  \n RequestRecieved  stage  \n Authenticated requests to non -critical, routinely accessed resources  \nFor an example of how to establish these rules , refer to the official Kubernetes \ndocumentation: https://kubernetes.io/docs/tasks/debug -application -cluster/audit/ . \n  \n\n \n \n \nU/OO/168286 -21 | PP-22-0324  | August  2022 Ver. 1.2 59 \nNational  \nSecurity  \nAgency  \nCybersecurity \nand Infrastructure \nSecurity Agency  \nKubernetes Hardening Guidance  \nNational  \nSecurity  \nAgency  \nAppendix N:  Example Flags to Enable Audit Logging  \nIn the control plane , open the kube-apiserver.yaml  file in a text editor.  Editing the \nkube-apiserver  configuration requires administrator privileges.  \nsudo vi /etc/kubernetes/manifests/kube -apiserver.yaml      \n \nAdd the following text to the kube-apiserver.yaml  file: \n    --audit-policy-file=/etc/kubernetes/p olicy/audit -policy.yaml  \n    --audit-log-path=/var/log/audit.log  \n    --audit-log-maxage=1825  \n \nThe audit-policy-file flag should be set with the path to the audit policy , and the \naudit-log-path  flag should be set with the desired secure location for the audit logs \nto be written to. Other additional flags exist, such as the audit-log-maxage flag \nshown here , which stipulates the maximum number of days the logs should be kept, \nand flags for specify ing the maximum number of audit log files to retain, max log file size  \nin megabytes, etc. The only flags necessary to enable logging are the audit-policy-\nfile  and audit-log-path  flags. The other flags can be used to configure logging to \nmatch the organizat ions policies.  \nIf a users kube-apiserver  is run as a Pod, then it is necessary to mount the volume \nand configure hostPath  of the policy and log file locations for audit records to be \nretained. This can be done by adding the following sections to the kube-\napiserver.yaml  file as noted in the Kubernetes documentation : \nhttps://kubernetes.io/docs/tasks/debug -application -cluster/audit/  \nvolumeMounts:  \n -mountPath: /etc/kubernetes/audit -policy.yaml  \n  name: audit  \n  readOnly: true  \n -mountPath: /var/log/audit.log  \n  name: audit -log \n  readOnly: false  \n \nvolumes:  \n- hostPath:  \n   path: /etc/kubernetes/audit -policy.yaml  \n   type: File  \nname: audit  \n - hostPath:  \n   path: /var/log/audit.log  \n   type: FileOrCreate  \nname: audit -log \n\n",
  "cves": [],
  "techniques": [],
  "advisory": "cybersecurity-alerts",
  "title": "ctr_kubernetes_hardening_guidance_1.2_20220829",
  "source": "nsa",
  "id": "0b1a8e419b09320d0a75aec87fd91cfebdd86e4e7dfb9b0d2db33196a8df123a"
}