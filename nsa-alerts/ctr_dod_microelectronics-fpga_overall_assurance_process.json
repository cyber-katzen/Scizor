{
  "markdown": " \n \n \n \nNational Security Agency  \nCybersecurity  Technical Report  \n \n \n \n \n \nDoD Microelectronics:  \nField  Programmable Gate Array  \nOverall Assurance Process  \n \n \n \n \n \n \n \nMay 2024  \n \nU/OO/ 156722 -24 \nPP-24-1462  \nVersion 1.1 \n\n\n \n \nU/OO/ 156722 -24 | PP-24-1462  | May 2024 Ver. 1.1  ii \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Overall Assurance Process  \n \n \n This document was created through collaboration with each of the \nJFAC labs: National Security Agency (NSA), Air Force Research \nLab (AFRL) RYDT, Naval Surface Warfare Center (NSWC) Crane, \nand Army Development Command (DEVCOM)/AVMC.  \nFor additional information, guidance, or assistance with this \ndocument, please contact the Joint Federated Assurance Center \n(JFAC) at JFAC_HWA@radium.ncsc.mil . \n  \n  \n\n\n \n \nU/OO/ 156722 -24 | PP-24-1462  | May 2024 Ver. 1.1  iii \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Overall Assurance Process  \nNotices and history   \nDocument change history  \nDate  Version  Description  \nDecember  2022  1.0 Initial Publication  \nMay 2024  1.1 Minor gram mar fixes. Updated r eferences to FPGA documents . \nDisclaimer of warranties and endorsement  \nThe information and opinions containe d in this document are provided \"as is\" and without any warranties \nor guarantees. Reference herein to any specific commercial products, process, or service by trade name, \ntrademark, manufacturer, or otherwise, does not constitute or imply its endorsement, recommendation, or \nfavoring by the United States Government, and this guidance shall not be used for advertising or product \nendorsement purposes.  \nPublication information  \nAuthor(s)   \nNational Security Agency  \nCybersecurity Directorate  \nJoint Federated Assuran ce Center  \nContact information  \nJoint Federated Assurance Center : JFAC_HWA@radium.ncsc.mil  \nGeneral Cybersecurity Report Inquiries: CybersecurityReports@nsa.gov  \nDefense Industrial Base Inquiries and Cybersecurity Services: DIB_Defense@cyber.nsa.gov  \nMedia inquiries / Press Desk: Media Relations, 443-634-0721, MediaRelations@nsa.gov  \nPurpose  \nThis document was developed in furtherance of NSAs cybersecurity missions . This  includ es its \nresponsibilities to identify and disseminate threats to National Security Systems, Department of Defense \ninformation systems, and the Defense Industrial Base, and to develop and issue cybersecurity \nspecifications and mitigations. This information may be shared broadly to reach all appropriate \nstakeholders.   \n\n \n \nU/OO/ 156722 -24 | PP-24-1462  | May 2024 Ver. 1.1  iv \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Overall Assurance Process  \nExecutive summary  \nThis document describes the overall process for developing  Field  Programmable Gate \nArray ( FPGA ) Assurance best practices  for the Department of Defense ( DoD).  \nThis document also serves as a guide for implement ing an assurance process  for other \ncustom hardware categories .  \nThe ove rarching assurance process  is summarized as follows:  \n Standardize practices  \n Detect and mitigate  threats  \n Understand and respond to attacks  \nThis document identifies  standard  practices, as well as the methods to use to evaluate \npotential mitigation s designed to detect and counter  threats.  \n  \n\n \n \nU/OO/ 156722 -24 | PP-24-1462  | May 2024 Ver. 1.1  v \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Overall Assurance Process  \nContents  \nDoD Microelectronics: Field Programmable Gate Array Overall Assurance Process  ........... i \nExecutive summary  ................................ ................................ ................................ ......................  iv \n1. Standardize practices ................................ ................................ ................................ ...............  1 \n1.1. Identify and define specific levels of assurance  ................................ ................................ ....................  2 \n1.2. Establish concrete criteria  ................................ ................................ ................................ ..............................  3 \n1.3. Identify the set of known threats of interest  ................................ ................................ ............................  5 \n1.4. Determine to which level of  assurance each threat is relevant  ................................ ......................  5 \n1.5. Identify the common mitigations against the threats  ................................ ................................ ...........  6 \n1.6. Work with vendors and stakeholders  ................................ ................................ ................................ ........ 7 \n2. Detect and mitigate threats  ................................ ................................ ................................ ..... 8 \n3. Understand and respond to attacks  ................................ ................................ .......................  9 \n4. Use the assurance process  ................................ ................................ ................................ ..... 9 \n5. Conclusion  ................................ ................................ ................................ ..............................  10 \nAppendix A: St andardized Terminology  ................................ ................................ ..................  11 \nAppendix B: LoA1 Mitigation Overview  ................................ ................................ ...................  14 \n \nTable s \nTable I: Example FPGA LoA Table  ................................ ................................ ................................ .........................  4 \n\n \n \nU/OO/ 156722 -24 | PP-24-1462  | May 2024 Ver. 1.1  1 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Overall Assurance Process  \n1. Standardize practices  \nThe goal of th e Levels of Assurance (LoA)  process is to develop a robust set of \nstandards and guidance that captures how  programs  select and achieve the desired \nLoA. For the field programmable gate array ( FPGA ) effort, this guidance was captured \nin the following ten documents , available at https://www.nsa.gov/Press -Room/DoD -\nMicroelectronics -Guidance/ . \n CTR: DoD Microelectronics: Field Programmable Gate Array Quick Start Guide  \n CTR: DoD Microelectronics: Field Programma ble Gate Array  Overall  Assurance \nProcess   \n CTR: DoD Microelectronics: Levels of  Assurance Definitions and Applications   \n CTR: DoD Microelectronics: Field Programmable Gate Array Best Practices  \nThreat Catalog  \n CTR: DoD Microelectronics: Field Programmable Ga te Array  Level of Assurance \n1 Best Practices   \n CTR: DoD Microelectronics: Third -Party IP Review Process for Level of \nAssurance 1  \n CTR: DoD Microelectronics: Field Programmable Gate Array  Level of Assurance \n2 Best Practices  \n CTR: DoD Microelectronics: Third -Party IP Review Process for Level of \nAssurance 2  \n CTR: DoD Microelectronics: Field Programmable Gate Array  Level of Assurance \n3 Best Practices  \n CTR: DoD Microelectronics: Third -Party IP Review Process for Level of \nAssurance 3  \nIn any assurance process, it is essential that  the methodology is consistent and \nrepeatable. The t hree levels of assurance (LoA), their mitigations , and the metrics to \nmeasure their effectiveness are designed to be vendor and product independent. They \nare designed to provide mitigations at the highest level of granularity where they are \neffective and target the threats that originate from a malicious actor  that has  malicious \nintent and could have capabilities ranging from the level of a commercial enterprise up \nto that of a well-resourced  nation -state  effort.   \n\n \n \nU/OO/ 156722 -24 | PP-24-1462  | May 2024 Ver. 1.1  2 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Overall Assurance Process  \nIn developing standards and best practices, the assurance team performing this work \nshould be able to:  \n Identify and define specific L oAs. Using the LoAs established in FPGA Overall \nAssurance Process is strongly recommend for all hard ware problems. Different  \nprocesses  may be appropriate in the software environment . \n Establish concrete criteria to determine what attacks should  be mitigated to \nachieve each LoA.  \n Identify the threats of interest and bundle them in groups with common \nmitigat ions.  \n Determine which threat category is relevant to each LoA . \n Use the  Attack -Countermeasures Analysis ( ACMA ) tree approach to validate the \nassumptions made and ensure the thoroughness of the mitigation approach.  \nThese areas of effort are further defined in the following sections with a summary of \nhow each was addressed in the FPGA arena.  Standardized terminology for the \nassurance effort is defined in Appendix A . \n1.1. Identify and define specific  levels of assurance  \nThe LoAs are designed to protect hardware systems in programs that have varying \nassurance requirements. Standards and guidelines need to be designed to support \nthese multiple assurance levels. In most cases, the standards and guideline s will need \nto accommodate both high and low LoAs.   \nThe LoAs defined  by the FPGA team were intended to apply across the hardware \nspace. The number of levels spanning  the extremes should  be decided.  It is essential \nthat the number of levels remain small. Hardware assurance is the act of identifying \nthreats and mitigations that apply to the specific device type. This identification process \nis a judgement process undertaken by subject matter experts and not something easily \nmeasured. As a result, a  guiding a ssumption of this process is that it is effectively \nimpossible to measure the difference in importance between two threats with extreme \naccuracy. While a broad categorization is possible, attempts to provide too much \ngranularity will make it extremely diff icult to agree on threats of conce rn or mitigation \neffectiveness.  Additionally, the number of available mitigations should also factor into \nthe decision of how many levels are appropriate. Different levels of assurance should \ndictate different sets of miti gations. If there are only two available mitigations for given \nthreats, it does not make sense to have five levels of assurance.  \n\n \n \nU/OO/ 156722 -24 | PP-24-1462  | May 2024 Ver. 1.1  3 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Overall Assurance Process  \nIn the end, the  number of  levels should provide the appropriate protections without \nimposing unnecessary costs on  programs. In the case of FPGA Assurance, three levels  \naccount ed for the needs of projects utilizing FPGAs.  \n LoA1 - Threats that are inexpensive to implement but with high utility to the \nadversary . (Refer to Appendix B  for an overview of LoA1 mitigations.)  \n LoA2 - Threats that have greater expense and investment but are less targetable \nor most likely conducted for pre -positioning.  \n LoA3 - Threats that are very expensive to implement or have unpredictable \noutcomes.  \nThese levels are further detailed in the Levels of Assurance Definitions and Applications  \ndocument.  \n1.2. Establish concrete criteria  \nOnce the  assurance team defines the specific LoAs , they should  outline a set of \nconcrete criteria by which a threat may be evaluated to determine at which LoA it should  \nbe mitigate d. The criteria should  be as objective as possible and enable the team to \nassess each threat through a simple set of questions. Additionally, the criteria does not \nnecessarily need to limit a threat to a single assurance level for mitigation as there may \nbe multiple ways to carry it out . However, the circumstances that categorize  the threat in \nits respective LoA should be clear.  \nIn the case of FPGA LoAs, the criteria were  based on the cost to implement a threat \nand the usefulness of the attack  to the adversary . More details regarding these criteria  \nmay be found in the Levels of Assurance Definitions and Applications  document. The \nintent here was t o avoid threat rankings based on  small variations in an attack, but \nrather to use simple questions about how each threat can be carried out and the effects \nthey would have.  \nThe required access, technology, and investment needed to carry out an attack  would \nstand as criteria used to measure its cost. The value of effect, or utility , criteria \nmeasures the positive outcome of the attack for an adversary, given success.  The \ntargetability criteria measures the utility of an attack. Each criteria has been designed, \nas much as possible, to be a straightforward question with limited ambiguity. These \ncriteria serve not only as a guide for measuring attacks, but also for assessing \nmitiga tions. To be of value , a mitigation does  not need to be perfect, but it should  have \na measurable effect on one of the criteria significant enough that an attacker would  take \n\n \n \nU/OO/ 156722 -24 | PP-24-1462  | May 2024 Ver. 1.1  4 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Overall Assurance Process  \nadditional actions covered by a higher LoA . Table I provides a description of the threat \ncosts and impact s at each LoA level . \nTable I: Example FPGA LoA Table  \n  \n \n  \n \n  \n \n \n \nThreat Effort or  Cost  \nAccess  \n A single available \npoint of access  A difficult point of \naccess or multiple \navailable points of \naccess  Multiple points of \ndifficult access  \nTechnology  \n Existing public \ntechnology  Low implementation \nrisk technology  Technologically \nfeasible  \nInvestment  \n Minimal investment of \nresources  A large \nmultidisciplinary team  A nation scale and \ndirected priority  \nThreat Consequences  or Impact  \nValue of \nEffect  \n Disable or subvert a \nsystem  Establish \nvulnerabilities (for \nfuture exploitation)  Degrade system \nperformance  \nTargetability  \n Inherently targetable \nand useful  Affect only a subset of \nsystems  Blind attacks ( difficult \nto precisely target  or \ncontrol the outcome)1 \n \n                                                \n1 LoA3 systems are best approached with a full risk analysis to identify which blind attacks are of concern to a given system. Realistic risks in this \nspace are often idiosyncratic, and the most concerning blind attacks are typically not the most expensive.  LoA3 is the least one size fits all of \nall the categories specifically because many such systems are judged to need to concern themselves with such unpredictable ef fects.   \nLoA1  \n LoA2  \n LoA3  \n\n \n \nU/OO/ 156722 -24 | PP-24-1462  | May 2024 Ver. 1.1  5 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Overall Assurance Process  \n1.3. Identify the s et of known threats of interest  \nThe next step in the  assurance  process is to list the threats of interest and bund le them \nin groups with the common mitigations  expected to address them . Those threats should \nbe explored ensuring the entire attack space is identified.  Each of the attacks should be \nidentified and docume nted into a threat catalog.  This process is expert driven and it is \nstrongly recommended that, if possible , the assurance team has expertise in the \ntechnology, and expertise with  operations in the human, cyber , and supply chain \ndomains.  \nTo explore the att ack space, the team should begin by brainstorm ing to  identify all \npossibilities.  This should consider the complete supply chain . The team should  think \nthrough all the possible attacks that could happen, including those that are already \nmitigated.  \nA major component of the FPGA Assurance development process was the FPGA Trust \nStudy developed by Sandia National Labs . A study with that level of detail may not be \nnecessary in all cases, but the complete supply chain for the technology of interest \nshould  be considered.  \nNext, these attacks should be condensed into a smaller set. Attacks should not be \nremoved, but similar attacks at similar places in the supply chain should be grouped. \nThat is, attacks which the experts believe are likely to be mitigated by  the same \napproach should be in the same category.  \nFor instance, in the FPGA Trust Study there is a threat \"Adversary modifies GOTS \nsystem at design.\" This attack includes hacking into a contractor network, but it also \nincludes inducing  a contractor on th e design team to do something. It even includes \nbreaking and entering into the contract design facility. The commonality is that all of it is \nhappening dur ing the design process at a U.S. Government contractor site and in a \nplace where there is access to d esign code. As a result, these  threats  can be captured \nunder a single threat.  \n1.4. Determine to which level of assurance each threat is \nrelevant  \nTopic experts should measure each threat by the established assurance criteria and  \nmap them to their appropria te level of a ssurance for mitigation.  \n\n \n \nU/OO/ 156722 -24 | PP-24-1462  | May 2024 Ver. 1.1  6 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Overall Assurance Process  \nFor the FPGA Assurance  Trust Study , each threat was assigned to two experts and \nmeasured against the cost and utility criteria. Any points of disagreement were \ndiscussed until resolved or raised up to the greater group. The goal is not to make \ncomparisons between threats or decide which is more important, but to decide which \nthreat s meet the criteria at a given level of assurance and to categorize them at that \nlevel.  \nThe goal of the entire assurance process is to p rovide honest feedback about the \nvulnerability of a supply chain, regardless of the potential mitigation cost or the ability to \nmitigate any particular threat. The goal is not to provide a list of the top five or  best \nbang for your buck mitigations. If  cost/benefit decisions need to be made, they should \nbe made separately  from the assurance process . Even threats that are known from the \noutset to be too costly  or technically infeasible to address should  still be included in the \nassessment to present a ho listic view of the threats and to enable fully informed \nmitigation  decisions.   \n1.5. Identify the common mitigations against  the threats  \nThe next step is to evaluate  known mitigations  as follows:  \n1. Mitigation proposal  - In mitigation proposal , a mitigation is identified and \ndefine d.  \nIn the FPGA Assurance Trust Study , this was a brainstorming session in which \nexperts identified what mitigations exist. Additional inputs to this are research or \ndevelopment efforts that are preparing for tech tran sition. An identified mitigation \nshould be associated to the threats that it is intended to  mitigate . \n2. Mitigation vetting  - In mitigation vetting , an expert performs a  \"sanity check\"  to \ndetermine plausibility. The purpose of this check is to determine if a mitigation \nwould make a difference assuming it is as effective as it could be.  \nTo be a valid solution, a mitigation needs to increase one of the five criteria for \nevaluating an attack by at least a n LoA  step as represented in Table I. A valid \nmitigation will make one of the criteria so much more difficult to carry out \nthat the threat category will become at least one LoA higher.  For instance , a \nmitigation may make it impossible for a single compromised insider to carry out \nthe threat , moving the access criteria from LoA1 to LoA2. In other words, a \nmitigation is of interest if it makes an attack within the threat harder in a way that \nis measurabl e by the  criteria.  \n\n \n \nU/OO/ 156722 -24 | PP-24-1462  | May 2024 Ver. 1.1  7 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Overall Assurance Process  \n3. Mitigation evaluation  In mitigation evaluation, a mitigation that has survived \nvetting should  be evaluated. While vetting assumes that the mitigation is as \neffective as it claims or could be, the evaluation determines how  effective the \nmitigation actually is.   \nAs mitigations receive positive evaluations , they are grouped into mitigation packages. \nMitigation packages come in two forms : standard packages and custom packages. The \nintent is that most programs seeking a low lev el of assurance will be able to use a \nstandard package. Higher levels of assurance may require a custom package.  \nA standard package is a pre -developed set of mitigations that achieve  a specific level of \nassurance when implemented.  They should  be developed by a team of experts that is \nindependent of implementation and qualified to evaluate threats. The package identifies \na set of mitigations that are achievable and cost efficient for the given level of assurance \nand includes them in a single document. Hardwa re vendors may perform some of these \nmitigations . For instance, in FPGA A ssurance, some key elements of assurance happen \nin the design and manufacture of the FPGA itself. Other mitigations should  be \nimplemented by the program at design, manufacture , or thr ough a device's lifecycle.  \nA program that has concerns  with the standard packages may create a custom \npackage. A custom package mitigates the same threats, and should  use mitigations \napproved through the same process. But it may choose to implement differe nt \nmitigations that are more cost efficient or operationally convenient for that program. \nOver time , recurring custom packages will lead to the development of new standard \nassurance packages.  \n1.6. Work with vendors and stakeholders  \nThe involvement of partners  and stakeholders  was vital in the development of this \ndocument  and is vital in developing the appropriate assurance . These stakeholder \ngroups  should include the D oD JFAC labs and D oD program elements. Working \ntogether , these groups will better def ine what levels of assurance are needed, what the \nthreats are , what mitigations exist , and which mitigations  need to be developed. In \naddition, input from relevant hardware vendors may help to inform the development in \nterms of existing threats and mitigat ions.  \nAdditionally, collaboration among all of the stakeholders enables  early adoption of \nassurance practices since the concerns of all parties are known from the beginning. In \nthe case of FPGA assurance, this collaboration was conducted through the FPGA \n\n \n \nU/OO/ 156722 -24 | PP-24-1462  | May 2024 Ver. 1.1  8 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Overall Assurance Process  \nWorking Group (FPGAWG) of the National Defense and Industry Association (NDIA). \nThis group establish ed a set of guidelines using existing mitigations , first while working \non a more comprehensive p ackage , and then  in parallel with its release. This \ncollaborative team also reviewed all products coming from the effort for completeness \nand applicability.  \nFinally, specific areas were also identified where the hardware vendors could provide \nmaterial ass istance with improving the current FPGA assurance environment. Together, \nthe collaboration resulted in documented assurance practices and policies support ed by  \nall stakeholders.  \n2. Detect and mitigate threats  \nFollowing the development of policy and best pr actices, two things should  follow:  \n Conduct an evaluation of the effectiveness of the recommended mitigations . \n Research and develop new mitigations to close existing threat gaps and \nmaximize the efficiency of those already in place. This effort should focus  on two \nareas:  \n Threats that do not have current or cost -effective mitigations , and  \n Threats that do not have mitigations at higher levels of assurance.  \nThe key to evaluating the mitigations is identifying an appropriate organization to \nevaluate the mitigation that is sufficiently independent from its implementation. Not all \nmitigations are technical. To be successful , an assurance program needs multiple \norganizations available to evaluate multi ple kinds of mitigations . These include technical \nmitigations  and more traditional counter -intelligence mitigations.  \nWhatever organization is doing the evaluation should be independent of the \norganization that developed the mitigation. Evaluations can be compromised by \nconflicts of interest when  the evaluators management or the evaluators themselves are \ninvested in the success of the mitigation . As a rule, the results of the evaluation should \nnot be subject to review by anyone who is  in the chain of comman d of the developer. In \naddition, the evaluators should not consult  on the development of the mitigation.  \nThe evaluation  team will determine if the mitigation : \n Fully mitigates a threat , \n Could mitigate a threat in com bination with other mitigations , or \n\n \n \nU/OO/ 156722 -24 | PP-24-1462  | May 2024 Ver. 1.1  9 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Overall Assurance Process  \n Fails  to mitigate t he threat in a substantial way . \nThe evaluation team should create a set of criteria that defines the limits and conditions \nin which the mitigation is effective. This should include the level of monitoring or \nchecking appropriate to validate that the mitigation is employed correctly.  \nIn the development of assurance policies, the work done to list threats and mitigations \nwill identify areas in which either no mitigation exists, no comprehe nsive mitigation \nexisted , or only expensive complex ones are available. Research should be focused on \nclosing these gaps .  \nFor FPGA Assurance, the B road Agency Announcement (BAA)  process collect ed \nproposals for solutions to risk areas where there were no e ffective mitigations. The idea \nwas to complete the research and solution development in a year and then release  it for \nuse throughout the Do D. This process illustrates that the  assurance process will evolve \nwith the changing threats and mitigations  and wil l require updates as applicable . \n3. Understand and r espond to attack s \nAn assurance process needs to be responsive  to changes in adversary attacks. As \nsuch, it needs capability and infrastructure to detect and evaluate new threats. \nEvaluation s should result in actionable information that can be used to create new \nmitigations.  \nIn the hardware realm, these tasks require laboratory capabilities. Where capability  \ngaps exist, research should be targeted on developing detection and analysis \ntechniqu es. Additionally, it will be necessary to fund any measures needed to align \ncurrent lab capabilities with the evaluation needs.  \n4. Use the assurance process  \nA DoD program with hardware assurance concerns can rely on  their assurance process \nas a guide to mitigate attacks at the appropriate level. When applying these practices, \neach program should keep the following in mind:  \n The program office should  determine the appropriate assurance level that applies \nto their project. DoD Microelectronics  Levels of Assurance Definitions and \nApplications contains guidance for determining the appropriate level of \nassurance for a system and its components. The concepts presented there can \nbe directly applied to any area of hardware assurance.  \n\n \n \nU/OO/ 156722 -24 | PP-24-1462  | May 2024 Ver. 1.1  10 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Overall Assurance Process  \n Once the level o f assurance is chosen for a project, that program should merge \nthe associated mitigation plans into its own Program Protection Plan as soon as \npossible . \n The hardware in question will remain assured at the LoA chosen as long as the \nrecommended mitigations a re in place.  \n In cases where a program can demonstrate that compromised hardware does \nnot result in a project vulnerability, the program  would not have to qualify for an \nLoA. \n In cases where the program has developed custom mitigations, they should have \na defined path for approval of their approach.  \nHardware assurance is not something that is added onto a project at the end. It needs \nto be planned for from the very beginning as it will have impact on: \n Hardware acquisition strategy,  \n Design development environm ent and personnel vetting,  \n Network security,  \n Software validation and use,  \n Design testing,  \n Reliability and product testing, and  \n Product assembly.  \nIn the effort to stand up the FPGA Assurance Process, all of these factors were taken \ninto account and supporte d.  \n5. Conclusion  \nThis document captures the process used by FPGA Assurance  for implementing a \nhardware assurance process.  This guidance can be used  across the DoD, or it can be \ntailored  for a specific program.  For questions or inquiries contact JFAC at \nJFAC_HWA@radium.ncsc.mil . \n\n \n \nU/OO/ 156722 -24 | PP-24-1462  | May 2024 Ver. 1.1  11 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Overall Assurance Process  \nAppendix  A: Standardized Terminology  \nThe following terms are used in the Joint Federated Assurance Center Field \nProgrammable Gate Array Best Practices documents. These terms are modified from \nDefense Acquisition University definitions to support common understanding.  \nApplication design   The collection of schematics, constraints, hardware description \nlanguage (HDL), and other implementation files developed to generate an FPGA \nconfiguration file for use on one or many FPGA platforms.  \nApplication domain   This is the area of technology for the system itself, or a directly \nassociated area of technology. For instance, the system technology domain of a radar \nsystem implemented using FPGAs would be \"radar\" or \"electronic warfare.\"  \nConfiguration file   The set of all data produced by the application design team and \nloaded into an FPGA to personalize it. Referred to by some designers as a bitstream, \nthe configuration file includes that information, as well as additional configuration \nsettings and firmware, which some designers may not consider part o f their bitstream.  \nControllable effect   Program -specific, trigger -able function allowing the adversary to \nattack a specific target.  \nDevice/FPGA device   A specific physical instantiation of an FPGA.  \nExternal facility   An unclassified facility that is out of the control of the program or \ncontractor.  \nField programmable gate array (FPGA)   In this context , FPGA includes the full range \nof devices containing substantial reprogrammable digital logic. This includes devices \nmarketed as FPGAs, complex programmable logic devices (CPLD), system -on-a-chip \n(SoC) FPGAs, as well as devices marketed as SoCs and containing reprogrammable \ndigital logic capable of representing arbitrary functions. In addition, some FPGAs \nincorporate ana log/mixed signal elements alongside substantial amounts of \nreprogrammable logic.  \nFPGA platform   An FPGA platform refers to a specific device type or family of devices \nfrom a vendor.  \n\n \n \nU/OO/ 156722 -24 | PP-24-1462  | May 2024 Ver. 1.1  12 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Overall Assurance Process  \nHard IP   Hard IP is a hardware design captured as a physical layout, in tended to be \nintegrated into a hardware design in the layout process. Hard IP is most typically \ndistributed as Graphic Design System II (GDSII). In some cases, Hard IP is provided by \na fabrication company and the user of the IP does not have access to the full layout, but \nsimply a size and the information needed to connect to it. Hard IP may be distributed \nwith simulation hardware description language (HDL) and other soft components, but is \ndefined by the fact that the portion that ends up in the final hard ware was defined by a \nphysical layout by the IP vendor.  \nLevel of assurance (LoA)   A Level of Assurance is an established guideline that \ndetails the appropriate mitigations necessary for implementation given the impact to \nnational security associated with subversion of a specific system, without the need for \nsystem -by-system custom evaluation.  \nPhysical unclonable function (PUF)   This function provides a random string of bits of \na predetermined length. In the context of FPGAs, the randomness of the bitstrin g is \nbased upon  variations in the silicon of the device due to manufacturing. These bitstrings \ncan be used for device IDs or keys.   \nPlatform design   The platform design is the set of design information that specifies \nthe FPGA platform, including physical layouts, code, etc.  \nSoft IP   Soft IP is a hardware design captured in hardware description language \n(HDL), intended to be integrated into a complete hardware design through a synthesis \nprocess. Soft IP can be distributed in a number of ways, as functional  HDL or a netlist \nspecified in HDL, encrypted or unencrypted.  \nSystem   An aggregation of system elements and enabling system elements to achieve \na given purpose or provide a needed capability.  \nSystem design  System design is the set of information that de fines the \nmanufacturing, behavior, and programming of a system. It may include board designs, \nfirmware, software, FPGA configuration files, etc.  \nTarget  A target refers to a specific deployed instance of a given system, or a specific \nset of systems with a  common design and function.  \n\n \n \nU/OO/ 156722 -24 | PP-24-1462  | May 2024 Ver. 1.1  13 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Overall Assurance Process  \nTargetability  The degree to which an attack may have an effect that only shows up in \ncircumstances the adversary chooses. An attack that is poorly targetable would more \nlikely be discovered accidentally, have unintended conse quences, or be found in \nstandard testing.  \nThird -party intellectual property (3PIP)   Functions whose development are not \nunder the control of the designer. Use of the phrase intellectual property, IP , or 3PIP in \noutlining this methodology of design revie w does not refer to property rights , such as, \nfor example, copyrights, patents, or trade secrets. It is the responsibility of the party \nseeking review and/or the reviewer to ensure that any rights needed to perform the \nreview in accordance with the methodo logy outlined are obtained.  \nThreat category  A threat category refers to a part of the supply chain with a specific \nattack surface and set of common vulnerabilities against which many specific attacks \nmay be possible.  \nUtility  The utility of an attack is  the degree to which an effect has value to an \nadversarial operation. Higher utility effects may subvert a system or provide major \ndenial of service effects. Lower utility attacks might degrade a capability to a limited \nextent.  \nVulnerability  A weakness  in software, firmware, hardware, or service component that \ncan be exploited, causing a negative impact to the confidentiality, integrity, or availability \nof an impacted component or components.  \n\n \n \nU/OO/ 156722 -24 | PP-24-1462  | May 2024 Ver. 1.1  14 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: FPGA Overall Assurance Process  \nAppendix B : LoA1 Mitigation Overview  \nThe followi ng are the top takeaways from provided recommendations. Full details on \nimplementation can be found in the JFAC FPGA best practices documentation.  \n Design Process  \n Apply robust NIST approved cybersecurity processes to development \nenvironments.  \n Perform regular design and code reviews with multiple people involved at each \nstep.  \n Perform robust testing with complete requirements coverage and high code \ncoverage.  \n Use a reproducible build process to generate FPGA bitstreams/configurations.  \n Apply asymmetric aut hentication algorithms to all configuration files before \nsystem start. Manage the private keys using an HSM.  \n Design IP and Tools  \n Obtain design software from reputable sources. Validate before installation that \nits hash is as expected.  \n Select third -party IP  carefully, and do  not accept encrypted or obfuscated IP \nblocks. Either notify JFAC about its use or evaluate it for suitability through a \nsecurity audit.  \n Parts Acquisition and Assembly  \n Acquire parts through reputable channels. Validate parts  authenticity  either \ncryptographically or physically.  \n Assemble the system in a controlled way, or validate afterwards that it was \nassembled correctly. Ensure that the correct configuration and keys are installed.  \n Reporting  \n Assist JFAC by providing high -level informatio n about the programs FPGA \nusage.  \n Contact JFAC if there is suspicion of technical interference by an adversary.  \n \n\n",
  "cves": [],
  "techniques": [],
  "advisory": "cybersecurity-alerts",
  "title": "ctr_dod_microelectronics-fpga_overall_assurance_process",
  "source": "nsa",
  "id": "ac968b2514d9bd3d6d797c932efcae2557caf50cf0e4db4a3dedc5e8f7fcc1d6"
}