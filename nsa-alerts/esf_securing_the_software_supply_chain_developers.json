{
  "markdown": "DEVELOPERSRECOMMENDED PRACTICES GUIDE FORSECURING THE \nSOFTWARE SUPPLY CHAIN\n1 \nEnduring Security Framework \nAugust  2022  \n\nSecuring the Software Supply Chain: Recommended Practices for Developers  ii \nExecutive  Summary \nCyberattacks are conducted via cyberspace and target an enterprises use of cyberspace for the \npurpose of disrupting, disabling, destroying, or maliciously controlling a computing environment  or \ninfrastructure; or destroying the integrity of the data or stealing controlled information.1 \nRecent cyberattacks such as those executed against SolarWinds and its customers, and exploits that \ntake advantage of vulnerabilities such as Log4j , highlight weaknesses  within software supply \nchains, an issue which spans both commercial and open source software and impacts both p rivate \nand Government enterprises. Accordingly, there is  an increased need for software supply chain \nsecurity awareness and cognizance regarding  the potential for software supply chains to be \nweaponized by nation state adversaries using similar tactics, techniques,  and procedures (TTPs) .  \nIn response, the White House  released an Executive Order on Improving the Nations Cybersecurity \n(EO 14028). EO 14028 establishes new requirements to secure the federal governments software \nsupply chain. These requirements involve systematic reviews, process improvements, and security \nstandards for both software suppliers and developers, in addition to  customers who acquire \nsoftware for the Federal Government.  \nSimilarly, the Enduring Security Framework2 (ESF) Software Supply Chain Working Panel has \nestablished this guidance to serve as a compendium of suggested practices for developer s, \nsupplier s, and customer stakeholders to help ensure a more secure software supply chain . This \nguidance is organized into a three part series: Part 1 of the series focuses on software developers; Part 2 focuses on software suppliers; and Part 3 focuses on software customers.  \nCustomers (acquiring organizations) may use this guidance as a basis of describing, assessing, and measuring security pract ices relative to the software lifecycle . Additionally, suggested practices \nlisted herein may be applied across the acquisition, deployment, and operational phases of a software supply chain.  \nThe software supplier (vendor)  is responsible for liaising betwe en the customer and software \ndeveloper. Accordingly, vendor responsibilities  include ensuring the integrity and security of \nsoftware via contractual agreements, software releases and updates, notifications, and mitigations \nof vulnerabilities. This guidance  contains recommended best practices and standards to aid \nsuppliers in these tasks . \nThis document will provide guidance in line with industry best practices and principles which software developers are strongly enc ouraged to  reference. These principles include security \nrequirements planning, designing software architecture from a security perspective, adding security features, and maintaining the security of software and the underlying infrastructure (e.g., \nenvironments, source code review, testing).  \n1 Committee on National Security Systems (CNSS)  \n2 The ESF is a cross -sector working group that operates under the auspices of Critical Infrastructure Partnership \nAdvisory Council (CIPAC) to address threats and risks to the security and stability of U.S. national security systems. \nIt is comprised of experts from the U.S. government as well as representatives from the Information Technology, Communications, and the Defense Industrial Base sectors. The ESF is charged with bringing together representatives from private and public sectors to work on intelligen ce-driven, shared cybersecurity challenges.  \n\n \nSecuring the Software Supply Chain: Recommended Practices for Developers  iii \n  \nDISCLA IMER \nDISCLAIMER OF ENDORSEMENT  \nThis document was written for general informational purposes only. It is intended to appl y to a \nvariety of factual circumstances and industry stakeholder, and the information provided herein is \nadvisory in nature.  The guidanc e in this document is provided as is. Once published, the \ninformation within may not con stitute the most up- to-date guidance or technical information. \nAccordingly, the document does not, and is not intended to, constitute compliance or legal advice. Readers should confer with their respective advisors and subject matter experts to obtain advice \nbased on their individual circumstances.  In no event shall the United States Government be liable \nfor any damages arising in any way out of the use of or reliance on this guidance.  \nReference herein to any specific commercial product, process, or service by trade name, trademark, \nmanufacturer, or otherwise, does not constitute or imply its endorsement, recommendation, or \nfavoring by the United States Government, and this guidance shall not be used for advertising or \nproduct endorsement purposes. All trade marks are the property of their respective owners.  \nPURPOSE  \nNSA, ODNI, and CISA developed this document in furtherance of their respective cybersecurity missions, including their responsibilities to develop and issue cybersecurity recommendations  and \nmitig ations. This information may be shared broadly to reach all appropriate stakeholders.  \nCONTACT  \nClient Requirements / Inquiries:  Enduring Security Framework nsaesf@cyber.nsa.gov   \nMedia Inquiries / Press Desk:  \n NSA Media Relations, 443 -634 -0721, MediaRelations@nsa.gov   \n CISA Media Relations, 703 -235 -2010, CISAMedia@cisa.dhs.gov   \n ODNI Media Relations, dni -media@dni.gov  \n  \n\n \nSecuring the Software Supply Chain: Recommended Practices for Developers  iv \n \n \nTable of Contents  \nExecutive Summary  ................................................................................................................................................................ ii \n1 Introduction  ..................................................................................................................................................................... 1 \n1.1 Background  ............................................................................................................................................................. 1 \n1.2 Document overview  ............................................................................................................................................  2 \n2 Developer  ..........................................................................................................................................................................  3 \n2.1 Secure product  criteria and management  ................................................................................................. 4 \n2.2 Develop Secure Code  ........................................................................................................................................ 11 \n Modification or Exploitation of Source Code by Insiders  ........................................................  12 \n Open Source Management Practices  ................................................................................................ 19 \n Secure Development Practices  ............................................................................................................ 20 \n Code Integration  ....................................................................................................................................... 22 \n Defect/Vulnerability Customer Reported Issue  ..........................................................................  22 \n External Development Extensions  ....................................................................................................  23 \n2.3 Verify Third -Party Components ................................................................................................................... 24 \n Third -Party Binaries ................................................................................................................................ 24 \n Selections and Integration  .................................................................................................................... 25 \n Obtain Components from a Known and Trusted Supplier  ...................................................... 25 \n Component Maintenance  ...................................................................................................................... 26 \n Software Bill of Materials (SBOM)  ..................................................................................................... 26 \n2.4 Harden the Build Environment  .................................................................................................................... 27 \n Build Chain Exploits  ................................................................................................................................ 28 \n Exploited Signing Server  ....................................................................................................................... 33 \n2.5 Deliver Code  ......................................................................................................................................................... 34 \n Final Package Validation  ........................................................................................................................  34 \n Potential Tactics to Compromise the Software Packages and Updates  ............................. 35 \n Compromises of the Distribution System  ....................................................................................... 35 \n3 Appendices  ..................................................................................................................................................................... 37 \n3.1 Appendix A: Crosswalk between Scenarios and SSDF  ........................................................................ 37 \n3.2 Appendix B: Dependencies  ............................................................................................................................. 39 \n3.3 Appendix C: Supply Chain Levels for Software Artifacts (SLSA)  .................................................... 40 \n3.4 Appendix D: Artifacts and Checklist  ........................................................................................................... 42 \n3.5 Appendix E: Informative References  ......................................................................................................... 55 \n3.6 Appendix F: Acronyms Used in This Document  .................................................................................... 59 \n \n \n\n \nSecuring the Software Supply Chain: Recommended Practices for Developers  1 \n  \n1 Introduction  \nUnmitigated vulnerabilities in the software supply chain pose a significant risk to organizations. \nThis paper presents actionable recommendations for a software supply chains development, production and distribution, and management processes , to increase the resiliency of these \nprocesses against compromise.  \nAll organizations have a responsibility to establish software supply chain security practices to \nmitigate risks, but the organizations role in the software supply chain lifecycle determines the \nshape and scope of this responsibility.  \nBecause the considerations for securing the software supply chain vary based on the role an organization plays in the supply chain, this series presents recommendations geared toward these \nimportant roles , namely,  develop ers, suppliers, and customers (or the organization acquiring a \nsoftware product).  \nThis guidance is organized into a three -part series  and will be released coinciding with the software \nsupply chain lifecycle . This is Part 1 of the series which focuses  on software developer s. Part  2 of \nthe series  focuses  on the software supplier  and Part 3 of the series  focus es on the software \ncustomer. Th is ser ies will help foster communication between these three different roles and \namong  cybersecurity professionals that may facilitate increased  resiliency and security in the \nsoftware supply chain process.   \nIn this series, terms such as risk, threat, exploit, and vulnerability are based on descriptions defined \nin Committee on National Secur ity Systems Glossary (CNSSI 4009) .\n3 \n1.1 Background  \nHistorically, software supply chain compromises largely targeted commonly known vulnerabilities \norganizations that were left unpatched. While threat actors still use this  tactic to compromise \nunpatched systems , a new , less conspicuous method of compromise  also threatens  software supply \nchains and undermin es trust in the patching systems themselves that are critical to guarding \nagainst legacy compromises. Rather than waiting for public vulnerability disclosures, threat actors  \nproactively inject  malicious code into products that are then legitimately distributed downstream \nthrough the global supply chain. Over the last few years, these next -generation  software supply \nchain compromises have significantly increased for both open source and commercial software \nproducts.  \nTechnology consumers generally manage software downloads and broader, more traditional \nsoftware supply chain activities separately. Considering both the upstream and downstream phases \nof software as a component of supply chain risk management may help to identify problems and \nprovide a better way forward in terms of integrating activities to achieve systemic security. However, there are also some differences to account for in the case of software products.  A \ntraditional software supply chain cycle is from point of origin to point of consumption  and generally \nenables a customer to return a malfunctioning product and confine any impact . In contras t, if a \n \n3 CNSSI -4009.pdf  \n\nSecuring the Software Supply Chain: Recommended Practices for Developers  2 \n \n \n software package is injected with malicious code which proliferates to multiple consumers; the \nscale may be more difficult to confine and may cause an exponentially greater impact.  \nCommon  methods of compromise used against  software supply chain s include exploitation of \nsoftware design flaws, incorporation of vulnerable third- party components into a software product, \ninfiltration of the suppliers network with malicious code prior to the final software product being \ndelivered, and injection of malici ous software that is then  deployed by the customer.  \nStakeholders  must seek to mitigate security concerns specific to their area of responsibility. \nHowever, other concerns may require a mitigation approach that dictates a dependency on another stakeholder o r a shared responsibility by multiple stakeholders. Dependencies that are \ninadequately communicated or addressed may lead to vulnerabilities and the potential for compromise .  \nAreas  where these types of vulnerabilities may exist include :  \n Undocumented features or risky functionality , \n Unknown and/or revisions to contractual, functionality or security assumptions between \nevaluation and deployment , \n Supplier s change of ownership and/or of geo -location , and  \n Poor supplier enterprise  or development hygiene. \n1.2 Document overview  \nThis document contains the following additional sections and appendices :  \nSection 2  recommends  principles Developers may use to help secur e the  software development  \nlifecycle (SDLC ), an  important process used to protect the software supply pipeline.  \nSection 3 is a collect ion of  appendices supplementing  the preceding section s: \nAppendix A: Crosswalk B etween the NIST SP800 -218;  Mitigating the Risk of Software \nVulnerabilities by Adopting a Secure Software Development Framework  (SSDF)4 and U se Cases \ndescribed herein . \nAppendix B: Dependencies  \nAppendix C : Supply -Chain Levels for Software Artifacts (SLSA )5 \nAppendix D: Recommended Artifacts and Checklist  \nAppendix E: Informative References  \nAppendix F: Acronyms  \nEach section contains examples of threat scenarios  and recommended mitigations. Threat scenarios \nexplain how processes that compose a given phase of the software development lifecycle (SDLC) \nrelate to common vulnerabilities that could be exploited. The recommended mitigations present \ncontrols and mitigations that could reduce the impact of the threat s. \n \n4 Draft NIST SP 800- 218, Secure Software Development Framework (SSDF) Version 1.1: Recommendations for \nMitigating the Risk of Software Vulnerabilities  \n5 GitHub - slsa-framework/slsa: Supply -chain Levels for Software Artifacts  \n\n \nSecuring the Software Supply Chain: Recommended Practices for Developers  3 \n  \n2 Developer   \nThe secure software development lifecycle  (Secure SDLC ) is an important  process  used  to secure  \nthe software  supply  chain . An example of the individual group activities and the relationships \nbetween the customers, developers and s uppliers are represented in the figure below:  \n \nFigure  1: Software Supply Chain Group  Relationships and Activities  \nThe process starts  when  the suppliers program management  team  collects feature  requests  from  \ntheir customers user -base, technical base , and marketing  teams.  These  features  include  both  \noperational and security  enhancements  to the product  and are used  to generate use cases that are \nthen  formulated  into  prioritized  requirements.  The supplier and developer management team s \nwork together to define the requirements  that  are used  to produce the architecture  and high -level \ndesign  which  a development  team  uses to produce a product . In addition , the combined \nmanagement  team defines the product development security policies and practices that are used \nwhen producing the product. The process defines how development activities will be structured \nand what artifacts will be collected for verification and validation.  The following is a short list of \nexamples of  the Secure  SDLC process and practices:  \n NIST Secure Software Development Framework,6 \n Carnegie Mellon University Secure Software Development Lifecycle  Processes ,7 \n ACM The Protection of Information in Computer Systems ,8 \n OWASP Secure Development Lifecycle ,9 \n \n6 NIST Secure Software Development Framework  \n7 Carnegie Mellon University S ecure Software Development Lifecycle  Processes  \n8 https://web.mit.edu/Saltzer/www/publications/protection/  \n9 OWASP Secure Software Development Lifecycle (SSDLC)  \n\n\nSecuring the Software Supply Chain: Recommended Practices for Developers  4 \n \n \n  Cisco Secure Development Lifecycle ,10 \n Synopsys Secure Software Development Lifecycle  Phases ,11 \n US-Cert Secure Software Development Lifecycle  Processes ,12 \n OpenSSF  Secure Software Development Fundamentals Courses .13 \nIn addition to the high -level development documents produced, the management team defines the \nsecurity practices and procedures used for secure software development such as : \n Secure coding practices, \n The code review process , \n Software repository procedures, testing, and vulnerability assessments , \n Procedures f or securely building  and distributing the product . \nOnce  released, a product  is monitored  for defects  through  a support  channel , available to product \ncustomers, and developers can securely provide updates  and upgrades  to address reported issues.  \nFor each  operation  within  the Secure SDLC,  artifacts  are created  which  attest to the adherence to \nthe processes  required and outlined . These artifacts are outline in  Appendix D: Artifacts and \nChecklist . \n2.1 Secure p roduct  criteria  and  management  \nAs described in  Section  2. 2 Develop s ecure code  through Section 2.5 Deliver c ode , the \ndeveloper  use cases are dependent  on the procedures  and policies defined  within  a Secure SDLC \nprocess. Development team managers and members adapt  and customize  this process to  meet the ir \nspecific needs. The Secure SDLC  identifies the exact procedures and policies that are  used  to ensure  \nthat secure  development  practices are implemented and artifacts are created to attest to the \nadherence of the adopted Secure SDLC  plan with res pect to the implementation and distribution of \nthe product .  \nA development team is comprised of experts  in development, quality assurance (QA), build \nengineering, and security. The product  management  team  is comprised  of individuals with product  \nleadership  experience , and includes  product  and development  managers, security  architects , and \ncompany -level quality  control  assessors, all contributing to product  release oversight .  \nThe top -level organizational management  team  must ensure  secure  development  policies and \nprocedures are supported  within  the budget  and schedule  and are implemented  and adhered  to by \nthe assigned  development  team s. The figure below outlines a secure development process and \nlifecycle . \n \n10 Cisco Secure Development Lifecycle  \n11 Synopsys Secure Software Development Lifecycle  Phases  \n12 US-Cert Secure Software Development Lifecycle Processes  \n13 Secure Software Development Fundamentals Courses  \n\n \nSecuring the Software Supply Chain: Recommended Practices for Developers  5 \n  \n \nFigure  2: Secure Software Development Process  (DoD Chief Information Officer, 2021)14 \nThe example process  illustrated above  ensure s that secure, resilient  products are developed . It also \nillustrates that the development  process  can be measured  using well -defined , tangible  artifacts  that \nmay  be collected, evaluated, and recorded to validate the use of the documented  secure  principles  \nand guidelines  outlined  by the product management  team.  \nThreat  scenarios  \nWhen  developing and delivering  a product,  the following common  threats  may  occur  during  the \nsoftware development lifecycle : \n1. Adversary i ntentionally injecting malicious code or a developer  unintentionally including \nvulnerable  code  within  a product. \n2. Incorpor ating vulnerable  third -party  source  code  or binaries within  a product  either  \nknowing ly or unknowingly. \n3. Exploit ing weaknesses  within  the build  process  used  to inject  malicious  software  within  a \ncomponent  of a product . \n4. Modif ying a product  within  the delivery mechanism,  resulting in injection of  malicious \nsoftware  within  the original package, update , or upgrade bundle  deployed by the customer.  \nFor more information on each threat scenario  refer to Section 2.2 Develop S ecure Code  through \nSection 2.5 Deliver Code . These  sections contain  more details  for the threat scenarios  and define  \nstrategies  for each  of the types of incidents or compromises that  can occur  during the development  \nand release of a product.   \n \n14 DoD CIO Enterprise DevSecOps Fundamentals, Version 2.0, March 2021  \n\n\nSecuring the Software Supply Chain: Recommended Practices for Developers  6 \n \n \n Recommended  mitigations  \nThe supplier and developer management  team should  set policies  that  ensure  development  \norganizations  have  security -focused  principles and guidelines  in place  to: \n Generate architecture and design documents , \n Gather a trained, qualified , and trustworthy  development team , \n Create threat models of the software prod uct, \n Define and implement security test plans , \n Define release criteria and evaluate the product against it , \n Establish product  support and vulnerability handling policies and procedures , \n Assess the developers capabilities and understanding of the secure development process  \nand assign training , \n Document and publish the security procedures and processes for each software release .  \nArchitecture and design documents  \nArchitecture and design  documents should be  based on  customer  and marketing  requirements  that \nhave  been  gathered, correlated, and prioritized. S pecific security -related assessments and \nreliability criteria  derived  from  operational customer  environments  and known  product risk \nassessments  should be included in the requirements . The requirements  should take  into  account  \nsecurity  criteria  for specific industries such  as NIAP, FedRAMP,  HIPAA,  or FIPS -140  and that are \nbased  on Zero T rust  principles . Architecture and design documents should address all \nrequirements defined and describe the components, interfaces used , and functionality needed to \nimplement the product in various levels of detail based on the needs of the development group.  \nThe development team  \nMembers of the development  team  should be  trained  and qualified  to perform  the security \ndevelopment  tasks  outlined  in the architecture  and high -level design  document.   \nThreat models  \nImpartial, senior -level security architects and developers should create threat  models  of the \nproduct under  development . These personnel should be  familiar with identifying trust  boundaries,  \nrelationships,  and inflection  points  where  data  or systems  might  be compromised.  Threat  models  \nshould  be developed  for all critical  software  components,  as well  as for all critical  systems in the \nbuild pipeline .  \nAll code and systems involved  within  the build  pipeline  should  be reviewed  on an ongoing  basis  \nagainst  the associated  threat  model . Changes  should  be made as needed  to ensure neither the code  \nnor systems have  structural  vulnerabilities.  Threat  models should  further be: \n Updated  as functionality changes,  for major  releases, or minimally  at least  annually, \n Made available  to other  internal  engineering  teams  that are picking  up or operating  any \nassociated  software  components  or systems.   \n\n \nSecuring the Software Supply Chain: Recommended Practices for Developers  7 \n  \nManagement  policies  should  also  specify  that developers assigned to create the threat model s use \ncomponent -level designs for completeness. Models  should  be reviewed  and approved  by at least  \ntwo independent  engineers  on the team  and evolve  as architectural and design  changes  occur . The \nthreat model process needs to be  adaptive when organizational policies and procedures change.  \nSecurity test plans  \nAn impartial Q uality Assurance ( QA) individual , team, or an impartial entity with QA expertise \nshould define and implement security test  plans .  \nA QA team  is comprised  of automation  and build  expert(s) who leverage modern techniques \nrequired  to apply  secure  testing strategies  for all components  defined  within  the architecture  and \nhigh -level design  documents.   \nDevelopers  should  perform  unit - and system- level security  tests that are validated  by QA. This \nallows QA to perform  further  security  testing to cover  a broad er and deeper  set of tests  with  less \nduplication of effort . The strategies  defined  within  the test  plan  should include: \n Code coverage,  which  is integrated into  each  build  and tracked  as part  of implementing  the \ntest and development  plan , \n Baseline levels  of code coverage should  ideally be achieved  on all code that  is checked  in, \nbefore  new  code  is committed , \n Policies should  be defined  to maximize  code  coverage  and address the SSDF  tasks  defined  in \nPO.2.1, PW.5.2  and PW.8.2  of the National Institute of Standards and Technology ( NIST ) \nSpecial Publication (SP)  800 -218 Standard , \n Test coverage should  identify  the percentage of code  paths  the test plan  covers  as well  as \nthe types  of test tools  used . \nWhen release readiness  criteria  are defined , they may  include  requirements  for the following  types  \nof tests: \n Static  and dynamic  application security  testing (SAST  and DAST)  should  be performed  on all \ncode prior  to check -in and for each  release using  a standard set of company -approved  tools.  \nResults  of testing  should  be documented,  and all discovered  vulnerabilities  should  be \nanalyzed  and addressed ,  \n Software  Composition  Analysis  should  be performed  on all third -party software  to include \nreview  against  the MITRE  Common  Vulnerabilities and Exposures  (CVE)  and the NIST  \nsoftware  security  vulnerability bulletins.  (NIST  SSDF  PW.3.2), \n Fuzzing  should  be performed  on all software  components  during development  to ensure \nthat they  exhibit  expected  behavior  with different inputs.  Results  should  be documented,  \nand any anomalies or  vulnerabilities  should  be addressed ,  \n Where possible, plan to employ memory -safe programming languages to mitigate a large \nportion of the most common exploitable vulnerabilities , \n For many types of software products including security software  and general- purpose \noperating systems, many government customers may require independent lab testing \n\nSecuring the Software Supply Chain: Recommended Practices for Developers  8 \n \n \n against a Nati onal Information Assurance Partnership ( NIAP ) Protection Profile (NIAP  \nCCEVS) ,15 \n Verification  should be done to ensure that applicable anti -exploitation features are \nleveraged in development depending on the platform on which the software will operate. \nSuch features complicate or prevent exploitation of many classes  of unforeseen \nvulnerabilities. The Application Software  Protection Profile v1.416 includes requirements \nfor Anti -Exploitation Capabilities under  FPT AEX Ext.1 and is available at niap -ccevs.org \nunder NIAP -Approved pps, \n Penetration  testing should  be done  as routinely as possible, but not less than once per  year , \ndepending on potential risk (e.g., cloud  products  should  be pen-tested  more  frequently) , \n Use a testing approach that considers only externally visible behavior of the product without knowledge of the code, nor the inner working of the software to assure that repaired vulnerabilities are truly fixed against all possible compromises . \nThe r esults  of all security  testing  should  be documented , security defects should be fixed,  and a \nsynopsis  of the test results should be made available to customers. This synopsis should i nclud e any \nCommon Vulnerability Scoring System ( CVSS ) scores . The QA results should  be used  as one of the \nmeasurements  of a product s readiness  for release. \nRelease criteria  \nThe management team  should  establish , manage,  and apply  release criteria  and evaluate whether  \nthe product satisfies  the criteria.  The criteria  should  include: \n No unacceptable security vulnerabilities found when performing all required threat \nmodeling and testing are  pending , \n Cybersecurity  hygiene  of the development  environment  was  maintaine d during  \ndevelopment, as described  in Section  2. Developer , and the relevant  artifacts  were  \ncollected and securely  stored  for future  reference, \n Products were  developed  following  the secure  software  development  practices and tasks  \nset by the organization , and relevant  artifacts  were  collected and securely stored  for future  \nreferences. Examples  of the artifacts  are the design  and architecture  documents  (ex. system \nand software  component  data  flow, UML model), the threat  model, verification and  test \nresults, revision  history  of software  design , all the components , and a list of open  issues and \nknown  vulnerabilities , \n Produce, correlate, and validate a Software Bill of Materials (SBOM ). Contents  of the SBOM  \nare described  in Section  2.3.5  Threat scenario: s oftware bill of m aterial s (SBOM)  and \nthe National Telecommunications and Information Administration ( NTIAs ) The Minimum  \nElements for a Software  Bill of Materials  (SBOM) ,\n17 \n The product  management  team  ensures  that all released  binaries are digitally  signed  with  a \nkey associated  with  a root  certificate  from  a trusted  certificate  authority , \n \n15 https://www.niap -ccevs.org/Profile/PP.cfm  \n16 https://www.niap -ccevs.org/Profile/info.cfm?ppid=462&id -462 \n17 https://www.ntia.doc.gov/report/2021/minimum -elements -software -bill-materials -sbom  \n\n \nSecuring the Software Supply Chain: Recommended Practices for Developers  9 \n  \n All released software meets  company -wide  crypto graphic  standards. These  standards \nshould  be based  on relevant  industry best  practices  or (for federal agencies) applicable \ngovernment standards  such  as NIST  SP 800-175B ; Guideline for Using Cryptographic \nStandards in the Federal Government: Cryptographic Mechanisms  and be enforced  with  an \nappropriately  defined  responsible, accountable, consulted, and informed  (RACI ) matrix , \n All shipping of open source meets  company -wide  standards including vulnerability \nassessment  of the source and  this information is made available to development  groups . \nShip the latest  stable versions  of open  source, removing  or providing  a support  plan  for any \nopen source software that  has reached e nd of life, and ensuring  licensing,  if any, is fully \nunderstood  and compliant  with the open source usage policy . \nProduct support and vulnerability handling policies  \nThe management team defines the product  support  and vulnerability handling  policies and \nprocedures as they address the entire  lifecycle of the product from  conception  to end of life (EOL).  \n Using a vulnerability submission system, all known  security  issues and  vulnerabilities \nshould  be collected and tracked  as product  defects  in the organizations  defect  tracking  tool . \nThis includ es common weakness enumeration ( CWE ) and CVSS  scores,  specific impacts on \nthe component,  and any other  relevant  supporting  data.  Vulnerability information  should  \nonly be stored in access -controlled pages  in the defect tracking  system,  given  the potential  \nsensitivity ,  \n The organization  should  have  a central company -wide  The Product Security Incident \nResponse Team  (PSIRT ) that  supports  a public -facing  reporting tool  (for example  a web  \npage)  that  makes  it easy  for external researchers  to report vulnerabilities  in the \norganizations  products.  The PSIRT  team  should  work  with  external researchers  to \nacknowledge  and gather  information  on any reported vulnerabilities,  and to ensure that  any \nreported vulnerability is fixed.  Organizations  should  practice  responsible  disclosure on all \nvulnerabilities , \n Updates  to all in-field  software , including patches  and product updates  should be delivered \nusing  a secure  protocol like  HTTPS/TLS.  The in-field  software  products  should  perform  \nintegrity  or signature  checks on all delivered  files  to ensure the files  are valid.  This applies  \nto delivering  updates  to both  on-prem ise and in-the-cloud  software  products.   \nAssessment and training  \nThe management  team  defines  policies  and procedures  used  to assess  developers capabilities and \nunderstanding of the secure  development  process.  These  policies  should  address: \n Who requires training, \n How frequently they must train ,  \n Who is authorized to conduct the training , \n The training topics, \n How to evaluate the trainees ability to meet the standards established by the training .  \n\nSecuring the Software Supply Chain: Recommended Practices for Developers  10 \n \n \n Security  training for the develop ment team  is ideally conducted  by a centralized , expert  security  \nteam  who  can help  product  teams  grow their  expertise  in secure  development. It also provides  \nengineers  a point  of contact  when they  have  specific security questions.   \nThe training should  include: \n Secure  software  development  and design , \n Secure  code reviews , \n Software  verification  testing , \n Use of security  and vulnerability assessment  tools  during  development . \nDevelop ers should  take  regular and relevant  security  training,  both for common topics and those  \ndeemed  necessary  for the individual  role. Successful completion  should  be tracked  for all engineers . \nOrganizations  should  ensure  individuals complete  security  training commensurate  with  the impact  \nlevel of the system and software  to which  the individuals are assigned.  \nEngineers  within  the development  organization  should  also  be required to take  annual  training of \norganization -approved  cybersecurity best  practices.  An example of this training  would  include  how  \nto spot  suspicious emails  and the point of contact for reporting a  suspected  breach. This  training \nshould  include  a test at the end of the course  to ensure understanding  of the material. See also  NIST  \nSP 800 -50, Building an Information Technology Security Awareness and Training Program, for \nmore information on how training should be conducted and measured .  \nIndividuals within a d evelopm ent team should be evaluated periodically, at least annually, to \nmeasure their knowledge and compliance against product security goals. At a minimum, this should \nbe a representative survey, displaying a team s or individuals  awareness of required corporate \ntraining, and any artifact s that attest to compliance with policy. Gaps should be examined to \ndetermine and address root causes, e.g., if there is a lack of usable tools to implement organizational security expectations.  \nSecurity procedures and processes  \nThe management  team  documents  the security  procedures and processes.  These documents should  \nbe reviewed, updated , and to the extent possible , made  publicly available for each  software  release. \nThis must be done without  divulging sensitive  security  information  about  the product. These are \nliving documents,  which  are reviewed  both when questions  arise  during the development  of the \nproduct and after  the product has been  released  in a formal  after -actions report or lessons -\nlearned  session  with  all members involved  in the secure  development  process . \nAlignment  with  SSDF  \nThe mitigations provided in this section align w ith the activities found in  NIST SP 800 -218 , Secure \nSoftware Development Framework (SSDF) Version 1.1: Recommendations for Mitigating the Risk of \nSoftware Vulnerabilities. The following table alig ns tangible development activities with the SSDF \nrecommendations : \n  \n\n \nSecuring the Software Supply Chain: Recommended Practices for Developers  11 \n  \nTable 1: Mitigation alignmen t with SSDF  \nMitigation   Activity in SSDF v.1  \nArchitecture and design \ndocuments   PO.1.1, PO.4.1, PO.4.2, PW.4.3, \nPW.3.1 \nDevelopment team  trained in \nsecure development   PO.2.2, PW.1.1, PS.1.1, PS.3.1, \nPW.4.2, PW.4.3, PW.5.1, PW.5.2, PW.6.1, PW.6.2, PW.7.1, PW.7.2 \nThreat models   PW.1.1 \nSecurity test plans   PO.3.1, PO.3.2, PO.3.3, PO.4.1, PO.4.2, PW.4.3, PW.5.1, PW.5.2, PW.6.1, PW.6.2, PW.7.1, PW.7.2, \nPW.7.2, PW.8.1, PW.8.2, PW.9.1, \nPW.9.2, RV.1.1, RV.1.2, RV.3.3, \nRV.3.4 \nDocument r esults with CVSS \nscores; verify security defects \nare fixed  RV.3.2, RV.3.4, PS.2.1, PS.2.2  \nProduct release  Deliver testing  and threat \nmodel documentation, \nvulnerability reports, and \nSBOM.  PS.2.1, PS.2.2, PW.2.1, RV.1.2  \nSupport channel to report  \nflaws . RV.1.1 \nDigitally sign shipping \nbinaries with key and trusted \nroot certificate  PS.2.1  \nProduct support  Track known security \nissues/vulnerabilities  RV.1.1, RV.1.2, RV.1.3  \nIncident response with public -\nfacing reporting tools, fix \nreported items and disclose  RV.1.3, RV.2.1, RV.2.2, RV3.1, RV.3.3 \nUpdate  in-field products  PS.3.2  \nAssess ment and training of \ndevelopers   RV.3.4 \nSecurity procedures and \nprocesses for each release   RV.2.2 \nCryptographic and third -party \nsoftware integration standards   PW.3.2, PW.4.1  \nNote  SSDF Activity Codes : PO  Prepare Organization; PW - Produce Well -Secured \nSoftware; PS  Protect Software ; and RV  Respond to Vulnerabilities . \n2.2 Develop  Secure C ode  \nSource  code development  involves  reviewing  the approved  product requirements  and design  \ndocuments and implementing  all required  features  and functionality . This  should be done \naccording to  the policy  and procedures for writing  source  code  and in a specified  computer  \nprogramming language  (e.g., C++, Java, Python , RUST, etc.) as specified in SSDF PO.1.1, PO.2.2, and \nPW.1 of NIST SP 8 00-218. \nCare should  be taken  when  there  is an opportunity  to select  the programing  language  to be used  for \ndevelopment, considering  whether  the language  is statically  or dynamically typed,  and what  \n\nSecuring the Software Supply Chain: Recommended Practices for Developers  12 \n \n \n protections  are inherently  built  into  it to mitigate  vulnerabilities  and provide  memory  and thread \nsafe operations. Secure  software development follow s the principles outlined by Saltzer and \nSchroeder  in The Protection of Information in Computer Systems ,18 which include:  \n Open  design , \n Fail-safe  defaults, \n Least  privilege, \n Economy  of mechanism, \n Separation  of privileges , \n Total  mediation , \n Least  Common mechanism, \n Psychological  acceptability , \nDevelopers  may  also  integrate  common  core  libraries and reuse  trusted modules  which  have  \nalready been  vetted by the organization  as defined  in SSDF PW.4. In many cases these  guidelines \noutline  the approved  security  settings for compilers  and the deployment of standardized \ndevelopment  environments  and tools  as specified  in SSDF PO.3 and PW.6. Source  code will typically \nbe version  controlled and managed  in a source  code  control  system following  the guidelines in \nSSDF PS.1.1 and PS.3, and developers  may  be required  to perform  peer -reviews  of their  source  \nprior  to allowing  code  to enter  a main  repository as specified  in SSDF PS.1.1 and PW.7. At times,  \nengineers  are required to compare  and merge  changes  across  code  lines  and repositories  to \nmanage  source  code  properly in a distributed team  model. \n Modification  or Exploitation of  Source  Code by Insiders   \nThe Cybersecurity and Infrastructure Security Agency  (CISA) defines insiders as any person who \nhas or had authorized access to or knowledge of an organizations resources, including personnel, \nfacilities, information, e quipment, networks, and systems .19 \nCISA defines i nsider threat as  the p otential for an insider to use their authorized access or \nunderstanding of an organization to harm that organization. This includes intentional as well as unintentional acts.  \nSoftware  development  group  managers should ensure  that the development  process  prevents  the \nintentional  and unintentional  injection  of malicious code  or design  flaws  into  production code.  \nSource  code modifications can occur  at the developer  level in one or more  of the following  \nscenarios:  \n When an engineer is compromised by outside influence or dissatisfaction , \n When an engineer is poorly trained , \n When engineers put backdoors into a product , \n When remote development systems are not secured or when protections are removed , \n When accounts and credentials for terminated or inactive personnel remain available. \n \n18 http://web.mit.edu/Saltzer/www/publications/protection/index.html  \n19 https://www.cisa.gov/defining- insider -threats  \n\n \nSecuring the Software Supply Chain: Recommended Practices for Developers  13 \n  \n1. Compromised e ngineer s \nThe c ompromised  engineer  is a difficult  threat  to detect and assess.   A compromised employee \nmay be under pressure from outside influences or may have a grudge to avenge. . P oor \nperformance review s, lack of  promotion , or disciplinary  action s are only a few of the events  that \nmight  cause  a developer  to take  action  against  an organization  and sabotage its development  \neffort.  Additionally, n ation  state s or competitor s can  leverage an insiders  struggles  with  \ncontrolled substances,  failing relationship s, or debt , among other things . \nBecause a developer  has inside  knowl edge  of the code  base  and is often an expert  in their \nrespective coding language, environment, and style, developers can  design  subtle  \nvulnerabilities  that  are very  difficult  to detect.  In addition,  access to design  details  not publicly \navailable can provide inside  knowledge  of weak  architectur e or code areas  that  contain  security  \nweaknesses  that may  be exploited. \nOnce  implemented  and injected  into  the build  infrastructure, built -in vulnerabilities  are \ncompiled,  signed , and hashed, allowing  the high- level security  validation checks  to pass  without  \nany indication  of compromise.  \n2. Poorly trained  engineer s \nEngineers who have not been  properly trained in security  design  and coding practices can \nunintentionally i ntroduce  vulnerabilities  within  source  code  that,  once  submitted  into  a source  \ncontrol  repository, can be difficult to detect.  The type  of vulnerabilities can range  from  buffer  \noverflows  to logic  flaws , the latter being harder to discover . These  \"zero -day bugs\"  can reside in \na product for a long time and are instrumental in providing an easy compromise vector for \nadversaries that discover them.  \n3. Ease  of develo p ment feature s (backdoors) \nDevelopers  will sometimes  add debugging  features  within  a product  to facilitate  the \ntroubleshooting , setup , or problem-reporting processes  commonly  performed  before initial  \ndevelopment. These  features,  in many cases, are privileged operations  that allow  the \ndevelopment  team  to obtain  statistics and logs  and issue  remote  commands  to reconfigure  the \nsystem under  development.  While  these  developer features  can be helpful, often  they  are tightly  \nintegrated  into  the product s core  component s, making them hard to remove . In some  cases , \nthey cause  components  to be extended  to facilitate  the tools  and features  being  used  but not \nformally designed within the product.  \nThese  features  are often planned  to be completely removed  before  release, but in some \nsituations , they are not removed due to the core  component  integration  and the level of work  or \nrisk  involved  with removing them near  the scheduled  time  of product  delivery.  These  features  \ncould be disabled upon  release, but when left in the shipp ed product  they create risk of \ndiscover y and exploit ation . Another ease of development concern is when only one portion or \nfunction  of an application requires elevated privileges, but the entire  application  is configured \nto run with the privileges required to perform the single task. Privileges should  be raised  to \ncomplete  specific functions  and then  immediately  lowered to reduce  the attack  surface.  \n\nSecuring the Software Supply Chain: Recommended Practices for Developers  14 \n \n \n 4. Compromised remote  development  systems  \nA common practice within the development environment is allowing remote  development  for \nemployees  or contracting off-site third -party  developers. Many of these  remote  developers  \nwork from home or a satellite office and  use organization -supplied  machines and resources \nconnected  over  a VPN.  When  using  this environment,  the remote  office  becomes  an extension  of \nthe organization s network , and the developer  has access to all the development  resources  \nnormally associated  with  a standard work  environment  to include  creating,  compiling , and \nchecking in source  changes.   \nWhile  there  are many benefits  to facilitating  this work  environment,  remote work  come s with  \nrisk.  The home  or remote  office  network  may not  provide  the same  level of network  protection  \nas a companys  on-premise  facility.  In addition,  remote  employees  may  be more  tempted to use \nrestricted  network -based  applications  for social  media,  web surfing, games,  and in some  cases  \nremoving  local  computer  protections  to facilitate  their  use.  In this environment,  these  systems \ncan become  compromised,  allowing an adversary  to use backdoors  within the remote  \nenvironment  to access and modify  source  code  within  an otherwise  protected  organization  \ninfrastructure.  \n5. Use of lingering account s or credentials  of a terminated or inactive user  \nWhen employees have been terminated, reassigned to another project, or away from work for \nan extended duration, their privileges and accounts often remain operational, and may be used to perform malicious activity without the account owners knowledge.  In this case, the owner  of \nthe account  is not monitoring its use and is unaware of any malicious activity  performed with it. \nUnauthorized  use of accounts in this way grants access  to all the development  resources  \navailable to  the original  account  owner.  \nRecommended  Mitigations  \nSpecific processes  may  help mitigate the risk of  intentional  or unintentional  injection  of malicious \ncode in a development  project,  including: \n Implementing a well -balanced authenticated source code check -in process , \n Performing automatic static and dynamic security/vulnerability  scanning, \n Conducting nightly builds with security and regression tests , \n Map features to requirements , \n Prioritize code reviews and review critical code , \n Secure Software Development/Programming T raining, \n Harden the Development Environment. \n1. Implement a well -balanced authenticated source  control c heck -in process  \nFundamental  to the protection  of the source  code repository  and its contents  are the methods  \nused  to control  access to it and the validation process  used  to ascertain  whether  a check -in is \ngood . Access and validation  start with  good  source  code management  (SCM)  principles  to \ntrack  modifications  to a source  code  repository. Such principles include  a running  history  of \nchanges  to a code  base  and resolv ing conflicts when  merging updates  from  multiple \ncontributors.  As an example , the acquisition processes for free and open source software , \n\n \nSecuring the Software Supply Chain: Recommended Practices for Developers  15 \n  \ncommercial off the shelf software  source components , and the management of a secure \nsoftware repository  are outlined in Figure 3.  \n \nFigure  3: Secure Repository Process Flow  \nThe secure repository should initially and continuously look for new vulnerabilities and \nupdates within the added components . A log of all developers and the components they \ndownload should be kept . If a component becomes flagged due to a new vulnerab ility  or update \nin the future, the developers who have downloaded the component should be automatically notified to address the issue . In this manner, when new vulnerabilities arise, it will also be \nevident which programs/projects are affected.  \nAt a minimum,  the source  control  system should  be protected  using  industry recognized \nmultifactor authentication ( MFA), not only to log check -ins, but for all access  to the secured  \nrepositories.  When  check -ins are made, an audit  trail  is created  that  logs  the MFA  deve loper  ID, \nfiles modified , and date  and time  of the check -in. Depending  on the complexity,  security  \nrequirements,  development  resources  available, and time  constraints, consider the following  \nwhen implementing  a well -balanced  source  control  check -in process : \nPeer/l ead review \n Allow n o code that has not been peer or lead reviewed to be check ed-in to a source \ncontrol repository, \n Require comments listing the  relevant requirement for the check -in, \n Include the MFA ID of the reviewer  and the reason  for the modification  of the \nsource , \n Include  any cross -development  dependencies  on another  development  effort  co-\ndependencies should never be checked -in separately , \n\n\nSecuring the Software Supply Chain: Recommended Practices for Developers  16 \n \n \n  Perform unit  and security  tests . \nWorking  and production  branches  \nTo control  the quality  of the produced software, two or more  branches  of the development  \ntree  are maintained.  During  the normal  software  development  process , all code  can be \nstored in the working,  general purpose,  development  branch . As a component  development  \neffort  evolve s, the source  supporting  a delivery feature is coded, tested,  and reviewed  by \nsenior  engineers , and the functions  and requirements  of the component  are cross -\nreferenced . This ensures the feature set is met  and nothing exists for any feature creep. \nApproved  code  is moved  to a production branch  by a development  integrity  assurance  team  \nmade up of senior  level engineers,  build  engineers  and designers. The production branch,  \nsometimes referred to as the release branch,  is used  as the sole repository  from  which  \nrelease product  is built.  This  branch  should  be protected  with  reviewers  and continuous  \nintegration /continuous  deliver y (CI/CD)  tests  with  SAST  enforced  at the SCM.  The process  \nflow  for branch  readiness and transfer  could  be summarized as:  \n1. Developers  work  in the development  branch . \n2. Leads  promote  software to a QA branch  when  source  is code reviewed  and \napproved . \n3. QA individuals/teams test from  QA branch . \n4. Once  integrated  code  is tested and approved  it is moved  into  the production branch . \nAccess to the production  branches  is restricted to a small  set of build and  development  team  \nmembers. All builds  used  to create production products  are created from  the production branch  \nof the repository. Once  a product is released, the product  branch  should  be labelled and locked  \ndown  with  restricted customer or  read-only access.  The implementation  of this lockdown \nprocedure ensures  secure  and reproducible builds.  \n2. Perform static and d ynamic  security /vulnerability s canning  \nPerforming automatic static and dynamic vulnerability scanning on  all components of the \nsystem for each committed change is key to  the security  and integrity  of the build  process  and \nto validate the readiness of product release. This  automatic scanning  can perform code  analysis  \nto determine if restricted a pplication programming interfaces ( APIs ) that  contain  \nvulnerabilities  such  as a buffer -overflow  or memory  leakage are used  within  the source  under  \nevaluation, as well  as other  security  related  scanning.  Perform ing static analysis to scan for \nsecrets before commit and during CI/CD block s secrets from the code base. The  complexity  and \nthoroughness  of these  static  scanning technologies  vary greatly . These tools should be used by \ntest teams as well as locally used by the development engineer . Most secure  development  \nprocesses recommend this practice .  \nSeparate and higher  quality  scanning tools should  also  be used  within  the product build  \nenvironment.  It should  also  be noted  that static  analyzers  work  better on statically- typed  \nlanguages  such  as C++, since  the type  of variables  used  within  the code  are known  at compile  \ntime,  whereas  dynamically -typed  languages  such  as Python resolve the variable  types  at \nruntime.  As functions and components are completed and able to be executed, dynamic testing \ncan find additional security weaknesses . These are often user input errors or malicious  \ninjections and can only be identified during testing at runtime.   For web  applications, Interactive  \n\n \nSecuring the Software Supply Chain: Recommended Practices for Developers  17 \n  \nApplication Security Testing ( IAST ), Dynamic Application Security  Testing ( DAST ) and Runtime \nApplication Self -Protection (RASP) tools should be  used , as specified in NIST 800 -53 v5 .20 \nBecause IAST tools tend to have far more false positives  than SAST, particularly with web \napplications , SAST tools that use introspection are encouraged  when implementing the security \ntesting requirements within this environment.  \n3. Conduct n ightly  builds  with  security and r egression  tests \nTo ensure  the integrity  and quality  of the development  process,  nightly  builds  should  be \nperformed  that  include manual and automated  security  and regression  tests.  Test  cases should  \nbe implemented  during  the design  of the software  and extended  during  coding  to validate all \nareas  of functionality  for both  good  and bad  scenarios.  Using this process,  any flaws  or \nchanges,  whether malicious or inadvertent, can  be recognized  and addressed.   \nThe nightly build s with regression test s should be  implemented by a QA  engineer and \nincorporated into the build environment by a build engineer.  This is different than  the case of a \ndevelopers own automated  unit  test,  which  may  run manually during coding and automatically  \nduring the building of the component  and for which  the developer is generally responsible.  \nArtifacts  such  as logs  and automated  email  notifications  sent when regression  tests fail help \nnotify  and track  where  and when problems arise. The nightly build  process  also  acts  as a good  \nperformance matrix  to assess  the developer  capabilities and comply  with  security  and \ndevelopment  processes.  Refer to S ection 2.4 Harden the B uild Environment  for more details \nrelated to production builds of the product as compared to local development builds \ndocumented in this section.  \n4. Map f eature s to requirements  \nIt is important that all components and functionality of a product are architected and designed \nto interact with the system using secure design practices, including threat modeling and attack \nsurface analysis. Once all security risks are identified and mitigated, architecture and design \ndocuments are finalized and disseminated to development groups f or implementation. Low -\nlevel design and functional specifications are created that map directly to the given architecture \nand high -level design, and development task s and schedules are mapped out. During the coding \nand implementation of the system, care must be taken to ensure that all development  efforts  \nmap to specific system requirements and that there is no feature creep that might \ncompromise product integrity or inject vulnerabilities.  \nFormal, informal , and peer  review s help ensure that code added to the repository meets specific \nrequirements and only those requirements. These reviews can also  identify modules and \nunused code that are included as part of a larger package or component feature. When possible, \nonly required modules should be  included in  the product .  \nAdditionally, developers should r emove unused modules and code that is out of scope of the \nrequirements and design document. Restrict ing the addition of developer tools, like those that \naid debugging, configuration, and monitoring, to only t hose approved within the design of the \n \n20 https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800- 53r5.pdf  \n\nSecuring the Software Supply Chain: Recommended Practices for Developers  18 \n \n \n system also mitigates the attack surface . Report ing and address ing feature creep as soon as \npossible  helps as well. \nIn addition, the build environment should support the scanning and detect ion of all plug -ins \nwithin the system. The results should be cross -correlated  with an allowed list to ensure \nunauthorized components havent been added. These scans produce artifacts that describe \nwhich components and software features are included in a product, and , more importantly, \nensures that all components have undergone analysis so  each components  risk is understood \nand documented.  \n5. Prioritize code reviews and review critical code  \nCode reviews are performed using two different processes, formal and informal, and are \nimplemented during different stages of the development lifecycle .  \nCode reviews should be  prioritized, and , at a minimum, the most critical code should be \nidentified  and reviewed  using both the formal and informal review processes. C ritical  code \nincludes c omponents that : \n Use or provide  crypto graphy , \n Require  privilege  escalation ,  \n Access protected resources ,  \n Are essential to the  purpose of the software , or \n Have a high  percentage  path  flow , among other factors . \nIn addition to formal  and informal  code  reviews, automated  code  review  tools  should  be \ndeployed to provide  full code  review  coverage.  \nThe informal  code review  process  is used  by the internal  development  group  when measurable \nstages  or checkpoints are achieved  during development. The informal review  is used  to ensure  \nsecure  coding policies and procedures are being  followed  and that the source  under  review  \nmeets  the design  and functional  requirements  documented  within  the low- level  design  of the \ncomponent.  These  reviews  are conducted  by members of the development team  to include  \nproject  leaders  and senior  developers  and ensure  security  and integrity  during development  of \nthe specified  component . While informal, the proces s for conducting code reviews produce s \ndocu mentation  on the areas  reviewed  and the results, which  can be used  to help measure a \ndevelopment groups  performance. I nformal code  reviews are also used as a training tool to \nprovide awareness to group members on how to implement secure coding practices. \nFurthermore, informal code  reviews  can be used  to measure  the quality  of the code review  \nprocess  and developer  deliverables being  produced. \nFormal  code reviews  ensure the secure integration of key components using best practices . \nThese reviews  focus on  the integrity of a syste m, design , and architecture, while addressing all \nfunctional, security , and reliability concerns. They  also identif y any areas of concern that may \nbe exploited or compromised.  \nEngineering groups, including  members of QA, build experts, designers , and architects, usually \nconduct formal code reviews. In formal code reviews, before review of the source, t he owners  of \nthe component  describe the component s functionality  and its interaction  with  other \n\n \nSecuring the Software Supply Chain: Recommended Practices for Developers  19 \n  \ncomponents  in the system.  A designated individual takes detailed  notes   that  track  the review  of \ncore component s of the product.  \nThe outcome of the formal code review is  a list of activities  and concerns  that must  be \naddressed prior  to shipping  the product.  Formal  code reviews  aid the development effort by \nshar ing knowledge  of the product design and implementation which enhance the  build  and \ntesting  requirements  of the development  environment . Formal code reviews also allow  the \nproduct management team  to measure  the products readiness  for release , ensuring it meet s all \nrequirements  prior  to release.  \n6. Secure  Software  Development/Programming  Training   \nAs outlined  in Section  2.1, Recommended Mitigations , subsection Assessment and \ntraining , developers  should  continually take  training  to understand  the secure  development  \npractices required for corporate  development. This  resource must  also  be made  available \nduring the development  lifecycle  to allow  developers  to contact  experts  and address concerns  \nor questions  they  may  have  about  a specific  security  issue,  practice , or vulnerability.  During  \nday- to-day development  activity,  access to security  experts  should  be used  to facilitate the peer  \nand source  code reviews  and to  train  individuals  on corporate security  practices and evaluate \ntheir knowledge.  A well -implemented  peer  review  process  allows engineers  to prevent  defects,  \nminimize  security  weaknesses, and promote  team  collaboration and knowledge -sharing.  \n7. Harden  the Development  Environment   \nAs with  the build  environment,  the development  environment  must  be hardened. The \nmitigations defined  in Section  2.4 H arden  the B uild Environment  also  apply  to the \ndevelopment  environment . However, while  the production build  systems are generally located  \nin a protected segmented  network  with  limited  access,  the development  build environment  may  \nbe housed in endpoint systems that are less isolated . For example,  many development  \nenvironments  allow  remote  VPN  access  to the internal development machines  to facilitate  \nremote  workers  and allow them  to participate  in development  activities.  In such  situations,  \nwhen connecting to a corporate  development  environment  from  a remote  location,  a VPN  and \nMFA must  be used  to protect  the development  environment.  Endpoint  security  software  should  \nbe installed  to prevent,  detect,  and respond  to threats  against  the host  system.  In addition to \nVPNs,  implementers  should consider  the use of a jump -host , a system which  acts  as a portal  \nbetween  a less secured  remote  host  and the protected  development  environment.  This allow s \nall activity  to be continuously  monitored  and providing protection  and/or  limited  accessibility  \nand operational privileges  for the remote  developer. A threat  model  and vulnerability \nassessment  are required  for all development  environments  associated  with  product \ndevelopment.  \n Open  Source  Management  Practices   \nDevelopers commonly use open source code  in application  development,  with  projects  potentially \nhaving  multiple dependencies  on open source  libraries  which may contain vulnerabilities.  \n\nSecuring the Software Supply Chain: Recommended Practices for Developers  20 \n \n \n Recommended  mitigations  \nDevelopment organizations should e mploy  dedicated  systems that download,  scan , and perform  \nrecurring checks of open source  libraries for new version s, updates,  and known  or new  \nvulnerabilities  (see Figure 3 above) . As with  all software, we strongly  recommend  educating  \ndevelopers  on considerations for the use of  open source software, close -source  software, and \nevolving  best -practice  mitigations. Please  refer  to the SSDF  for more  details , specifically PW.4.1, \nPW.4.4, PW.4.5 and PO.1.3 .  \n Secure  Development  Practices   \nManagers  of a software  development  group  should  ensure  that  the development  process used  to \ngenerate,  test, and preserve  source  code are accomplished  using well -defined  and secure  practices. \nThis helps establish trust  in the engineering  tools -chain  and procedures  used.  These practices \naddress the following  security  concerns:  \n Secure  developer environment , \n Use secure  development  build  configurations , \n Use secure  third -party  software  tool -chain and compatibility libraries. \nRecommended  mitigations  \nThe following are recommended  activities to implement  secure  development  practices:  \n1. Secure  the developer  environment   \nWhen  ensuring  the integrity  of the development  environment,  care  must  be taken  to harden the \ndevelopment  systems within  the build  pipeline  as defined  in Section  2.4 Harden the Build \nEnvironment . In addition, all development  systems  must  be restricted  to development  \noperations  only. No other activity such  as email  should  be conducted for  business  nor personal \nuse.  If possible,  development  systems should  not have  access  to the Internet  and may  be \ndeployed as local  virtual  systems  with  host -only access.  All tools  installed on development  \nsystems must  be pre-approved,  to include  debuggers, test tools,  vulnerability scanners , and \nmodeling software  even  when confined  to single  local  development  use.  \n2. Use  secure development  build c onfigurations  \nMany exploits  use common  compromise  techniques  such  as buffer  overflows,  return -oriented \nprogramming (ROP)  execution  gadgets, delayed dynamic  function  loading , and overriding  \nSoftware  Exception  Handlers (SEH) . For many  of these  techniques,  compiler,  assembler, linker , \nand interpreter  tools  have  been  extended  to include  defenses  to mitigate  these  risks.  The \nfollowing  is a list of build -chain  defensive techniques  that should  be deployed to narrow the \ncompromise  vectors:  \na) Stack  Cookies   Prevents  stack  overwrites, \nb) Address Space  Layout  Randomization  (ASLR)   Prevents  ROP/Hardcoded IP \nreferences , \nc) SEHOP   Prevents  SEH  hooking , \nd) Data Execution  Protection  (DEP)   Stack/Heap  execution  prevention, \n\n \nSecuring the Software Supply Chain: Recommended Practices for Developers  21 \n  \ne) No Execute  Bit (NX)   CPU  flag execution  prevention of memory  locations , \nf) Static  Libraries   Prevents  preloading of malicious dynamic  libraries, \ng) Stripping Binaries  Removing  symbols from  binary files  makes  it harder  for the file \nto be reverse engineered , \nh) Hardware Specific Preventions   More  preventions  are available based  on built -in \nhardware support . \nWhile  the above  preventions  are crucial  for ensuring  the runtime  protection  of software  under  \ndevelopment, the development  team  should  also  provide  to the end user  a suggested  \nenvironment  for which  their  software  may  run securely. For example, many  antivirus  products  \nprovide behavior  analysis  engines  to provide additional security  checks,  for example:  \na) Heap  Spray  Mitigation   Monitoring  commonly  targeted  heap  address es, \nb) Stack  Pivot  Detection   Detects  ROP , \nc) ROP  Call Detection   Detects  JMP/RET  (unconventional  program flows) , \nd) DLL  Injection  Detection   Dynamic Link Library (DLL)  location  and signature  \nvalidation, \ne) Null Page  Detection   Dereference exploitation  prevention, \nf) Root -Kit Detection   Address hooking  prevention, \ng) Behavioral Heuristics   Detection  of unusual  CPU,  memory  and resource  activity . \nIt is also important to note that while interpretive languages generally do not have the \nvulnerability risks outlined above, the implementation of the interpreter itself and the \nunderlying system libraries they use do, therefore these mitigations hold true for them as well.  \n3. Use  secure  third -party  software  toolchains and  compatibility  libraries  \nIn developers'  build  processes , various  tasks  are often  integrated  within  an Integrated  \nDevelopment  Environment  (IDE).  Many of these  environments  are self-contained  and allow all \ndevelopment, to include coding,  compiling,  linking,  packaging, and debugging,  to be performed  \nfrom  within  the tool.  The IDE may  even  provide  the ability  to check -in a source  to a repository.  \nIn many cases,  these  IDEs  support  multiple compiler  languages  and environments  and the \nability  to extend  the IDE by installing  plug -ins. Because  of the complexity  and untrusted \nsource s, IDEs may  become  compromised , lead ing to an insecure  local  development  \nenvironment.  To ensure  the integrity  of the development  process,  all IDEs  and their associated  \nplug -ins used  within  a developer environment  must  be preapproved,  validated, and scanned  for \nvulnerabilities  before  being  incorporated onto  any developer  machine.   \nBuild environments  may  require  the use of operating  system specific  utilities and commands. \nFor example, a Windows  environment  may  require Linux  operating  system commands  during  \nthe build  process  on the developer  host.  Such  build  environments  may  necessitate the need for \nthird -party  operating  system tools  and utilities to be installed  on the development  host  to \nprovide compatibility  between  the development  environment  and the production  build  \nenvironment.  In addition,  many development  environments  require API conversion  libraries to \n\nSecuring the Software Supply Chain: Recommended Practices for Developers  22 \n \n \n facilitate a common  coding environment  between  two disparate operating  systems such  as \nWindows  and Linux.  Toolchains and compatibilities  libraries are available through commercial  \nand open  source software.  Both the compatibility  toolchains and libraries also  need  to go \nthrough  a vulnerability assessment  prior  to being  adopted  within  the development  \nenvironment.  \n Code  Integration   \nDevelopment  managers  want  to ensure  that  the components  and software  integrated  into  the \ndelivered  product is tested  within  the integrated  environment  for which it  will be deployed. This \nprocess  involves  incorporating  all required dependencies  including  source  code,  components,  and \nadditional required metadata and utilities  into  a single  system.  During  code  integration,  the \ndeveloper  will ensure  that  the code  can be successfully  built , and monitor  and evaluate the runtime  \nbehavior . Software should be  integrated  using zero  trust principles as recommended  in NIST  \nSP800 -207 . \nRecommended  mitigations  \nAll third -party  modules  should  be tested for known  vulnerabilities  against  the Common  \nVulnerabilities and Exposures  (CVEs)  that  are listed in  the National  Vulnerability Database (NVD).  A \nsoftware  composition  analysis  tool  can help automate  this process.  The development  team  should  \nalso subscribe  to alerts and reports from the National Cyber Awareness System and other sources \nfor the latest  software  vulnerability information. Once  modules  are reviewed  for vulnerabilities,  \nthey  can be added to a developer or Open source Review Board (OSRB)  repository  for all approved  \ndownloaded modules . This  trusted  repository should  continue  ongoing  testing  to identify  new  \nvulnerabilities  that  are reported within  the modules. It should  also  incorporate  a process  to provide \nvulnerability updates  and/or  patches  to the end-user.  Applications  include  code  from  other  sources, \nsometimes slightly modifying or adding  integration  code for their  specific  use purpose.   \nThere  are security  dependency  analyzers  and many  other tools  and services  that can help  detect  \nreused  components  with  known  vulnerabilities.  These  activities are typically conducted  in an \nIntegrated Development Environment (IDE)  and use the organizations  secure  coding practice  and \nguidelines  such  as in PW.3.1, PW.3.2 and PW.4.1 of the SSDF . Code  integration  should  be \nimplemented  using zero  trust principles. Trust  should  not be  implied  and therefore critical  \ncomponents  and functions  should  check  usage  and access  rights  within  the code  and only use \nescalated  privileges  when  necessary . Developers  should  ensure  that  code  and build  integration  \nprocess  is repeatable. Developers  and QA engineers  may  provide automated  regression  tests to \nensure  components  are integrated  properly and functioning as specified  in the design  and \nrequirements  document.  They  may  also  provide additional static and dynamic  scanning  tools  to \ndetect  coding errors  and security  flaws  within  the developed  code as defined  in SSDF PW.5.1 and \n5.2. \n Defect/ Vulnerability  Customer  Report ed Issue  \nManagers  of a software  development  group  should  ensure  that  the software  they  develop is free  of \nhigh -risk known  defects  and vulnerabilities.  When  vulnerabilities  are discovered  and reported by \nthe customer,  the development group should  respond  to the incident  and provide component  \nupdates  to mitigate  the risk associated  with  the defect  or vulnerability.   \n\n \nSecuring the Software Supply Chain: Recommended Practices for Developers  23 \n  \nRecommended  mitigations  \nSupplier s should  have  a public  process  to accept  reports  of potential  defects  and vulnerabilities  \nfrom  customers and third -party  researcher s. Suppliers should use  automated  vulnerability \nnotifications  from  trusted  organizations  such  as the Cybersecurity  & Infrastructure  Security  Agency  \n(CISA)  to receive  timely  alerts  of the recent  and high -risk threats. All notifications should  be \nevaluated  with  respect  to the relationship  to the product  and prioritized  based  on risk  assessment.   \nEngineers  should  then  be assigned  to review,  diagnose,  and resolve  issues  as defined  in PW.8, \nRV.1.1 and RV.1.3. To decrease the attack  surface, a process should  be implemented  to identify  the \nclass of the vulnerability and examine  other product  features  and components  that might  \npotentially  be affected  by the same  identified  class  of incident . Customers  should  be provided \ntimely  responses with  the organizations  internal  vulnerability management  policy,  which  should  \nbe based  on industry  best  practice  documents  such  as NIST  SP 800 -216  guidelines . Updates to  \nsoftware are made available using a secure channel as required and specified in SSDF RV.2.1, \nRV.2.2. The update  is also  made available  and communicated  to all customers of the product  \ndescribing the defect or  vulnerability and resolution.  Updates  may  also  be automatica lly applied  to \na product based  on the update  strategy  configured  by the customer . \n External  Development  Extensions  \nOnce  released, product  functionality  may  be extended  by a development  team  other  than  the \noriginal  product  development  team.  In many cases,  this external development  team, with  respect  to \nthe development  responsible for the product,  may  need  to add additional functionality to the \nproduct or customize  it for specific customers needs  which  were  not met  or implemented  by the \nowner  of the product.  This  external development  team  may  be a solution  team  within  the company  \nthat produced the product  or a Value -Added  Reseller  (VAR).  An example of the type  of activity  that  \nmight  be performed  is the addition of a required authentication  method  to the existing  product.  \nWhen such  an activity  occurs,  modifications to the product can include  the addition  of software  \npackages  to support  the feature, as well  as graphical user  interface (GUI)  changes  to enable  and \nmanage  the new  functionality. During  this activity,  vulnerabilities  may  be introduced by the new  \ndevelopment, the new  packages  deployed, or modifying a provided API that is not being  used  as \ndesigned  or intended.   \nRecommended  Mitigations  \nWhen possible, extensions to  a product  must  follow  all secure  development  practices as the \noriginating  product  development  as defined  in SSDF PS.1.1, PW.4.1, PW.4.2, PW.5 and PW.7. In \naddition, a Software Bill of Materials (SBOM) should be made available  that  detail s the additional \npackages  and software  that  was  added. If signing is required, the certificate  that  is used  (if not from  \nthe same  supplier  of the product) must  be clearly identified. Modules that are modified  from  \noriginal  source  must  be clearly identified  within  the new  SBOM  and original  component  \ninformation  and owners  identified  along  with  all the new  information  required  to describe  the \nmodified  module, as required by SSDF PS.1.1.  \nThe PSIRT  must  be available  and ready to assist  end users  when problems  occur,  even  if the cause  \nof the problem  is related to the extensions  added as required  in SSDF RV.2.1. In many cases,  the \nVAR  will act as a go-between  between  the customer  and the product  PSIRT  to help resolve issues.  \n\nSecuring the Software Supply Chain: Recommended Practices for Developers  24 \n \n \n Feedback  to engineers  to resolve issues must  be aggregated  between  the customer  and external \ndevelopment  group  to allow  timely,  accurate information  which  may  be used  to resolve issues.  Any \ncode modifications due to vulnerabilities  within  the provided APIs  and functionality  must  be \ncorrected and published  to all customers.  The activities provide  mitigations  in line with  the \nfollowing  SSDF  activities:  \n1. PS.1.1 - Store  Code  and Executables , and Review  and Approve  All Changes . \n2. PS.3.2 - Create  and maintain  a SBOM  for each  software  package  created . \n3. PW.4.1 - Acquire  well -secure  components . \n4. PW.4.2 - Create  secure  software  components in-house . \n5. PW.5  - Create Source  Code  Adhering to Secure  Coding  Practices . \n6. PW.7  - Review  and/or  Analyze  Human -Readable Code . \n7. RV.2.1 - Analyze  each  vulnerability to gather  sufficient  information  to plan  its remediation . \n2.3 Verify  Third -Party  Components  \nDevelopers  routinely  incorporate third -party  commercial  software  components as an aspect  of \ntheir activities to leverage  existing  Application Programming Interface ( API)  capabilities.  These  \ncomponents  may  be Free Open Source Software ( FOSS)  or Commercial Off -the-Shelf Software \n(COTS).  When  sourcing  these  components,  developer s will typically make  their  selection  based  on \ncriteria  including the capabilities the component  enables  and the sustaining  engineering  support  \nmodel  for the component.  Prior  to the incorporation of third -party  components  by engineers,  an \norganization  may  require an approval  process  as outlined in S ection 2.2.4, Code I ntegration . This \nprocess may include vulnerability database analysis,  secure  composition  analysis,  vulnerability \nanalysis, risk assessment , and source  code evaluation  on the component s under  consideration , the \nresult s of which indicate whether the specific components  identified  are allowed  or not. Once  \nselected, the identified  components  are continually monitored, if possible,  by using  an automatic  \nvulnerability tracking  service  that  prioritizes and fixes  identified  vulnerabilities within  open source  \ncompone nts. PSIRT  teams  may  discover  new  vulnerabilities  and alert product  owners  to remediate \nwhen a new  vulnerability is discovered.  \n Third-P arty  Binaries  \nThird -party  software, sometimes delivered in binary format, is like a black  box for the engineer  or \nthe organization  who  is integrating  it. The software  may  not be actively maintained and may  have  \nsecurity  weakness  or vulnerabilities.  \nRecommended  mitigations  \n1. Binary scanning  and software  composition  analysis  tools  can often detect  unknown files  \nand the open source  components  contained  in binary packages, identifying the security  \nweaknesses  associated  with  these  components  without  the need  for source  code.  The tools  \nmay  evaluate and provide a score  of the vulnerabilities detected.  The activities are highly  \nrecommended  to verify the integrity  of the third -party  software.  The output  can be \ncompared  with  the SBOM , or the source  codes  provided by the third party , to verify the \nSBOM.  \n\n \nSecuring the Software Supply Chain: Recommended Practices for Developers  25 \n  \n2. The development  team  runs  the binary scan  of the third- party  software, identif ies potential \nthreats  including unknown components,  open  software  components and vulnerabilities.  \nThe output  of the composition  analysis  should  be considered  in the organizations  decision  \nto select  and integrate  the software  component. Please see Section  2.3.2 Selections and \nIntegration  and 2.3.3 Obtain Components from a K nown and T rusted Supplier for \nmore information . \n Selections  and  Integration  \nBefore  the integration  of third- party  components,  each  component  must  be evaluated  for the \npotential  security  risk  that  might  be associated  with  it. The evaluation  includes  reviewing  and \ntesting  the software.  \nRecommended  Mitigations  \nSAST/DAST  and other  appropriate review  such  as composition  analysis  must  be performed to \ndetermi ne if the risk is acceptable . Once  determined, the source  code  (not binaries alone) should  be \nintegrated  into  the build  environment  allowing  the security  scanning processes  of the build  \nenvironment  approved  by the organization  to take  place. Whenever  possible,  images  should  be \nbuilt  from  the source  and not downloaded from  the internet , unless  there  is an understanding  of the \nprovenance  and trust  of delivery. \n Obtain  Components  from  a K nown  and  Trusted  Supplier   \nWhen  considering  the selection  of a third -party  component,  care should  be taken  to build  a \nrelationship with  a known  and trusted supplier  that  has a proven record  for secure  coding  practices \nand quality  delivery of their  components.   \nRecommended  mitigations  \nWhen  the organization  makes  decisions  concerning selection,  use,  changes,  or updates  of third -\nparty  or open -source  software  for its products, it should  perform  a risk assessment  and ensure  the \nresidual risks are acceptable.  The organization  should  verify a third -partys  ownership  and control  \nstatus , their  Data Universal Numbering System (DUNS ), and past  performance of the suppliers  and \ntheir upstream  supplier s, if such information is available, . The selection  will also  take  into  account  \nthe producers country  of origin  and adhere to the Defense Federal Acquisition  Regulation  \nSupplement  (DFARS) , as required. \nSuppliers should produce  artifacts  attesting  to the development  process,  quality , and security  \naspects of the component  being  considered  for inclusion  in an organizations  software  product.  The \navailability  of artifacts  does  not exclude  the process  listed  in Section  2.3.2 Selection s and  \nIntegration . In addition,  an organization  will compile  a list of known  trusted supplier s and their  \nassociated  artifacts  that have  been  integrated  into  the companys  products  as well  as a repository of \nthe third -party  components that have  been  vetted.  A record of all components that comprise a \nproduct are captured within an SBOM to im plicitly aid in the verification and vetting of the product \nin its entirety. Trusted  suppliers  are measured  by: \n1. The third- party  component  meets  all requirements  for the product  considering  adoption . \n\nSecuring the Software Supply Chain: Recommended Practices for Developers  26 \n \n \n 2. The quality  of the third -party  component  is verified  based  on the results of testing by the \nadopting  organization . \n3. The quality  of the artifacts  supplied, for example  the validation of an SBOM , is validated  as \nbeing  correct. \n4. The owner  of the component  has a history  of timely  responses  to known  vulnerabilities  \nreported within  the third -party  component . \n5. The third- party  mechanism  to report vulnerabilities  is easy to use . All available updates  of \nadopted  component s are easy  to integrate into  the development  environment  using well \ndefined and underst ood update procedures between the third -party and development \ngroup . \n Component  Maintenance  \nOnce  a third -party  component  has been  selected  and integrated  into  the product,  care must  be \ntaken  to monitor  modification  of the component  by the supplier, specifically  with  respect  to \naddressing know  CVEs  and vulnerabilities  that  have  been  reported by the development  team  and \ncommunity  of customers of that component.  Adoption  of changes  follows the same  process  as \noutlined  in Section  2.3.2,  Selection s and  Integration . \nRecommended  mitigations  \nThe adopting  product  organization  should  monitor  available CVE  reporting mechanisms  and third -\nparty  support  channels  to determine whether  vulnerabilities  identified  within  an adopted  third -\nparty  component  can impact  the products  and take  appropriate  actions to solve  or mitigate  the \nvulnerability.  The contract  with  the third party  should  resolve future  vulnerabilities.  The owner  of \nthe third -party  component  must  also  notify  the product organization  of the presence of a \nvulnerability,  the risk associated  with  it and a timeframe for when the vulnerability will be \naddressed and made  available to the product  organization.  \n Software  Bill of Material s (SBOM)  \nThe details  of an integrated third -party  component  should be reported in an SBOM  for the product  \ndeveloped  to easily validate approved components and identify the presence of vulnerable \ncomponents when defects are discovered . Several specifications  define  the format  of an SBOM:  \n1. The Linux Foundati on Projects  Software Package Data eXchange (SPDX) . \n2. OWASP  CycloneDX . \n3. NIST  Software Identification (SWID) tags . \nRecommended  Mitigations  \nAn SBOM  provided by a supplier  or owner  of the third -party  component  should  be validated  and \nupdated  as needed. Any disc repancies  should  be reported to the supplier.  To that end,  software  \ncomposition  analysis  (SCA)  tools  should  be applied  to the software  deliverable from  the third -party.  \nThe third  party's  SBOM  can be compared  with  the SBOM  produced by the SCA  tools.  As described  in \n2.5 Deliver C ode , the binary  scanning SCA  tool can identify  the contents  of the final  deliverables  \nfrom  the third -party  software.   \n\n \nSecuring the Software Supply Chain: Recommended Practices for Developers  27 \n  \nIf an SBOM  is not available  from  the supplier,  the development  team  will derive  the required \ninformation  to describe  the third -party  component  within  the SBOM  for example  by utilizing \nsoftware  composition  analysis  tools. When  third -party  source  is modified  by a developer, both  the \ninitial  SBOM,  if provided by the supplier  and the updated  SBOM,  describing  the enhancement  or \ndefects  addressed  in the original  supplied  source , can be defined  within  the SBOMs  dependency  \nrelationship element . \nPlease refer  to Section  5 (recommended  data  fields)  in NTIAs  The Minimum  Elements for a Software  \nBill of Materials  (SBOM). \n2.4 Harden  the  Build  Environment  \nThis document outlines two types of build environments, the individual developer environment \nand the production build environment. An example of both environments are outlined in Figure 4. \nFor more information on the individual developer environment, refer to section 2 .2.3 Secure  \nDevelopment  Practices . \n \n \nFigure  4: Secure Build Environment  \nThe production build  environment  is where  reproducible  deliverables are  built . The components \nthat comprise a product are provided to end users  as a bundle  that may  include multiple modules  \nand a cryptographic  signature . The cryptographic signature validates  that  the software  has not \nbeen  tampered  with  and was  authored  by the software  supplier . The build process may  include  \nautomated  tasks  to validate the security  of the software.  The software  is installed by customers and, \nafter  some  validation,  put into  production. The build  environment  may  produce software  that is \n\n\nSecuring the Software Supply Chain: Recommended Practices for Developers  28 \n \n \n then  deployed as software -as-a-service  (SaaS).  These  applications provide  some  functionality  over  \nthe network.  The resulting software  is usually  not distributed  to customers for installation.  Other  \ncommon  build  environments  under  consideration  include the following : \n Continuous  integration/continuous  deployment.  In this case,  the software  is installed in a \nsubset  of the cloud  for immediate  feedback  and A/B testing , \n Building software  as part  of a rapid  iterative cycle, such as using an Agile  Development  \nmethod.  The resulting  software  may  be distributed  to customers or may  be used  for testing \nwithout  distribution , \n Building software  as part  of an open source  project, where  executables are not  distributed  \nas an output  of the project.  In this case,  the software  resulting from  the build  is intended  to \nbe a baseline  for testing  and to identify  problems  early in the development  cycle.   \nCommon  to all scenario s are the tasks  associated  with  architecting,  implement ing, and maintaining  \nor optimizing  the build  process . Also common are  provisioning  and configuring  equipment  as build  \nservers  or VMs,  networking,  and configuring user  permissions.  \nThe build  system must  be developed  and maintained  with  the same  level of security,  integrity , and \ndiligence as the source  code  and resultant product  itself , as described in Section  2.1 \nRecommended Mitigations . \nThe build  environment  may be hosted  in on-premise  systems or may  be hosted  in a cloud. T he same  \nrigor  and discipline  for hardening  an on-premise  build  environment  should  be used  for the cloud -\nbased  build  environment.   \nNote:  It might  be advantageous  to build  software  in both  cloud  and on-premise  environments  and \ncompare  the results.  If the results  do not match,  there may be evidence  of supply -chain  tampering.  \n Build C hain  Exploits  \nThe build  environment  is a prime  target  in a supply  chain  compromise . In this scenario, a \ncompromise may occur when a threat actor:  \n1. Infiltrates the development  network . \n2. Performs  a scan  to locate  the repository and build  systems  and to identify  vulnerabilities . \n3. Crafts  an exploit  to compromise  the build  system or repository (or both. \n4. Deploys  the exploit . \nIn this case, the exploit is subsequently included in:  \n Compiled s ource  code , \n Included libraries , \n Reintroduced when libraries r evert  to older third -party  librar ies with  vulnerabilities , \n Resident  memory,  which  gets  embedded in source  during compile  time  via a rootkit -the \nsource  is not modified  in the repository, \n\n \nSecuring the Software Supply Chain: Recommended Practices for Developers  29 \n  \n Network  Man -in-the-Middle ( MITM ) attack , which  modifies source  when being  pulled down  \nto build  system. \nRecommended  Mitigations  \nThe build  pipeline  infrastructure includes all systems that  come  in contact  with  the development  \nand build  process  such as source  code repositories,  engineering  workstations,  build  systems,  \nsigning servers, and deployment  servers for both  on-premise  machines  and those  hosted  in the \ncloud. Each  of these  systems should be : \n Completely locked down , \n Protected from external and off -local area networ k (LAN ) activity , \n Monitored for data leakage,  particularly code repositories and engineering workstations , \n Configured to prevent infiltration and exfiltration . \nAdditionally: \n Subject build scripts and configuration files to the same code review process listed in \nSection 2.2.1 Modification or Exploitation of S ource Code by I nsiders . \n Use v ersion control for pipeline configurations , \n Ensure each system requires multi -factor authentication , \n Segregate the engineering network from the corporate network , \n Minimize and regularly audit service accounts , \n Log all access to the build pipeline , \n Protect any secrets associated with the build pipeline . \nLock systems  down  \nWhen locking down systems, only those operations specific to each systems function should be \nallowed. For example, build systems should only perform operations necessary to delivering builds.  \nProtect systems from external and o ff-LAN network activity  \nTo protect systems from potentially harmful network activity inbound and outbound network \nconnections other than allowed URLs and necessary services should be blocked.  \nTo assure that all source and other intellectual property on eng ineering machines is safeguarded, \neach systems cybersecurity defenses should be configured to prevent infiltration and exfiltration \non all engineering workstations (e.g., configuring intrusion detection and prevention, behavior \nblocking, reputation -based security, machine learning -based protection, appl ication isolation  and \ncontrol, and vulnerability protection).  \nVersion control pipeline configurations  \nPipeline  configurations should be version controlled. Administrators should  only update  the \nconfiguration  code , not the actual systems.  \nIn a continuous  delivery  (CD)  pipeline  environment,  the CD orchestrator should  be the only entity  \nthat manages  all the environments, for example the  development  and production  environments . All \n\nSecuring the Software Supply Chain: Recommended Practices for Developers  30 \n \n \n configurations  and rules  for the environments  should  be version -controlled and managed  by the \norchestrator. This will ensure  any changes whether malicious or accidental that could  weaken  \nthe security  posture  of the system will be immediately visible.  \nAdministrators should  rarely  have  to adjust  the systems themselves, as  configuration settings are in \ncode and executed by the pipeline.  An exception to this would be when an administrator has to fix \nthe mean time to failure in production. In this case, the secrets management tool can generate a \ntemporary  Socket Shell ( SSH ) key for a limited time to allow the administrator access . Once \napproved, administrative  changes  should  be adjusted  in the pipeline  configurations  and \nautomatically  managed.  \nAlso  ensure  that  the administrator  tools  are not in the public  environment  such  as a Kubernetes  \ncluster. Utilize  hardening guides  such  as the Kubernetes Security - Operating  Kubernetes Clusters  and \nApplications  Safely21 and Kubernetes Hardening Guidance22 \nSupport  the separation  of duties.  For example,  the lead  or business  owner  should  be the owner and \nadministrator of the build k eys. The root  account  should  not have  access to the key.   \nMulti -factor authentication  \nEach system should use multi -factor authentication (MFA): wherever possible, require MFA for \naccess to build pipeline systems. Limit access to build pipeline machines using best practices such \nas role -based access control and le ast privilege. For more information on this, please refer to NIST \nSP 800 -172 sec. 3.1, Role Based Access Control.  This specifically explains how to employ dual \nauthorization to execute critical or sensitive system and organizational operations.  \nSegregate the engineering network  \nEach system should  only be accessible via an engineering network that  is completely segregated \nfrom the organizations corporate network . If possible, the engineering network should have no \ndirect ac cess to the Internet. \nMinimize and regularly audit service accounts  \nThe use of service accounts, like non -human privileged accounts used to run automated processes, \nshould be minimized and carefully audited. Every service account login should be logged. These logs should includ e date and time and the origin of login. Service accounts should follow the least \nprivileged policy . All service accounts should be regularly reviewed to assure they are still needed , \nand unnecessary accounts removed . Per guidelines in NIST SP  800-53, software function privileges \nshould be raised when needed to perform a function and then lowered when completed.  \nLog all access to the build pipeline  \nAll access to build pip eline systems should be logged. At minimum, log the  MFA ID of the user \nauthent icating access and the date and time . \n \n21 https://kubernetes -security.info/ \n22 https://www.nsa.gov/Press -Room/News -Highlights/Article/Article/2716980/nsa -cisa-release- kubernetes -\nhardening -guidance/  \n\n \nSecuring the Software Supply Chain: Recommended Practices for Developers  31 \n  \nProtect any secrets associated with the build pipeline  \nBest practices should be implemented to protect any secrets associated with the build pipeline. \nSecrets can be but are not limited to account passwords, API keys, and private certificates . Require \ndefault usernames and passwords in the pipeline to be changed. Any documentation inside the organization containing secrets should have strong role -based access control. Additionally, avoid \nputti ng secrets in plain text in any code (e.g., automation code), avoid logging sensitive data in \napplication logs, and regularly  rotate secrets. Please refer to NIST SP 800 -57, Part 1, Revision 5  for \nmore details . \nRecommended Mitigations (advanced) \nThe follow ing mitigations describe  advanced  build  best practices, and may offer additional \nprotection when they complement the mitigations described previously in Section 2.4.1 Build \nChain Exploits . \nHermetic builds  \nAll transitive  build  steps,  sources,  and dependencies  should be fully declared up front  with  \nimmutable  references and the build  steps  should run with  no network  access.  \nThe developer -defined  build  script  must declare all dependencies, including sources  and other  \nbuild  steps,  using  immutable  refer ences  in a format  that the build  service  understands.  \nThe build  service : \n Must fe tch all artifacts  in a trusted control  plane , \n Must not allow  mutable  references , \n Must verify  the integrity  of each  artifact , \n Must prevent network access while running the build steps . \nIf the immutable  reference includes a cryptographic hash,  the service  must verify  the hash  and \nreject  the fetch  if the verification  fails.  Otherwise,  the service  must fetch  the artifact  over  a channel  \nthat ensures  transport integrity,  such  as Transport Layer Security ( TLS) or code  signing.  \nA \"best  effort \" is sufficient when attempting to prevent  network access while running build steps.  \nThis should deter  a reasonable  team  from  having  a non- hermetic  build, but it need  not stop a \ndetermined  adversary. For example, using  a container  to prevent  network  access is sufficient.  \nReproducible builds  \nReproducible builds  provide additional protection  and validation against  attempts  to compromise  \nbuild  systems . They  ensure  the binary products  of each  build  system match: i.e., they  are built  from  \nthe same  source,  regardless of variable  metadata such  as the order  of input  files,  timestamps,  \nlocales, and paths.  Reproducible builds are those where re- running  the build  steps  with  identical  \ninput  artifacts  results in bit-for-bit identical output. Builds  that cannot  meet  this must provide a \njustification  why  the build  cannot  be made  reproducible. \n Establish  and maintain  an authoritative  source  and repository to provide a trusted source  \nand accountability  for approved  and implemented  system  components  as defined  in NIST  \nSP800 -172  Sec. 3.4, \n\nSecuring the Software Supply Chain: Recommended Practices for Developers  32 \n \n \n  Employ  automated mechanisms to detect  misconfigured  or unauthorized  system \ncomponents;  after  detection,  remediation is performed to either patch, re -configure or \nremove  the identified components , \n Employ  automated discovery  and management  tools  to maintain  an up-to-date, complete, \naccurate,  and readily available inventory of system components , \n Identify  and authenticate  as defined  in NIST SP800 -172 Sec. 3.5 [Assignment:  organization-\ndefined systems and system  components ] before  establishing a network  connection  using  \nbidirectional  authentication  that is cryptographically based  and replay resistant , \n Employ  automated mechanisms for the generation, protection,  rotation, and management  of \npasswords for systems and system components  that  do not support  multi -factor  \nauthentication  or complex  account  management , \n Employ  automated or manual/procedural mechanisms  to prohibit  system components  from  \nconnecting to organizational systems unless  the components  are known,  authenticated, in a \nproperly configured  state,  or in a trust profile , \n Employ  a means  to allow  the comparison  of binaries  built  from  two or more  disparate, \nsegmented, protected, and secured  environments . \nThese mitigations  can be modeled after  the emerging  frameworks, including  build  requirements  of \nthe Supply -Chain  Levels  for Software  Artifacts ( SLSA ) project  and the Software  Component  \nVerification  Standard  (SCVS ). SLSA  provides for different  security  levels, each  of which  provide \nrequirements,  processes , and best  practices  to increase  trust  in software. SCVS  is set by the Open  \nWeb  Application  Security  Project  (OWASP ). The standards comprise  six families of control  \nrequirements  including build  environment  for verification  of the integrity  of software  supply  chain.   \nThe artifacts  for builds  should  include,  at a minimum , the source  repository, the third -party \ndependencies, the build  script , and the output  of the build. Some  products  may  supply  these  \nartifacts  to the recipient  of the software, though  in many cases the supplier may  require a special \nlicense  or agreement  to obtain  these  artifacts. For reproducible  builds,  the artifact  should  be the \noutput  of the script  that compares  the builds.  All artifacts  should  be retained  by the supplier  for the \nentire  support  lifetime  of the product until  it is marked  for end of life.  \nRecommended  Miti gations  \nAdvanced  techniques may include:  \n1. SSDF  PO.3  (Implement  a Supporting  Toolchain),  specifically  PO.3.2 and PO.3.3, since  the \ntoolchain  is used  in the build  process . \n2. SSDF  PO.4  (Define Criteria  for Software  Security  Checks),  specifically  PO.4.2 to gather  \ninformation  from  the build  to support  security  criteria . \n3. SSDF  PS.1  (Protect All Forms  of Code  from  Unauthorized  Access),  specifically  PS.1.1, \ngenerating  the information  in PS.2.1 and implementing  PS.3.1. \n4. SSDF  PW.6  (Configure  the Compilation  and Build  Processes) . \n5. SSDF  PW.8  (Test  Executable  Code) if the tests  are designed  to be run and verified  as part  of \nthe build  process . \n\n \nSecuring the Software Supply Chain: Recommended Practices for Developers  33 \n  \n6. SSDF  PW.9  (Configure  the Software  to Have  Secure  Settings  by Default),  particularly as part  \nof the architecting,  implementing  and maintaining  the build  process . \n7. SSDF  RV.1  (Identify  and Confirm  Vulnerabilities on an Ongoing  Basis)  particularly RV.1.2 \nwhere  automated  code scanning  may  be part  of the build  process . \n Exploited Signing  Server  \nSoftware  distributed as an artifact  to a customer  should be  delivered  with  a unique  cryptographic  \nsignature  which  verifies  the integrity  of the software  artifact.  However, if the signing  facility  itself  \nhas been  compromised,  then  the delivered artifact may  also have been  compromised,  and the \nsignature  validates the compromised  artifact  and not the  true artifact . \nNOTE : Software  delivered to a customer as  a service  rather  than  as a binary  artifact  will not typically  \nbe delivered with a cryptographic signature . \nA threat actor could i mpersonat e a target  by compromising the code -signing service , using  the \nsigning  system to sign  compromised  components or product s. They could do this by  leveraging \nmisconfigured  account  access  controls  on a server , or exploit ing the service  using a known  or zero-\nday exploit . \nA threat  actor could also  impersonate a target using  a self- signed  certificate  and injecting into  the \nbuild or signing  process . \nRecommended  mitigations  \nCode signing  is usually the last line of defense against  a software  supply  chain  exploit.  Both \nsuppliers and developers work together to ensure the integrity of the signing servers is not \ncompromised. Section 2.2.1 Protect All F orms of C ode from U nauthorized Access  of Part 2 of \nthis series, focusing on t he supplier , will discuss the  high -level supplier specific concerns . The \nfollowing procedures are used by developers to  ensure  code -signing integrity:  \n1. Implement  strong  authentication  methods  such as strong password s, certificates, two factor \nauthentication ( MFA) , and physical access control to  protect  the signing  infrastructure. \n2. Control u ser access  to the  signing  infrastructure , using least  privilege s, revocation  of user  \ncredentials  after  departure  or termination,  MFA for code commits,  and continuous  \nauthentication  utilizing  behavior  analytics . \n3. Conduct code signing on a phys ically isolated network segment . \n4. Use intrusion  detection  and protection  systems in the c ode-signing environment to protect \nthe code -signing resources, machines , and process used . \n5. Deploy  and use a Security Information and Event Management  (SIEM ) system. \n6. Use cryptography in transit  and at rest. \n7. Apply  hardening procedures on the signing environment  systems that allow customers to \ndeploy and install  only signed  and verified  release packages and products . \n8. Use centralized  log servers  (with  append  only,  encryption,  etc.)  \n9. Ensure the signing  system meet s baseline  security  standards. \n10. Require  multi -party approval  for physical and remote access and log all access . \n\nSecuring the Software Supply Chain: Recommended Practices for Developers  34 \n \n \n 11. Grant super -user  access  to only a small number of signing system admins. \n12. Implement the following Indirect Controls:  \na) Perform  periodic  vulnerability scans  (network  and web  application) , \nb) Perform  periodic  penetration  testing (network,  web,  wireless and red teaming) , \nc) Classify  documents  properly  (Confidential,  Top  Secret,  etc.) , \nd) Watermark  usage  on the documents, \ne) Use Data Loss Prevention (DLP ) tools , \nf) Proper ly dispos e of physical  media  by destroying it, \ng) Proper ly dispos e of digital  media  by wiping it, \nh) Monitor  / perform  integrity  checks of executables,  libraries, configuration  files,  etc. \n2.5 Deliver C ode  \nSoftware suppliers utilize  distribution  systems to deliver  software  packages to customers.  The \npackage  includes metadata ( e.g., a version  number)  and software  to install  or update  in the \ncustomers information  system and network.  Before  shipping the package  to the customers,  the \ndeveloper  should  perform  binary composition  analysis  to verify  the contents  of the package.  \nThe distribution  system is comprised  of a repository , which  stores  packages  for delivery, and a \npackage  manager  at the customer  information  system.  The two entities  establish  a secure  \nconnection  over  the internet  and transfer  the package.  \n Final  Package  Validation  \nThe final  package  or update  to be delivered  to a customer  may  have  issues that  expose  the \ndeveloper  and customers to cybersecurity and privacy risks . For example, it may  contain  \nconfidential  information  (e.g., hard coded  credentials,  personal data), open source software  license  \nissues, and components  included in files with  unknown origin . Moreover, the deliverable may  have  \nbeen  built  with  improper  compiler  options  or build  settings.  \nRecommended  mitigations  \n1. Binary software  composition  analysis  tools  can investigate  what  exactly  is included  in the \nfinal  deliverables and identify  potential  issues in the final  packages  described  above.  The \ndeveloper  should  run a binary scanning or composition  analysis  tool  and ensure  the \nintegrity  of its product  before  delivery.  The tool  can detect  potential vulnerabilities and \nthreats   including software  of unknown provenance  (SOUP)  and secrets inadvertently \nincluded in the final  packages   and produce  an SBOM  of the final  package  for the customer.   \n2. The organization  can compare  the binary analysis  output  and the other  artifacts from  the \nbuild  process  to ensure  that  the final  package  includes  only the intended  software  \ncomponents.  Upon  receipt , the customer  can run the binary software composition tool  to \nassess risks  by verifying  the contents  of the delivered  code  before  the deployment.  The \ncustomer  can continue  to utilize  the tool  for continuous  monitoring of post  deployment  \nvulnerabilities.  \n3. Supplier  and customer  run binary scanning or composition  analysis  tools  to verify the \nintegrity  of final  software  packages  or updates  provided by developers . The developer  can \n\n \nSecuring the Software Supply Chain: Recommended Practices for Developers  35 \n  \ninclude the SBOMs  within  the final  packages.  It may  depend  on contracts or arrangements  \nbetween  developer  and customers.  An SBOM  should  contain  all primary (top  level) \ncomponents  of the final  product, with  all their  transitive  dependencies  listed.  Depending  on \nthe contracts with  customers,  the supplier, or the developer  provides further  details.  \n Potential Tactics to Compromise the S oftware Packages and U pdates  \nThe package  may  be compromised  while  going  through the distribution  system from  the supplier  to \nthe customer.  An adversary may attempt a man -in-the-middle attack  or compromise  the sour ce \ncode repository.  As a result,  malware or vulnerabilities  can be introduced to the package, or an \nolder version  of the package  containing  an exploitable  vulnerability can be delivered to the \ncustomer.  Installing the compromised  packages will impose  risk to the customer  organization  and \nits system.  \nRecommended  mitigations  \nThe package  including  the correct metadata should  be signed  by a cryptographically- secure  \nsignature  algorithm before  being  uploaded  to the repository.  Alternatively,  the package  can include  \ncryptographic  hashes.  The package  manager  should  verify the signature  or hashes  and the \nmetadata including the version  number.  If verification  fails,  the manager  should  not process  the \npackage.  \nNote: The developer must take great care  to protect  its private  key used for the digital  signature or \nhashes  as anyone  can impersonate  the developer if the private  key is stolen.  A lost key may prevent  \nfuture  updates  or incur  challenges with  distributing  new  keys. \n1. Product- Level:  \na) Hash/Digital Signature  of Product Distribution  Package , \nb) Hash/  Digital Signature  of Product Update , \nc) Hash/  Digital Signature  of Product Upgrade . \n2. Component -Level:  \na) Hash/  Digital Signature  of Product Components  in Distribution  Package . \nNote: Further study is necessary to  define  what  types  of components  or static  files  should  be signed  or \nhashed  in a package. The static  files  can include  SBOM,  configuration  files  and documentation. For \ncorrect verification  of such  components,  guidelines  or standards  are needed to clearly  specify  how  to \ncreate  the signature and hash.  \n Compromises of the Distribution System  \nAttacks  to the distribution  system include compromising the repository  to introduce  malware into  \nthe packages  stored  in the repository, taking  advantage of the package  manager  vulnerabilities  to \ndirect it to a malicious site,  and a MITM  attack  between  the supplier,  the repository,  and the \npackage  manager. As a result,  a compromised  package can be delivered to the customer.   \nRecommended  mitigations  \nThe following activities may  be optional  if the developer  takes  the mitigation  measures  described  in \nsection  2.5.2 Potential A ttacks  to Compromise the Software Packages and Updates . The \n\nSecuring the Software Supply Chain: Recommended Practices for Developers  36 \n \n \n developer  should  secure  the repository according  to information  security  management  standards \nor guidelines  to ensure  its integrity.  For example,  apply  continuous  monitoring to prevent,  detect  \nand remediate threats  against  the repository.  The developer  should  practice  the recommendations  \ndescribed in the proceeding  sections of Section 2.5 Deliver Code of this document  to develop, test, \nand provide a secure  package  manager.  \nIn addition, the developer, or the supplier  should  manage new  vulnerabilities  associated  with  the \npackage  manager  and ensure  that the customer  uses  its latest  version.  Moreover, the developer  \nshould  ensure  that  transport layer  security  of the distribution  system is configured  proper ly to \nensure  the confidentiality,  integrity,  and authenticity  of information  to be transferred, for example,  \nthe TLS protocol  version,  algorithms  for key exchange, encryption,  message  authentication  and \nsignature  as recommended  by NIST  SP 800 -52 r ev.2 (Guidelines  for the Selection, configuration, and \nUse of Transport  Layer  Security  (TLS)  Implementations ).  \n1. Protect  All Forms  of Code  from  Unauthorized  Access  (SSDF PS.1). As described  in the \nprevious  subsection,  the developer  may  take  the following  mitigation  measures : \na) Repositories:  apply  information  security  management  standards or guidelines  to \nsecure  the repositories, \nb) Package  managers:  apply  secure  development  process  to provide  secure  and up-to-\ndate  package  managers  to the customer , \nc) Distribution  system transport layer  security:  select,  configure,  and use transport \nsecurity  implementation  recommended  by NIST  SP 800 -52 rev.2.  \n  \n\n \nSecuring the Software Supply Chain: Recommended Practices for Developers  37 \n  \n3 Appendices  \n3.1 Appendix  A: Crosswalk  between  Scenario s and  SSDF  \nThe reference numbers in the below crosswalk  may look similar for each role (Developer, Supplier \nand Customer) , however they correlate to their specific part of this ser ies. \nSSDF  # Developer  Supplier  Customer  \nPO.1  2.2.3 Secure  Development  \nPractices  2.1.1 Define  criteria for \nsoftware  security  checks   \nPS.1 2.2.1.1 Source  Control  \nCheck -in Process \n2.2.1.4 Code  Reviews  \n2.2.6 External \nDevelopment  Extensions  \n2.3.2 Selections  and \nIntegration  \n24.1  Build  Chain  Exploits  \n2.5.3 Secure  the \nDistribution  System  2.2.1 Protect  all forms  of \ncode from  unauthorized  \naccess   \n \n2.2.2 Provide a \nmechanism for verifying  \nsoftware  release integrity  \n(PS.1, PW.9)   \nPS.3 2.2.1.1 Source  Control  \nCheck -in Process \n2.2.1.2 Automatic  and \nManual Dynamic  and \nStatic  Security  / \nVulnerability  Scanning \n2.3.2 Selections  and \nIntegration  \n2.3.3 Obtain  Components  \nfrom  a Known and \nTrusted  Supplier  \n2.4.1 Build  Chain  Exploits  2.2.3 Archive  and protect  \neach  software  release   \nPW.1 2.3.2 Selections  and \nIntegration  2.3.1 Design  software  to \nmeet  security  \nrequirements    \nPW.3 2.2.3 Secure  Development  \nPractices  \n2.3.2 Selections  and \nIntegration  \n2.3.3 Obtain  Components  \nfrom  a Known and \nTrusted  Supplier  2.3.2 Verify  third -party  \nsoftware  complies with  \nsecurity  requirements  2.1 Procurement/Acquisition  (1) \nRequirements  Definition  / \nRecommended  Controls  \n(viii)(viii)  \n2.2 Deployment  (6) \n(2) Testing  Functionality (c) \nRecommended  Controls  (ii) \nVerify  contents  in SBOM  \n\nSecuring the Software Supply Chain: Recommended Practices for Developers  38 \n \n \n 2.3.4 Component  \nMaintenance  \n2.3.5 Software  Bill of \nMaterial (SBOM)  2.2 Deployment  (6)  \nDeploy  (3) Contracting  / \nRecommended  Controls  (v) (viii)  \n(ix)(x)  \nPW.6 2.2.3.2 Use of Unsecure  \nDevelopment  Build  \nConfigurations  \n2.4.1 Build  Chain  Exploits  2.3.3 Configure  the \ncompilation  and build  \nprocesses   \nPW.7 2.2.1.4 Code  Reviews  \n2.2 Open source  \nManagement  Practices  \n2.2.6 External \nDevelopment  Extensions  \n23.2  Selections and \nIntegration  \n2.3.3 Obtain  Components  \nfrom  a Known and \nTrusted  Supplier  2.3.4 Review  and/or  \nanalyze  human -readable \ncode   \nPW.8 2.2.1.3 Nightly  Builds  \nwith  Regression  Test  \nAutomation  \n2.3.2  Selections  and \nIntegration  \n2.4.1 Build  Chain  Exploits  2.3.5 Test  executable code    \nPW.9 2.2.3.2 Use of Unsecure  \nDevelopment  Build  \nConfigurations  \n2.4.1 Build  Chain  Exploits  2.2.2 Provide a \nmechanism for verifying  \nsoftware  release integrity  \n(PS.1, PW.9)  \n \n2.3.6 Configure  the \nsoftware  to have  secure  \nsettings  by default   \nRV.1 2.3.4 Component  \nMaintenance  \n2.4.1 Build  Chain  Exploits  2.4.1 Identify,  analyze, \nand remediate \nvulnerabilities  on a \ncontinuous  basis  \n  \n\n \nSecuring the Software Supply Chain: Recommended Practices for Developers  39 \n  \n3.2 Appendix  B: D ependencies  \nGreen  - Dependencies/artifacts  recommended  to be provided by the supplier  for benefit  of the \ndeveloper . \nDark Green  - Dependencies/artifacts  recommended  to be provided by third -Party  suppliers for \nbenefit  of the developer .  \nPink  - Dependencies/artifacts  recommended  to be provided  by the customer  for benefit  of the \nsupplier/developer . \n# Dependency  \n1 Provide issues  from  customers  \n2 Provide given hashes  as required \n3 SDLC  policies and procedures \n4 Secure  architecture,  high -level design  \n5 Qualified  team  assembly  with  code/security  training \n6 Independent  QA individual/team  \n7 Independent  security  audit  individual/team \n8 Open source  Review  Board  (OSRB)  with  repository  \n9 Product release management/resources  \n10 SBOM  \n11 Development  location  and information  \n12 Third -party  SBOM  \n13 Third -party  License  \n14 Release notes  (detailing  vulnerabilities  fixed)  \n15 Vulnerability  notifications  \n16 Publish  updates  and patches  to the customer  to address new  vulnerabilities  or weaknesses  \nfound  within  the product \n17 Requirements  and criteria  for success \n18 Implied  industry  security  requirements  \n19 Provide issues  from  operational environment,  take  updates  and patches  \n20 Vulnerability  notifications  and reporting from  the users  \n  \n\nSecuring the Software Supply Chain: Recommended Practices for Developers  40 \n \n \n 3.3 Appendix  C: Supply Chain Levels for Software Artifacts ( SLSA ) \nSupply-Chain  Levels  for Software  Artifacts  (SLSA)  is a security  framework from  source  to service,  \ngiving anyone  working  with  software  a common  language  for increasing  levels of software  security.  \nThe framework  is currently in Alpha  stage  and constantly being  improved  by supplier -neutral  \ncommunity.  Google  has been  using an internal  version  of SLSA  since  2013  and requires it for all of \ntheir production  workloads. http://slsa.dev  \nRequirement  Description  L1 L2 L3 L4 \nScripted build  All build  steps  were  fully defined  in some  sort of build  \nscript.  The only manual command,  if any,  was  to invoke  \nthe build  script.  \nExamples:  \n Build  script  is Makefile,  invoked  via make  all. \n Build  script  is. github  / workflows  / build.yaml,  \ninvoked  by GitHub  Actions.      \nBuild service  All build  steps  ran using  some  build  service, not on a \ndevelopers workstation.  \nExamples:  GitHub  Actions,  Google  Cloud  Build, Travis \nCI.  \n   \nEphemeral  \nenvironment  The build  service  ensured  that the build  steps  ran in an \nephemeral environment,  such  as a container  or VM, \nprovisioned  solely  for this build, and not reused  from  a \nprior  build.   \n  \nIsolated  The build  service  ensured  that the build  steps  ran in an \nisolated  environment  free of influence  from  other build  \ninstances,  whether  prior  or concurrent. \n It MUST  NOT  be possible  for a build  to access any \nsecrets  of the build  service,  such  as the \nprovenance  signing  key.  \n It MUST  NOT  be possible  for two builds  that \noverlap in time  to influence  one another.  \n It MUST  NOT  be possible  for one build  to persist  or \ninfluence the build  environment  of a subsequent  \nbuild. \n Build  caches,  if used,  MUST  be purely content -\naddressable to prevent  tampering.     \n  \nParameterless  The build  output  cannot  be affected  by user  parameters \nother than  the build  entry point  and the top-level \nsource  location.  In other  words, the build  is fully \ndefined  through  the build  script  and nothing  else.  \n \nExamples:  \n GitHub  Actions workflow  dispatch  inputs  MUST  be \nempty.      \n \n\n \nSecuring the Software Supply Chain: Recommended Practices for Developers  41 \n  \n Google  Cloud  Build  user -defined  substitutions  \nMUST  be empty.  (Default  substitutions,  whose  \nvalues  are defined  by the server, are acceptable.)  \nHermetic  All transitive  build  steps,  sources,  and dependencies  \nwere  fully declared up front  with  immutable  references , \nand the build  steps  ran with  no network  access.  \nThe developer -defined  build  script:  \n MUST  declare all dependencies,  including sources  \nand other build  steps,  using  immutable  references  \nin a format  that the build  service  understands.  \nThe build  service:  \n MUST  fetch  all artifacts  in a trusted control  plane.  \n MUST  NOT  allow  mutable  references.  \n MUST  verify  the integrity  of each  artifact.  \no If the immutable  reference  includes a \ncryptographic  hash,  the service  MUST  verify  \nthe hash  and reject  the fetch  if the verification  \nfails.  \no Otherwise,  the service  MUST  fetch  the artifact  \nover  a channel  that ensures  transport \nintegrity,  such  as TLS or code  signing.  \n MUST  prevent  network  access  while  running  the \nbuild  steps.  \no This requirement  is best  effort.  It SHOULD  \ndeter  a reasonable  team  from  having  a non-\nhermetic  build, but it need  not stop  a \ndetermined  adversary. For example, using  a \ncontainer  to prevent  network  access  is \nsufficient.      \n \nReproducible  Re-running  the build  steps  with  identical input  artifacts  \nresults in bit-for-bit identical output. Builds  that cannot  \nmeet  this MUST  provide  a justification  why  the build  \ncannot  be made  reproducible. \n means  that this requirement  is best  effort. The \ndevelope r-provided build  script  SHOULD  declare \nwhether  the build  is intended  to be reproducible or a \njustification  why  not. The build  service  MAY  blindly \npropagate  this intent  without  verifying  reproducibility.  \nA customer  MAY  reject  the build  if it does  not \nreproduce.    \n \n  \n\nSecuring the Software Supply Chain: Recommended Practices for Developers  42 \n \n \n 3.4 Appendix  D: Artifacts and Checklist  \nIn principle, any artifacts created during the lifecycle  of the software development process are \nowned by a developing organization. These organizations can determine what artifacts are made \navailable with potential and current customers  of a product with or without a Non -Disclosure \nAgreement (NDA). Availabilit y of information must take into consideration regulatory and legal \nrequirements, the customer requirements for the information and the risk involved by exposing \ninformation leading to the exploitation of the product. Exceptions may include open source  \ndeve lopment organizations, which are more inclined to make all development information available, \nto include source code.  \nWhen defining the availability of an artifact, the general terms used in this section will be the following:  \n1. Publicly disclosed , \n2. Externally available : \na) under a Non -Disclosure Agreement (NDA) , \nb) government agency mandated requirement . \n3. Private / company confidential . \nThe availability of an artifact varies between companies and agencies and is only described here as a \nreference for what might be p ossible when using artifacts to validate the software supply chain \nprocess. Some artifacts, such as a high -level architecture document may be intentionally generated \nto allow any perspective consumers an introductory artifact detailing the overall strategi es used in \nthe design, development, and operation of a product. These publicly disclosed documents may describe common industry nomenclature, such as Federal Information Process Standards ( FIPS ) \ncompliance, cryptography standards used, development processes adhered to or certifications processes passed. NDA and government mandated availability require contractual agreements \nproviding access to artifacts that would not normally be exposed by the organization that produced \nthe product . While private  or company confidential artifacts are generally low- level and detailed \nwork products that may contain sensitive secrets and knowhow and if exposed, provide potential \ninsight into products competitive implementation and threat vectors that may not be addresse d in \nthe product, therefor posing a threat if exposed outside of the producers environment. Private/company confidential artifacts are generally maintained by the Suppliers and Developers of the product to facilitate the auditing and validation of adherence to the Secure Software \nDevelopment Lifecycle  (Secure SDLC) and s ecurity practices set forth by the product owner, \ncompany, or organization. For more information on the Secure SDLC process, refer to Section 2.1 \nSecure Product Criteria and Management , subsection Recommended Mitigations, Item 8.  \nMost of the artifacts collected during the development lifecycle  are not meant to be shared outside \nthe developing organization yet may be preserved in persistent storage as evidence to verify the integrity of the policies and processes used during the development of a product. A developer should \nsecurely retain artifacts of software development for a certain duration according to the  secure \nsoftware development policies and processes. As a by -product of the pro cess used to implement and \nmitigate the attack surface and threat model of the software as well as the software build pipeline \nduring the development process, the following artifacts may be created, and collected:  \n  \n\n \nSecuring the Software Supply Chain: Recommended Practices for Developers  43 \n  \nArtifact Examples  Description/Purpose  \nHigh -level Secure \nDevelopment \nLifecycle Process \ndocument  Attestation to secure development practices which can cover:  \n Secure software architecture/design process  \n Attack surface investigation and threat modeling process  \n Secure software development/programmi ng training  \n Software security testing process  \n Source control check -in process  \n Trusted repository for modules and processes  \n Continuous integration and delivery (CI/CD) processes  \n Defect/vulnerability reporting and customer update process  \n Code review process  for security and continuous software security \nimprovement  \n Continuous verification of third -party binaries  \n Open source management practices  \n Hardening the build environment  \n Secure relationship with a third -party supplier  \n Process to secure the signing server  \n Final package validation process  \nProduct Readiness checklist  Attestation to product release, product readiness for shipment, and secure shipping criteria which can cover:  \n No pending known critical bugs and vulnerabilities (e.g. bug track report) \n Cryptographically signed components  \n Proper software licensing  \nProduct \nSupport/Response \nPlan  Attestation to vulnerability discloser and response process (e.g. handling \nof policy violation and anomalies)  \nSoftware Bill of \nMaterial (SBOM)   Atte station to the integrity of the producer  \n Attestation to the security and authenticity of components \nincluded in the product  \n Attestation to the third -party software components  \n Attestation to the integrity of software licenses  \nArchitecture/Design \nDocuments   Attestation to secure architecture/design practices  \n Mitigation of attack surface vulnerabilities  \n Attestation to mapping secure requirements to software \narchitecture and components  \nDeveloper Training \nCertificates/Training \nCompletion \nStatistics/data   Attestation to secure development practices  \n Attestation to secure coding practices  \nThreat Model Results \nDocument   Attestation to secure design practices  \n Attestation to secure third- party component integration practices  \n\nSecuring the Software Supply Chain: Recommended Practices for Developers  44 \n \n \n High -level Software \nSecurity Test Plan and \nResults  High -level, system and unit level test plan and results (A set of tests should \nbe commensurate with the requirements and risk profile of the product or service.)  \n Coverage details  \n SAST - Static Application Security Testing  \n DAST - Dynamic Application Security Testing  \n SCA - Software Composition Analysis  \n Fuzzing/Dynamic  \n Penetration  \n Red team testing  \n Black box testing  \n QA security feature analysis \nAutomatic and Manual \nDynamic and Static \nSecurity/Vulnerability \nReports (Security \nScanning Results) \nReports  The reports can cover:  \n Security Scanning Results for Static, Dynamic, Software \nComposition Analysis and Fuzzing  \n Security Scanning Results for Penetration or Red -Teaming  \n Attestation to secure development/build/test practices  \n Mitigation against known software weakness classes in the \nCommon Weakness Enumeration (CWE)  \n Mitigation against publicly known vulnerabilities and Common \nVulnerabilities and Exposures (CVEs)  \nOpen source Review \nProcess Docum ent \nand Allowed List Attestation to secure open source review process and management  \nBuild Log   Attestation to the integrity of securely built products  \n Attestation to no known critical errors/warnings  \n Attestation to use of tool -chain defenses (stack checking, ASLR, \netc.)  \nSecure Development \nBuild Configurations \nListing   Attestation to secure build environment  \nThird- Party Software \nTool-Chains List   Attestation to secure build environment  \nThe artifacts described in the table above may be used for attest ation of the integrity of an \norganization s secure development process that was used to produce a given product. \nOrganization s can then provide a high -level checklist, illustrated below, which may utilize artifacts \ncreated during the development process th at attest to the adherence, at some level, to  the \nrecommended practice during the development process. The developer may add a brief description \nregarding how the organization supports a check list item in addition to Yes/No/Not Applicable (NA)/Incomplete (Inc) response, e.g. alternative practices to support it and reasons for non -\napplicability.  \n  \n\n \nSecuring the Software Supply Chain: Recommended Practices for Developers  45 \n  \nThe document  references in the below t able are focused on the Developer portion  of this series. \nMeasurable Outcome/ \nDescription  Practice \nObserved  \nYes, No,  \nNA, Inc SSDF \nTasks  Artifact Examples  Document References  \nSecure Product Criteria & Management  \nDo you define policies that \nspecify risk -based software \narchitecture and design requirements?   PO.1.2  Architecture/Design Documents 2.1 Secure Product \nCriteria and Management  \nDo you require team members \nto regularly participate in \nsecure software architecture, \ndesign, development, and \ntesting training and monitor \ntheir training completion?   PO.2.2  \nRV.3.4 Training Completion \nData/Statistics  \nDeveloper Training \nCertificates 2.1 Secure Product \nCriteria and \nManagement  \n2.2.1 Secure Software \nDevelopment/ \nProgramming Training  \nHave development team \nmembers attended training \nprograms specific to their \nroles, development tools and \nlanguages to update their \nskills?   PO.2.2  Training Completion Data/Statistics  \nDeveloper Training Certificates 2.2.1 Malicious Modification of Source \nCode Threat Scenario, \nSubsection 6: Secure \nProduct Criteria and \nManagement  \nAt a minimum, for all critical \nsoftware components and \nexternal services  that your \nteam operates and own s, have \nyou completed the attack \nsurface survey and threat \nmodels for all such services?   PW.1.1 \nPW.2.1  Threat Model Results \nDocuments   2.1 Secure Product \nCriteria and \nManagement  \n2.2.1 Malicious \nModification of Source \nCode Threat Scenario  \nSubsection 5. \nRequirements to Design \n/ Development Feature \nMapping  \nDo you have up to date threat \nmodels for all critical \ncomponents your team ships \nthat have been reviewed by a \nperson trained in software \nsecurity and do you make this \ndocument available to other teams that pick up your component?   PW.1.1, PW.2.1  Threat Model Results \nDocument   2.1 Secure Product \nCriteria and Management  \n\nSecuring the Software Supply Chain: Recommended Practices for Developers  46 \n \n \n Has your team held a black -\nbox investigation for security?    Black box test results  2.1 Secure Product \nCriteria and \nManagement  \n2.3.1 Third -Party \nBinaries Threat \nScenario  \nDo you have and use security \ntools and methodology (e.g. recommended by NISTIR \n8397) for static, dynamic and \nSoftware Composition Analysis \nand ensure that all high severity issues are addressed?   PO.3.1  SAST, DAST, SCA test \nresults 2.1 Secure Product \nCriteria and Management  \n2.3.2 Selections and Integration Threat Scenario  \nDo you perform input fuzzing \nas part of a regular process for \nyour component or product's \ninputs?   PW.8.2  Fuzzing/Dynamic \ntest results  2.1 Secure Product \nCriteria and \nManagement  \nDo you have security testing as \npart of your overall QA plan to \nenhance the testing of specific \nfeatures of your product?    Product test results  2.1 Secure Product \nCriteria and \nManagement  \nHas your product or \ncomponents been identified as \nneeding penetration testing? If \nso, are all issues found \nrecorded in a bug tracker, with \nhigh priority defects set to \nprevent shipment of the \nproduct?   PW.8.2  Penetration Test \nResults  2.1 Secure Product \nCriteria and \nManagement  \nHas your product or \ncomponents been identified as \nneeding red -team testing? If \nso, are all issues found \nrecorded in a bug tracker, with \nhigh priority defects set to \nprevent shipment of the \nproduct?    Red- Team Test \nResults  2.4.3 Signing Server \nExploits Threat Scenario  \nHas your product or \ncomponents been identified as \nneeding testing for security \ngaps by an external party? If \nso, has your code or systems \nbeen tested for security gaps \nby an external party (e.g. JFAC \nSoftware Assurance providers, \npen testing, threat model   Third -party Test \nResults  2.1 Secure Product \nCriteria and \nManagement  \n\n \nSecuring the Software Supply Chain: Recommended Practices for Developers  47 \n  \nreviews, vulnerability scan \ntools and red- teams)?  \nDoes your release include an \nSBOM and confirmation that no unacceptable security vulnerabilities are pending, binaries are digitally signed and meet cryptographic standards?   SBOM  \nProduct Bug Tracking Report  2.1 Secure Product \nCriteria and Management  \n2.2.5 Defect/Vulnerability Customer Issues Report Scenario  \n2.2.6 External \nDeveloper Extensions \nThreat Scenario  \n2.3.5 Software Bill of \nMaterial (SBOM) Threat \nScenario  \n2.5.1 Final Package Validation Threat \nScenario  \nAre all public cloud resources \ncontinuously monitored by a \ntool that analyzes and alerts \nfor policy violations and \nanomalies?    Product Support / \nResponse Plan  2.1 Secure Product \nCriteria and \nManagement  \n2.2.6 External \nDevelopment \nExtensions Threat \nScenario  \nAre the alerts being actively monitored?    Product Support / Response Plan  2.2.5 Defect/ \nVulnerability Customer Issues Report Scenario  \nIs there a process in place to \nresolve policy violations \nwithin a specific amount of \ntime?    Product Support / \nResponse Plan  2.1 Secure Product \nCriteria and \nManagement  \n2.2 Develop Secure \nCode \n2.2.5 \nDefect/Vulnerability \nCustomer Issues Report \nScenario  \nDevelop Secure Code  \n\nSecuring the Software Supply Chain: Recommended Practices for Developers  48 \n \n \n Are all of your security issues \ntracked with a bug tracker and \nscored, for example using \nCVSSv3 scores to help \ndetermine fix prioritization \nand release scheduling?   RV.2.1 Secure Software \nDevelopment \nLifecycle  Process \ndocument  \nBug Tracker Report  2.1 Secure Product \nCriteria and \nManagement  \n2.2.1 Malicious \nModification of Source \nCode Threat Scenario  \n2.2.1.2. Automatic and \nManual Dynamic and \nStatic \nSecurity/Vulnerability \nScanning \nDo you use access -controlled \napplications to store sensitive vulnerability information for \nall issues affecting production \ncode that is more restrictive \nthan plain bug tracker defects?   PO.5.1  Secure Software \nDevelopment Lifecycle  Process \ndocument  2.4.1 Build Chain \nExploits Threat Scenario  \nDoes your team have a process \nto reduce a class of \nvulnerabilities based on \npreviously identified \nvulnerabilities or incidents ?  PW.7.2  Secure Software \nDevelopment \nLifecycle  Process \ndocument  2.2.1 Malicious \nModification of Source \nCode Threat Scenario  \n2.2.5 \nDefect/Vulnerability \nCustomer Issue Reports \nThreat Scenario  \nDo you perform nightly builds \nwith automated regression \nand security test to quickly \ndetect problems with recent builds?    Secure Software \nDevelopment Lifecycle  Process \ndocument  2.2.1 Malicious \nModification of Source \nCode Threat Scenario \nSubsection 3: Nightly \nBuilds with Regression \nTest Automation Plan  \nAre code check -ins gated by \ncode collaborators and source \ncontrol to prevent anyone \nfrom accidentally or \nintentionally submitting un -\nreviewed code changes?   PW.7.2  Secure Software \nDevelopment \nLifecycle  Process \ndocument  2.2.1 Malicious \nModification of Source \nCode Threat Scenario \nSubsection 1: Source \nControl Check -In \nProcess  \n2.3.1 Malicious \nModification of Source \nCode Threat Scenario  \n2.2.1 Malicious \nModification of Source \nCode Threat Scenario: \nSubsection 6. Secure \nSoftware Development/ \nProgramming Training  \n\n \nSecuring the Software Supply Chain: Recommended Practices for Developers  49 \n  \nDoes the team require code reviews for all code and build \nscripts / configuration \nchanges?   PW.7 Secure Software \nDevelopment Lifecycle  Process \ndocument  2.4.1 Build Chain \nExploits Threat Scenario  \nDoes the team measure and \nanalyze the quality of the code \nreview process?    Secure Software \nDevelopment \nLifecycle  Process \ndocument  2.1 Secure Product \nCriteria and \nManagement  \n2.2.1 Malicious \nModification of Source \nCode Threat Scenario  \nSubsection 4. Code \nReviews  \nDo you ensure only required \nmodules are included in the product and unused modules and code out of scope of the requirements and design document are uninstalled or removed, mitigating living-off-the-land attacks and \ndecreasing the attack surface?    Secure Software Development Lifecycle  Process \ndocument  \nRequirements Document  2.2.1 Malicious \nModification of Source \nCode Threat Scenario \nSubsection 5: \nRequirements to Design/ Development \nFeature Mapping  \nDo you map all your security \nrequirements to the software \ncomponent of the product and \ntrack their \ncompletion/adherence?    Secure Software \nDevelopment \nLifecycle  Process \ndocument  \nSecurity \nRequirements \nDocument  2.2.1 Malicious \nModification of Source \nCode Threat Scenario \nSubsection 5: \nRequirements to \nDesign/ Development \nFeature Mapping  \nAre unmodified third -party \nlibraries retrieved from a \ncommon location such as a \nsecured persistent storage or \nshared repository location out \nof band of the development \nprocess and not individually \nbuilt by your team?    Secure Software Development \nLifecycle  Process \ndocument  2.1 Secure Product \nCriteria and \nManagement  \n2.2.4 Code Integration \nThreat Scenario  \n2.3.1 Third- Party \nBinaries Threat \nScenario  \n2.3.3 Obtain Components from a \nKnown and Trusted \nSupplier Threat \nScenario  \n\nSecuring the Software Supply Chain: Recommended Practices for Developers  50 \n \n \n 2.3.4 Component Maintenance Threat \nScenario  \nDo you monitor new \nvulnerabilities applicable to \nyour software e.g. using \nregistered vulnerability \nnotification services?   RV.1.1 Secure Software \nDevelopment \nLifecycle  Process \ndocument  2.3 Verify Third -Party \nComponents  \n2.5.1 Final Package \nValidation Threat \nScenario  \nDo you have and adhere to \nresponsible disclosure requirements for all externally identified vulnerabilities?    Secure S oftware \nDevelopment Lifecycle  Process \ndocument  2.2.5 Detect / \nVulnerability Customer \nIssue Reports Threat \nScenario  \nAre all of your builds \ncontinuously built and tested?    Secure Software \nDevelopment \nLifecycle  Process \ndocument  2.4.2 Build Chain \nExploits; Advanced \nPractices Threat \nScenario  \nDoes a check -in immediately \ntrigger a build?    Secure Software Development \nLifecycle  Process \ndocument  2.4.2 Build Chain \nExploits; Advanced \nPractices Threat \nScenario  \nDoes a completed build \nautomatically go through some \nacceptance testing?    Secure Software \nDevelopment \nLifecycle  Process \ndocument  2.2.1 Malicious \nModification of Source \nCode Threat Scenario \nSubsection 3: Nightly \nBuilds with Regression \nTest Automation  \nIf the testing passes, is the \nbuild automatically deployed \nso others can consume it?   PO.3.1  Secure Software \nDevelopment \nLifecycle  Process \ndocument  2.2.1 Malicious \nModification of Source \nCode Threat Scenario \nSubsection 3: Nightly \nBuilds with Regression \nTest Automation  \nVerify Third- Party Components  \nDo you track all third -party \ncomponents you use directly and all internal components in \na secure and persistent \nrepository?  PS.1.1  \nPW.4.1  Secure Software \nDevelopment Lifecycle  Process \ndocument  \nOSRB Approved List  \nProduct/Component Scan Results  2.2.2 Open source \nManagement Practices  \n2.3.1 Third -Party \nBinaries Threat \nScenario  \nDo you have the requirement \nfor an Open source Review \nBoard to approve third -party  PW.4.1  \nPW.4.4  Secure Software \nDevelopment 2.3.3 Obtain \nComponents from a \nKnown and Trusted \n\n \nSecuring the Software Supply Chain: Recommended Practices for Developers  51 \n  \nlibraries included in a product \nand audit approved third -\nparty libraries for version \nadherence and vulnerabilities ? Lifecycle  Process \ndocument  \nOSRB Approved List  Supplier Threat \nScenario  \nDo you remove or mitigate critical known vulnerabilities \nor end of life issues of third -\nparty components before each release?   PW.4.5  Secure Software \nDevelopment \nLifecycle  Process \ndocument  \nOSRB Approved List  2.2.4 Code Integration \nThreat Scenario  \nWhen considering the \nselection of a third -party \ncomponent, do use a known \nand trusted supplier that has a \nproven record for secure \ncoding practices and quality \ndelivery of their components?   PO.1.3  Secure Software \nDevelopment \nLifecycle  Process \ndocument  \nOSRB Approved List  2.3.3 Obtain \nComponents from a \nKnown and Trusted \nSupplier Threat \nScenario  \nWithin a developer \nenvironment, do you monitor \nand approve of all IDEs and third -party \ndevelopment/debugging \nextensions to ensure their \nadoption does not weaken the \nsecurity posture of the local \ndevelopment environment?    Secure Software Development \nLifecycle  Process \ndocument  2.2.3 Use of Secure \nThird -Party Software \nTool -Chains and \nCompatibility Libraries  \n2.2.6 External \nDevelopment \nExtensions Threat \nScenario  \nDo you have a trusted \nrepository to support ongoing \nsoftware composition analysis \nand security testing for all \nexternal and downloade d \nmodules?    Secure Software \nDevelopment \nLifecycle  Process \ndocument  2.2.4 Code Integration \nThreat Scenario  \nHarden the Build Environment  \nHave you completed attack \nsurface investigation and \nthreat modeling for your build \nenvironment?    Threat/Risks Model \nResults Documents  2.1 Secure Product \nCriteria and \nManagement  \n2.2.1 Malicious \nModification of Source \nCode Threat Scenario: \nSubsection 7. Harden \nthe Development \nEnvironment  \n\nSecuring the Software Supply Chain: Recommended Practices for Developers  52 \n \n \n Do you ensure that only in \nvery rare cases, the build process accesses the open Internet and these cases are documented and approved within the security plan?   PO.5.1  Secure Software Development Lifecycle  Process \ndocument  2.4.1 Build Chain Threat \nScenario  \nDo you limit and secure access \nto your development \nenvironment to essential \nadministrators?    Secure Software \nDevelopment \nLifecycle  Process \ndocument  2.4 Harden the \nDevelopment \nEnvironment  \nDo you monitor the build chain for unauthorized access and \nmodifications?    Secure Software \nDevelopment \nLifecycle  Process \ndocument  2.4 Harden the \nDevelopment \nEnvironment  \nDo you document approval \nand audit logs of build chain \nmodifications?    Secure Software \nDevelopment \nLifecycle  Process \ndocument  2.4 Harden the \nDevelopment \nEnvironment  \nDo you enforce build -chain \nconfiguration defensive techniques required to narrow \nthe attack vectors of the \ncomponents and products \nbeing developed?    Secure Software Development \nLifecycle  Process \ndocument  \nBuild Logs  2.2.3 Use of Secure \nDevelopment Build Configurations  \nDo you ensure the integrity of \nthe individual development \nenvironment, caring to harden \nthe development systems \nwithin the build pipeline?    Secure Software \nDevelopment \nLifecycle  Process \ndocument  2.2.3 Secure Developer \nEnvironment  \nDoes your build process encrypt data in transit?    Secure Software Development \nLifecycle  Process \ndocument  2.5.3 Secure the \nDistribution System \nThreat Scenario  \nDoes each critical server \nwithin the build chain owned \nby the team have a clearly \ndefined owner responsible for \npatch maintenance?   PO.5.1  Secure Software \nDevelopment \nLifecycle  Process \ndocument  2.1 Secure Product \nCriteria and \nManagement  \n2.4.2 Build Chain \nExploits; Advanced \nPractices Threat \nScenario  \nDo you have a requirement \nthat server patch levels are checked periodically ?   Secure Software \nDevelopment \nLifecycle  Process \ndocument  2.1 Secure Product \nCriteria and \nManagement  \n\n \nSecuring the Software Supply Chain: Recommended Practices for Developers  53 \n  \nIs unnecessary outbound \ninternet connectivity blocked?   PO.5.1  Secure Software \nDevelopment \nLifecycle  Process \ndocument  2.2.3 Secure \nDevelopment Practices \nThreat Scenario \nSubsection 1: Secure the \nDeveloper Environment  \n2.4.1 Build Chain \nExploits Threat Scenario  \nIs unnecessary inbound \ninternet connectivity blocked?   PO.5.1 Secure Software Development \nLifecycle  Process \ndocument  2.2.3 Secure \nDevelopment Practices \nThreat Scenario Subsection 1: Secure the \nDeveloper Environment  \n2.4.1 Build Chain Exploits Threat Scenario  \nIs the integrity of the builds \nverified to ensure no malicious \nchanges have occurred during \nthe build and packaging \nprocess, for example, are two \nor more builds performed in \ndifferent protected \nenvironments and the results \ncompared to ensure the \nintegrity of the build process?    Secure Software \nDevelopment \nLifecycle  Process \ndocument  2.4.2 Build Chain \nExploits; Advanced \nPractices Threat \nScenario  \nDo you use the toolchain to automatically gather \ninformation that informs \nsecurity decision -making?   P0.4.2  Secure Software Development \nLifecycle  Process \ndocument   \nDoes the tool chain \nautomatically scan for \nvulnerabilities and stop the \nbuild process and report \nerrors when detected, if so \nconfigured?  PS.1.1  \nPW.7.2  Secure Software \nDevelopment \nLifecycle  Process \ndocument  2.2.1 Malicious \nModification of Source \nCode Threat Scenario \nSubsection 2: Automatic \nand Manual Dynamic \nand Static Security / \nVulnerability Scanning  \n2.2.4 Code Integration \nThreat Scenario  \nDo you store access credentials (e.g. hashes for \npasswords) and secrets in a \nsecure (e.g. encrypted) \nlocation such as a secure vault?     2.4.1 Build Chain Exploits Threat Scenario  \nSecure Code Delivery  \n\nSecuring the Software Supply Chain: Recommended Practices for Developers  54 \n \n \n  \n  Do you perform binary \ncomposition analysis of the final package?    Secure Software \nDevelopment \nLifecycle  Process \ndocument  2.5.1 Final Package \nValidation Threat Scenario  \nDo you have a Software Bill of \nMaterials (SBOM) that satisfies \nthe contracts?   PS.3.2  \nPW.4.1   2.5.1 Final Package \nValidation Threat \nScenario  \nDo you digitally sign all \nrequired binaries you ship?   PS.1.1  \nPS.2.1  Secure Software \nDevelopment \nLifecycle  Process \ndocument  2.5.2 Instrument \nIntegrity Checks, Code \nSigning and Hashing \nThreat Scenario  \nDo you ensure that no \nglobally- trusted certificates \nare directly accessible and use \na dedicated, protected signing \nserver when signing is \nrequired?   Secure Software \nDevelopment \nLifecycle  Process \ndocument  2.4.3 Signing Server \nExploits  \nAre you using organization approved Configuration \nManagement tools to sign your \nshipping binaries?    Secure Software Development \nLifecycle  Process \ndocument  2.4.2 Build Chain \nExploits; Advanced \nPractices Threat \nScenario  \nDo you comply with the use of \ncryptography recommended \nby the organizations security \npolicy?   PS.1.1  Secure Software \nDevelopment \nLifecycle  Process \ndocument  2.5.3 Secure the \nDistribution System \nThreat Scenario  \n2.4.3 Signing Server \nExploits  \n\n \nSecuring the Software Supply Chain: Recommended Practices for Developers  55 \n  \n3.5 Appendix  E: Informative R eferences  \nAbbreviation  Document  Name  \nACM Communications of the ACM  17, The Protection of Information in Computer \nSystems . Available at \n(http://web.mit.edu/Saltzer/www/publications/protection/index.html ) \nBSIMM10  Migues  S, Steven  J, Ware  M (2019)  Building Security  in Maturity  Model (BSIMM) \nVersion  10. Available  at (https://www.bsimm.com/download/ )  \nCISA  Cybersecurity & Infrastructure Security Agency. Available at \n(https://www.cisa.gov/defining -insider -threats ) \nCISCO_SDLC  Cisco.  2021. Cisco  Secure  Development  Lifecycle.  Available  at \n(https://www.cisco.com/c/dam/en_us/about/doing_business/trust -\ncenter/docs/c isco -secure -development -lifecycle.pdf ) \nDoD CIO  DoD Enterprise, 2021. DevSecOps  Fundamentals  Lifecycle Phases  Version 2.0. \nAvailable at \n(https://dodcio.defense.gov/Portals/0/Documents/Library/DoDEnterpriseDevS\necOpsFundamentals.pdf ) \nEO14028 EOP.  2021. Improving  the Nations  Cybersecurity,  Executive  Order 14028, 86 \nFR 26633, Document  number  2021 - 10460. Available  at \n(https://www.whitehouse.gov/briefing -room/presidential -\nactions/2021/05/12/executive-order- on-improving -the-nations -\ncybersecurity /) \nFIPS140  National Institute  of Standards and Technology.  2019.  Security  Requirements  \nfor Cryptographic Modules. Available  at \n(https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.140 -3.pdf ). \nIDASOAR Hong  Fong  EK, Wheeler  D, Henninger  A (2016)  State -of-the-Art Resources  \n(SOAR)  for Software  Vulnerability  Detection,  Test,  and Evaluation  2016. \n(Institute  for Defense Analyses  [IDA],  Alexandria, VA),  IDA Paper  P-8005. \nAvailable  at (https://www.ida.org/research -and-\npublications/publications/all/s/st/stateoftheartresources -soar -for-software -\nvulnerability- detection -test-and-evaluation -2016) \nINTEL  Intel.  Software  Supply  Chain  Threats;  A White  Paper  Version 1.0, July 2021. \nAvailable at https://www.intel.com/content/www/us/en/security/supply -\nchain -threat -whitepaper.html  \nISO27034 International  Organization  for Standardization/International Electrotechnical  \nCommission  (ISO/IEC),  Information  technology   Security  techniques  \nApplication  security   Part  1: Overview  and concepts,  ISO/IEC  27034 -1:2011, \n2011. Available  at (https://www.iso.org/standard/44378.html ) \nMITRE_CAPEC  MITRE.  2021. Common  Attack  Pattern  Enumeration  and Classification.  Available  \nat (https://capec.mitre.org/data/definitions/437.html ) \nMITRE_CVE  MITRE.  2021. Common  Vulnerability and Exposure,  CVE.  2021. Available  at \n(https://cve.mitre.org/index.html ). \n\nSecuring the Software Supply Chain: Recommended Practices for Developers  56 \n \n \n MSSDL  Microsoft  (2019)  Security  Development  Lifecycle.  Available  at \nhttps://www.microsoft.com/en -us/sdl \nNASASTD8739 National Aeronautics  and Space  Administration.  2021. SOFTWARE  ASSURANCE  \nAND  SOFTWARE  SAFETY  STANDARD,  NASA -STD -8739.8A.  Available  at \n(https://standards.nasa.gov/sites/de fault/files/standards/NASA/PUBLISHED/A\n1/nasa -std-8739.8a.pdf ). \nNICCS  National Initiative  for Cybersecurity  Careers and Studies,  National Initiative  for \nCybersecurity  Education. 2021  Workforce  Framework for Cybersecurity  (NICE  \nFramework).  Available  at (https://niccs.cisa.gov/workforce -\ndevelopment/cyber -security -workforce -framework ) \nNISTCSF  National Institute  of Standards and Technology.  2018.  Framework for \nImproving  Critical  Infrastructure Cybersecurity,  Version  1.1.  Available  at \n(https://doi.org/10.6028/NIST.CSWP.04162018 ) \nNISTMSDV  National Institute  of Standards and Technology.  2018.  Guidelines  on Minimum \nStandards for Developer  Verification  of Software. available at \n(https://www.nist.gov/system/files/documents/2021/07/13/Developer%20Ve\nrification%20of%20Software.pdf ) \nNTIASBOM National Telecommunications and Information  Administration.  2021. The  \nMinimum Elements  for a Software  Bill of Materials  (SBOM).  Available  at \n(https://www.ntia.doc.gov/report/2021/minimum -elements -software -bill-\nmaterials -sbom ) \nNVD  National Vulnerability Database. Available at (https://www.nist.gov/programs -\nprojects/national -vulnerability- database- nvd) \nOWASP_ASVS Open  Web  Application  Security  Project  (2019)  OWASP  Application  Security  \nVerification  Standard  4.0. Available  at https://github.com/OWASP/ASVS  \nOWASP_SAMM  Open  Web  Application  Security  Project  (2017)  Software  Assurance  Maturity \nModel Version  1.5. Available  at (https://owasp.org/www -pdf-\narchive/SAMM_Core_V1 -5_FINAL.pdf )  \nOWASP_SCVS  OWASP.  2021. OWASP  Software  Component  Verification  Standard.  Retrieved  \nSep.  25, 2021  (https://owasp.org/www -project -software -component -\nverification -standard/).  \nOWASP_TEST  Open  Web  Application  Security  Project  (2014)  OWASP  Testing Guide  4.0. \nAvailable  at (https://owasp.org/www -project -web -security -testing -\nguide/assets/archive/OWASP_Testing_Guide_v4.pdf )  \nPCI_SSLRAP  Payment  Card  Industry (PCI)  Security  Standards Council  (2019)  Secure  Software  \nLifecycle  (Secure  SLC)  Requirements  and Assessment  Procedures Version  1.0. \nAvailable  at \n(https://www.pcisecuritystandards.org/document_library?category=sware_sec\n#results ) \nSC_AGILE Software  Assurance  Forum  for Excellence in Code  (2012)  Practical  Security  \nStories  and Security  Tasks for Agile  Development  Environments.  Available  at \n(http://www.safecode.org/publication/SAFECode_Agile_Dev_Security0712.pdf ) \n\n \nSecuring the Software Supply Chain: Recommended Practices for Developers  57 \n  \nSC_FPSSD  Software  Assurance  Forum  for Excellence in Code  (2018)  Fundamental  Practices \nfor Secure  Software  Development:  Essential Elements  of a Secure  Development  \nLifecycle  Program, Third  Edition.  Available  at (https://safecode.org/wpcontent/ \nuploads/2018/03/SAFECode_Fundamental_Practices_for_Secure_Software_Dev  \nelopment_March_2018.pdf ) \nSC_SIC  Software  Assurance  Forum  for Excellence in Code  (2010)  Software  Integrity  \nControls:  An Assurance -Based  Approach  to Minimizing  Risks  in the Software  \nSupply  Chain.  Available  at \n(http://www.safecode.org/publication/SAFECode_Software_Integrity_Controls0\n610.pdf ) \nSC_TPC  Software  Assurance  Forum  for Excellence in Code (2017)  Managing  Security  \nRisks Inherent  in the Use of Third -Party  Components.  Available  at \n(https://www.safecode.org/wpcontent/uploads/2017/05/SAFECode_TPC_Whit\nepaper.pdf ) \nSC_TTM Software  Assurance  Forum  for Excellence in Code  (2017)  Tactical  Threat  \nModeling. Available  at \n(https://www.safecode.org/wpcontent/uploads/2017/05/SAFECode_TM_White\npaper.pdf ) \nSLSA  The Linux  Foundation.  2021. Improving  artifact  integrity  across  the supply  \nchain   SLSA.  Available  at (https://slsa.dev/ ) \nSP80050 National Institute  of Standards and Technology.  2021.  PRE -DRAFT  Call for \nComments:  Building a Cybersecurity  and Privacy  Awareness  and Training \nProgram, SPS 800 -50 Rev 1. Available  at \n(https://csrc.nist.gov/publications/detail/sp/800 -50/r ev-1/draft ). Retrieved  \nSep.  25, 2021.  \nSP80052 National Institute  of Standards and Technology.  2020.  Guidelines  for the \nSelection,  Configuration,  and Use of Transport Layer  Security  (TLS)  \nImplementations,  SP 800 -52 Rev.  2. Available  at \n(https://csrc.nist.gov/publications/detail/sp/800 -52/rev -2/final ). \nSP80053 National Institute  of Standards and Technology.  2020.  Security  and Privacy  \nControls  for Information Systems and Organizations . Available at \n(https://csrc.nist.gov/publications/detail/sp/800 -53/r ev-5/final ) \nSP80057 National Institute  of Standards and Technology.  2020.  Recommendation  for Key \nManagement:  Part  1  General, SP 800 -57 Part  1 Rev.  5. Available  at \n(https://csrc.nist.gov/publications/detail/sp/800 -57-part -1/rev -5/final ) \nSP800160 National Institute  of Standards and Technology.  2018.  Systems  Security  \nEngineering.  Available  at (https://doi.org/10.6028/NIST.SP.800- 160v 1) \nSP800161 \"National Institute  of Standards and Technology.  2021.  \"\"Supply  Chain  Risk  \nManagement  Practices for Federal Information  Systems  and Organizations.\"\"  \nAvailable  at \n(https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800 -161.pdf )\" \n\nSecuring the Software Supply Chain: Recommended Practices for Developers  58 \n \n \n SP800172 Enhanced  Security  Requirements  for Protecting  Controlled Unclassified  \nInformation:  A Supplement  to NIST  SP 800 -171. Available at \n(https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800 -172.pdf) \nSP800175B  National Institute  of Standards and Technology.  2020.  Guideline  for Using \nCryptographic  Standards in the Federal Government:  Cryptographic \nMechanisms.  SP 800 -175B  Rev.  1. Available  at \n(https://csrc.nist.gov/publications/detail/sp/800 -175b/rev -1/final ). \nSP800181 National Institute  of Standards and Technology,  National  Initiative  for \nCybersecurity  Education. 2020. Workforce  Framework for Cybersecurity  (NICE  \nFramework).  Available  at \n(https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800 -181r1.pdf) \nSP800193 National Institute  of Standards and Technology.  2018.  Platform  Firmware  \nResiliency  Guidelines,  SP-800 -193.  Available  at \n(https://csrc.nist.gov/publications/detail/sp/800 -193/final ). \nSP800207 National Institute  of Standards and Technology.  2020. Zero -Trus t Architecture, \nSP-800 -207 . \nhttps://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800 -207.pdf  \nSSDF  National Institute  of Standards and Technology.  2020.  Mitigating the Risk  of \nSoftware  Vulnerabilities by Adopting  a Secure  Software  Development  \nFramework (SSDF).  Available  at \n(https://nvlpubs.nist.gov/nistpubs/CSWP/NIST.CSWP.04232020.pdf )  \nSWEBOK3  IEEE Computer  Society.  2014. Guide  to the Software  Engineering  Body  of \nKnowledge.  Available  at (https://www.computer.org/education/bodies -of-\nknowledge/software -engineeri ng/v3 ) \nSYNOPSYS  Synopsys.  2021. Synopsys  Information  Security  Requirements  for Vendors. \nAvailable  at https://www.synopsys.com/company/legal/info -security.html  \nZDNET  IBM,  ZDNET.  2021. Managing  a Software  as a Vendor  Relationship:  Best  \nPractices.  Available  at (https://www.zdnet.com/article/managing -a-software -\nas-a-service -vendor -relationship -best -practices/ ) \n \n  \n\n \nSecuring the Software Supply Chain: Recommended Practices for Developers  59 \n  \n3.6 Appendix  F: A cronyms  Used in T his Document  \nAcronym  Meaning  \nAPI Application Programming I nterface  \nASLR  Address Space  Layout  Randomization  \nCI/CD  Continuous  Integration/ Continuous  Delivery \nCNSSI  Committee on National Security Systems Instruction  \nCVE   Common Vulnerabilities and Exposures  \nCVSS  Common  Vulnerability  Scoring  System  \nCWE  Common  Weakness  Enumeration  \nDAST  Dynamic  Application  Security  Testing  \nDLP  Data Loss  Prevention  \nDUNS  Data Universal Numbering  System  \nEO Executive  Order \nEOL  End  of Life \nESF Enduring Security Framework  \nFARS/DFARS Federal Acquisition  Regulation/Defense  Federal Acquisition  Regulation  \nFedRAMP  Federal Risk  and Authorization  Management  Program \nFIPS  Federal Information  Process  Standards \nHIPAA  Health  Insurance  Portability  and Accountability  Act \nHSM  Hardware Security  Module  \nHTTP S Hypertext  Transfer  Protocol  Secure  \nIAST  Interactive Application Security Testing  \nIDE Integrated  Development  Environment  \nLAN Local Area Network  \nMFA  Multi Factor  Authentication  \nMITM  Man in The Middle \nML Machine  Language  \nNIAP  National Information Assurance Partnership  \nNIST  National Institute  of Standards and Technology  (US DOC)  \nNTIA  National Telecommunications and Information  Administration  (US DOC)   \nNVD  National Vulnerability Database  \nOpenSSF  Open Source Security Foundation  \nOSRB  Open source  Review  Board  \nOSS Open  Source  Software  \nOWASP Open  Web  Application  Security  Project  \nPO Prepare Organization  \n\nSecuring the Software Supply Chain: Recommended Practices for Developers  60 \n \n \n PS Protect Software  \nPSIRT  Product Security  Incident  Response  Team  \nPW Produce Well -Secured Software  \nQA Quality  Assurance  \nRACI Responsible,  Accountable,  Consulted, and Informed  \nRASP  Runtime Application Self -Protection  \nRM Risk  Management  \nROP Return -oriented  Programming \nRV Respond to Vulnerabilities  \nSaaS  Software -as-a-Service  \nSAST  Static  Application  Security  Testing  \nSBOM  Software  Bill of Material  \nSCA  Software Composition Analysis  \nSCM  Supply  Chain  Management  \nSCM  Source  Code  Management  \nSCRM  Supply  Chain  Risk  Management  \nSCVS  Software Component Verification Standard  \nSDLC  Software  Development  Lifecycle  \nSEH  Software  Exception  Handler  \nSHA  Secure  Hash  Algorithm  \nSIEM  Security  Information  and Event  Management  \nSLSA  Supply -chain  Levels  for Software  Artifacts  \nSOUP  Software  of Unknown Provenance  \nSOX  Sarbanes -Oxley  Act \nSPDX  Software Package Data eXchange  \nSSDF  Secure  Software  Development  Framework \nSSH  Socket Shell  \nSwA  Software  Assurance  \nSWID  Software Identification  \nTLS  Transport Layer  Security  \nTSA  Time  Stamp  \nUML  Unified  Modeling  Language  \nVAR  Value -added Reseller  \nVCS  Versi on Control  System  \nVM Virtual  Machine  \nVPN  Virtual  Private  Network \n \n\n",
  "cves": [],
  "techniques": [],
  "advisory": "cybersecurity-alerts",
  "title": "esf_securing_the_software_supply_chain_developers",
  "source": "nsa",
  "id": "530c8477b9f75baa931caa4812b2353bdf780a39742b208a331e18a7696345a5"
}