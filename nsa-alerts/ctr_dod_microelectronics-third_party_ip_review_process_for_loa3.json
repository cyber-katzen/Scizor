{
  "markdown": " \n \n \nNational Security Agency  \nCybersecurity  Technical Report  \n \n \n \n \n \nDoD Microelectronics:  \nThird -Party IP Review Process for  \nLevel of Assurance 3 \n \n \n \nJune  2023 \n \nU/OO/ 170670 -23 \nPP-23-1628  \nVersion 1.0 \n \n \n \n \n  \n  \n\n\n \nU/OO/170670 -23 | PP-23-1628  | JUN 2023 Ver. 1.0  ii \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA3  \n \n This document was created through collaboration with each of the \nJFAC labs: National Security Agency (NSA), Air Force Research \nLab (AFRL) RYDT, Naval Surface Warfare Center (NSWC) Crane, \nand Army Development Command (DEVCOM)/AVMC.  \nFor additional information, guidance, or assistance with this \ndocument, please contact the NSA Joint Federated Assurance \nCenter (JFAC) at JFAC_HWA@radium.ncsc.mil .  \n  \n  \n\n\n \nU/OO/170670 -23 | PP-23-1628  | JUN 2023 Ver. 1.0  iii \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA3  \nNotices and history   \nDocument change history  \nDate  Version  Description  \nJUN 2023 1.0 Initial Publication  \n   \nDisclaimer of warranties and endorsement  \nThe information and opinions containe d in this document are provided \"as is\" and without any warranties \nor guarantees. Reference herein to any specific commercial products, process, or service by trade name, \ntrademark, manufa cturer, or otherwise, does not  constitute or imply its endorsement, recommendation, or \nfavoring by the United States Government, and this guidance shall not be used fo r advertising or product \nendorsement purposes.  \nPublication information  \nAuthor(s)   \nNational Security Agency  \nCybersecurity Directorate  \nJoint Federated Assurance Center  \nContact information  \nNSA Joint Federated Assurance Center:  JFAC_HWA@radium.ncsc.mil    \nCybersecurity Report Feedback / General Cybersecurity Inquiries: CybersecurityReports@nsa.gov   \nDefense Industrial Base Inquiries an d Cybersecurity Services: DIB_Defense@cyber.nsa.gov   \nMedia inquiries / Press Desk: Media Relations, 443 -634-0721, MediaRelations@nsa.gov   \nPurpose  \nThis document w as developed in furtherance of NSAs cybersecurity missions . This  includ es its \nresponsibilities to identify and disseminate threats to National Security Systems, Department of Defense \ninformation systems, and the Defense Industrial Base, and to develop and  issue cybersecurity \nspecifications and mitigations. This information may be shared broadly to reach all appropriate \nstakeholders.   \n\n \nU/OO/170670 -23 | PP-23-1628  | JUN 2023 Ver. 1.0  iv \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA3  \nExecutive summary  \nThis report outlines a methodology of design review su fficient to validate that third -party \nintellectual property (3PIP) is appropriate to incorporate i nto a Level of Assurance 3 \n(LoA3) system.  \nIn this context , 3PIP refers to functions whose development are not under the control of \nthe designer.  Use of the phrase intellectual property, IP , or 3PIP in outli ning this \nmethodology of design review does not refer to property rights , such as, for example, \ncopyrights, patents, or trade secrets. It is the responsibility of the party seeking review \nand/or the reviewer to ensure that any rights needed to perform the review in \naccordance with the methodology outlined are obtained.  \nThis process is intended to mitigate the Threat  Description  #5 Adversary compromises \nthird-party soft IP, as described in the FPGA Best Practices  Threat Catalog . The \nprocess attempts to m inimize the required level of effort while providing sufficient \nmitigation for use of 3PIP at  LoA3. The review process, when c orrectly executed, \nestablishes two  independent organization s with personnel cleared at the Secret level.  \nThe adversary would have to compromise or deceive both organization s for an attack to \nsucceed. This directly increases the level of access  required to carry out an attack. The \nprocess assumes that technology and level of effort are not substantial barriers to an \nadversary in modifying the 3PIP design.  \nIn addition to LoA1 and LoA2 attacks, LoA3 includes less targetable attacks and attacks \nthat provide pre -positioning as part of a more complex attack . \nThis review process can be applied in two distinct ways. One i s to approve a 3PIP in \ngeneral. The second is to approve a 3PIP for use in a specific role within a specific \nsystem. For instance, this process can be used to review a memory controller IP for use \nin any application. In some circumstances, it may be less e xpensive to review that same \nIP, but only for use in a single application. In this case, information about the system \naround the IP might be used to avoid reviewing certain components in detail.  \nThe JFAC FPGA Level of Assurance 3 Best Practices  appendix s pecifies this flow as \none option to assure 3PIP. In most cases that will correspond to review within a certain \nsystem. The other mitigation offered is to provide the 3PIP to the JFAC. When JFAC \nelects to review the 3PIP, it will be a review for any system context.  \n\n \nU/OO/170670 -23 | PP-23-1628  | JUN 2023 Ver. 1.0  v \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA3  \nThis review process is summarized  below : \n Assemble review prerequisites  \n Establish suitability of the 3PIP for review  \n Partition the design  \n Perform manual code review  \n Perform test -driven code review  \n Document and sign the review package  \n \n  \n\n \nU/OO/170670 -23 | PP-23-1628  | JUN 2023 Ver. 1.0  vi \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA3  \nContents   \n \nDoD Microelectronics: Third -Party IP Review Process for Level of Assurance \n3 ................................ ................................ ................................ ................................ ........................ i \nExecutive summary  ................................ ................................ ................................ ......................  iv \nContents  ................................ ................................ ................................ ................................ ...... vi \n1. Introduction  ................................ ................................ ................................ ............................  1 \n2. Assemble review prerequisites  ................................ ................................ ...........................  1 \n2.1 Reviewer documentation  ................................ ................................ ................................ ................................ . 1 \n2.2 Record of review  ................................ ................................ ................................ ................................ .................  2 \n3. Establish suitability of the 3P IP for review  ................................ ................................ ........  3 \n4. Partition the design  ................................ ................................ ................................ ...............  5 \n4.1 Threats of interest  ................................ ................................ ................................ ................................ ..............  5 \n4.2 Controlled effects  ................................ ................................ ................................ ................................ ................  6 \n4.3 Persistent effects  ................................ ................................ ................................ ................................ ................  7 \n4.4 Defining a functional area  ................................ ................................ ................................ ...............................  8 \n4.5 Criteria for functional areas of interest  ................................ ................................ ................................ ....... 8 \n5. Perform manual code review  ................................ ................................ ...............................  8 \n6. Perform test -driven code review  ................................ ................................ .......................  10 \n7. Additional considerations  ................................ ................................ ................................ .. 11 \n8. Document and sign the review package  ................................ ................................ ..........  12 \n9. Pre-compiler and other machine -generated code  ................................ ...........................  13 \n10. Conclusion  ................................ ................................ ................................ ........................  13 \nAppendix A: Standardized terminology  ................................ ................................ ...................  15 \n \n \n\n \nU/OO/170670 -23 | PP-23-1628  | JUN 2023 Ver. 1.0  1 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA3  \n1. Introduction  \nThis report outlines a m ethodology  to perform a design review that is sufficient to \nvalidate a third-party intellectual property (3PIP) function for use within a Level of \nAssurance 3 (LoA3) system.  In this context, 3PIP refers to functions whose \ndevelopment are not under the control of the designer.  Use of the phrase intellectual \nproperty, IP , or 3PIP in outlining this methodology of design review does not refer to \nproperty rights , such a s, for example, copyrights, patents, or trade secrets. It is the \nresponsibility of the party seeking review and/or the reviewer to ensure that any rights \nneeded to perform the review in accordance with the methodology outlined are \nobtained . \n2. Assemble review  prerequisites  \nFor a 3PIP  review to be valid, the review itself must be thorough and auditable. \nProcedural elements related to the following must be in place for the duration of the \nreview : \n A clear record of who is conducting the review, and  \n The review process itself.  \n2.1 Reviewer documentation  \nA clear record of the individuals conducting the review must be maintained. This record \nmust justify why the individuals are well suited to the task. The information required is \nsimilar to the information collecte d by a company during the hiring process, such as \nidentification information and a resume. The following details must be included : \n Security suitability   For LoA3 3PIP  reviews , the individuals doing the review \nmust be  cleared, minimally at the Secret level . The organization performing the \nreview must maintain documentation proving that status.  \n Qualifications  Each reviewer must have a suitable degree and experience \nwith the design language or circuit type they review. For instance, a reviewer of \nVerilog co de must have an understanding of digital design typically acquired with \na degree in computer or electrical engineering and experience writing Verilog. \nLikewise, a reviewer of Very High Speed Integrated Circuit Hardware Description \nLanguage  (VHDL ) must simi larly have experience with VHDL. Furthermore, if the \n3PIP incorporates hard IP blocks, specific to the Field Programmable Gate Array \n\n \nU/OO/170670 -23 | PP-23-1628  | JUN 2023 Ver. 1.0  2 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA3  \n(FPGA ) platform, the reviewer must have experience with platforms from that \nvendor1. \n Independence   No reviewer should have  worked on the development of the \n3PIP. No reviewer may be a current employee of, or contractor to, the company \nthat designed or sells the 3PIP.  \n2.2 Record of r eview   \nThe review process itself must be auditable such that each decision is traceable to at \nleast two specific reviewers. In addition, time spent conducting the review must be \nrecorded for each decision. The time expected to complete the review will vary greatl y \ndepending on block complexity. The specific information to gather and document is \nsimilar to the information collected automatically by many  code review tools. \nSpecifically, for each artifact reviewed , the following must be maintained:  \n Identity of the re viewer   The name, or other ide ntifier, of the individual who \nperformed the review , and the c learance level of the reviewer at the time of \nreview.  \n Times of review   A record describing the day or days on which the review took \nplace, as well as the duration  of the review . \n Specific material reviewed   A record of the specific design files reviewed to \nmake the determination.  \n Tests run during the review   A rec ord of any functional tests run by the \nreviewers, including test  benches, scripts, and other inputs ne cessary to replicate \nthem.  \n Summary   A record explaining why the determination was made that the IP in \nquestion is suitable or unsuitable for use in an LoA3 system.  This summary \nshould also include the facts that led to that determination.  \nIn addition , this documentation must record how the IP was received, how it was stored , \nand how it was  distributed to the reviewers. A process should be put in place to ensure \nthat all reviewers receive the same versions of each file.  \nThis record of review should be  stored in the revision control system , but does not need \nto be distributed as part of the final signed review package.  \n                                                \n1 For example, if a 3PIP makes explicit use of a multiply accumulate block in an Intel FPGA the reviewer must have experience with Intel FPGAs, or pre vious \nrelated Altera FPGAs.  \n\n \nU/OO/170670 -23 | PP-23-1628  | JUN 2023 Ver. 1.0  3 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA3  \n3. Establish suitability  of the 3PIP for review  \nIP can only be reviewed if it is documented and developed using practices that cannot \nbe exploit ed to hide malicious changes. Without the ability to review the IP in its entirety, \nthis process cannot rule out the presence of embedded nefarious functions. This means \nwell-documented hardware description language ( HDL) with clean and meaningful \nnames and reasonable design  structure . Human review and simulation of the IP are not \neffective outside of those conditions. This section introduces a  set of criteria that \ndetermines whether such practices can be  employed.  \nThis step determines if a review of the 3PIP is feasible in a way that will generate \nsufficient assurance. Two  cleared  individual s not working together or on the same team  \ncan conduct this process by reviewing the 3PIP and asking these guiding questions:  \n Is the 3PIP distributed in an encrypted f orm that prevents the reviewers from  \nviewing it?  Is it distributed as a netlist2? \n If yes, the 3PIP is not suitable for review. It  is not recommended for  this \nflow. \n If the IP is delivered as a netlist or hardened core with accompanying \nHDL, a review of this  code is not sufficient for IP approval. A dditional steps \nare needed to ensure the equivalence of the distributed IP (netlist or hard \nmacro) and reviewed HDL files. If this is needed, contact JFAC for \nguidance . \n Is the 3PIP obfuscated3?  \n If yes, the 3PIP is  not suitable for review. It is not recommended for  this \nflow. JFAC can be contacted if the 3PIP is essential; however, the process \nto approve such a 3PIP will be expensive, time consuming, and likely to \nreturn a negative result.  \n Is the 3PIP clearly organized, with distinct modules that are limited in scope with \ninputs/outputs (I/O) limited to those needed to achieve their scope?  \n If no, the 3PIP is not suitable for review. It  is not recommend for this flow. \n                                                \n2 Whether a 3PIP is distributed as a netlist is not determined by the file format, but rather the conte nts. For example, a Verilog file is considered a netlist \ndistribution when a substantial amount of it is implemented with primitive or gate -level elements. This could either be primitives drawn from a library, such as a \nvendor specific primitive library, o r primitives that implement individual gates, such as the and and or elements in Verilog.  \n3 3PIP for which module names, signal names, or any other names, have been replaced with non -meaningful alphanumeric codes or random words is considered \nto be obf uscated. Similarly, HDL that makes repeated use of complex Boolean operators, where it could have used complex functional com mands, such as \nif/then blocks, is considered obfuscated.  \n\n \nU/OO/170670 -23 | PP-23-1628  | JUN 2023 Ver. 1.0  4 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA3  \n Is the 3PIP well documented? This should inc lude comments describing the \npurpose of modules; clear, human -readable names; and top -level documentation \ndescribing the interface.  \n If no, the 3PIP is not suitable for review. It is not recommended for this \nflow. \n Does the 3PIP feature parameterized code an d pre -compiler directives where \nparameters are used  to explicitly generate new HDL4 that is then added to the \nsystem? Is sof tware used to write HDL based on designer input? This includes \nexplicit code that writes code, uses terms like ifdef in Verilog, a nd uses the term \ngenerate in VHDL. This includes parameterized IPs that can be configured via  \nGUI or script.  \n If yes, the 3PIP is still suitable for review. Be aware that the review \nprocess will be longer.  \n Does the 3PIP have a substantial number of tests  distributed with it?  \n If no, the 3PIP is still suitable for review. Be aware that the review process \nwill be longer.  \nFor LoA3, the failure or subversion of critical IP in critical systems has the potential to \ncause grave harm to the U .S. motivating higher levels of IP assurance than in the lower \nassurance levels.  In the case of IP that fail to meet the criteria above, the user should \nchoose one of the following means of recourse:  \n Engage the vendor to provide special contract terms allowing the program to \nobtain unencrypted and unobfuscated releases of the 3PIP.  \n Seek the IP from another provider.  \n Engage another vendor to develop the IP on behalf of the program.  \n Develop the IP within the program . \nIdeally, the program should seek the following from the 3PIP ven dor: \n Fully readable HDL description of the IP.  This should include:  \n Clear organization and hierarchy  \n Clear meaningful name spaces  \n                                                \n4 Generator code that exists solely to add additional logging for debug sh ould not be counted as generator code for these purposes. Generator code added to \nmake a simulation work must be reviewed carefully in the process below, but should not count as generator code for this purpo se. \n\n \nU/OO/170670 -23 | PP-23-1628  | JUN 2023 Ver. 1.0  5 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA3  \n Developed in a hardware description language , such as Verilog or VHDL.  \nIt is not recommended to obtain code in a software prog ramming \nlanguage , such as C or Python.  \n Fully readable constraints files that describe the timing requirements, pin \ndirection and loading without references to generated code, and how it  fully \nconstrains the design.  \n Clear documentation with accompanying rel evant standards.  \n Simulation tests that fully test the 3PIP with 100% coverage if possible. If not \n100% coverage, seek documentation on the areas not tested for the purposes of \nincreased scrutiny.  \n4. Partition the design   \nOnce review suitability is established, the team reviews the overall design of the 3PIP \nand makes a plan to split the 3PIP into one or more functional units that can be \nreviewed and tested independently. This number can vary greatly depending on the \nscale of the 3PIP being reviewed. For some simple 3PIP modules, there may be no \nclear partitioning of the code into meaningful , functional modules. For others, there may \nbe a clear top -level design that quickly yields multiple functional modules.  \n4.1 Threats of interest  \nBecause this process is spe cifically intended to catch LoA3 threats, the review team \nshould  be very thorough in the review process.  At LoA3, this mean s threat s can \nencompass areas that are still known to the public as research.  At LoA3, this cou ld \ninclude attacks such as the  ability to introduce new code into a system design to \nimplement a broa d number of malicious functions, a  denial of service attack , or a \nreduction in reliability not tied to any trigger, which therefore cannot  be controll ed or  \ntimed in a controlled way . \nThe focus of this review is to look for malicious modifications to 3PIP. Because 3PIP \ncan have many origins , as well as many distribution methods, an adversary has many \noptions. In particular, an adversary has an opportunity to modify one copy of 3PIP \ndestined for a particular target , which is also called out in LoA2 attacks.  In LoA3, any \nthreat is in scope and therefore  an in -depth 3PIP review is required.  \nThese attacks are divided into two classes: controlled effects and persistent effects.  \n\n \nU/OO/170670 -23 | PP-23-1628  | JUN 2023 Ver. 1.0  6 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA3  \n4.2 Controlled effects  \nA controlled effect is just that: a specific control mechanism, coupled to a specific effect. \nIn more detail, such an attack contains both of the following : \n A means of command and control   The attack must be externally controlled \nthrough some pre -existing or additiona l control mechanism. Within  LoA3, assume \nthat such an effect must not occur during acceptance testing or typical use of the \ndevice. To achieve this, a trigger must either be sufficiently long and unstructured \nor represent  an out -of-spec behavior in a defined protocol.  \n As a rule of thumb, unstructured triggers of interest must contain, at a \nminimum, 32 bits of unstructured data. Logic to recognize such a \nsequence could be implemented many ways, but any implementation \nwould contain at least 5 bits of state, as well as substantial additional logic \ncomprised of combinatorial and state elements.  \n Structured triggers of interest must occur within the context of a specific \ndefined protocol, and cannot simpl y represent any out -of-spec behavior. \nFor instance, an incorrect cyclic redundancy c heck (CRC ) field is overly \nbroad, as many systems will encounter incorre ct CRC  fields. However, a \nspecific bit error pattern within a CRC field could be sufficient.  \n A usefu l effect   Within  LoA3, useful effects can include specific actions on the \npart of the system, denial of service, leak of data, and so on. The presence of \ndenial of service in this list means that most any functional area in a 3PIP design \nwill cause a usef ul effect . \nExample that meets the criteria  of a controlled effect : A modification to a \ncommunication protocol function that waits  for a specific magic packet before \ndeactivating its operation entirely until the system is reset.   \nThis attack is controllable:  the adversary knows the protocols pa rsed by the core at \ndesign time  and they can understand its role in a specific system. It is use ful: it would \nallow an adversary to deactivate an important function of the device on command. This \nis an exam ple of a controlled e ffect that would be of interest at both LoA2 and LoA3  \nExample that meets  the criteria  of a controlled effect  for LoA3 : A modification to a \nmicrocontroller core deactivates memory protection when a specific sequence of \ninstructions is r un. \n\n \nU/OO/170670 -23 | PP-23-1628  | JUN 2023 Ver. 1.0  7 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA3  \nThis attack meet s the criteria  at LoA3, as this would be considered pre positioning for an \nattack. If this is found, it is assumed there is also a secondary trojan able to take \nadvantage of the memory deactivation and execute alternative code. If this is \ndiscovered, it must be reported and would represent a negative finding.  \n4.3 Persistent effects   \nIn contrast to a controlled effect, a persistent effect lacks an external activation \nmechanism. Instead, it is always or regularly present. Persistent attacks against FPGA \n3PIP are constrained principally by testing performed during development and ordinary \nuse of the device. To be useful , they must evade detection during functional testing \nconducted by the designer and cannot immediately interfere with use of the system. \nThis is entirely feasible depending on the test methodology chosen by the system \ndesigners and the 3PIP block itself.  \nFor this review, the reviewers may assume that two categories of tests  are run on the \n3PIP. The first is the set of any  tests or exemplar use cases distributed with the 3PIP \nitself. The second is a system -level functional test performed to validate that a complete \nsystem using the 3PIP successfully achieves a s ystem -level outcome. When reviewing \na 3PIP block for a specific  program, rather than for general use, the team may also take \nas an input any additional tests developed for the block itself. However, the team should  \nnot assume that the 3PIP developer ran additional tests not shared with the review \nteam.  \nExample that me ets the criteria: A multipoint bus 3PIP provides direct access \nfrom the Joint Test Action Group (JTAG) port of the FPGA to the bus.   \nThis attack is specifically useful, giving the adversary access to arbitrary data on the \nbus, which could easily include cr itical or classified data. It would be expect ed to be \nquickly discovered if placed into a widely distributed piece of 3PIP as soon as a user \ntried to use the JTAG port themselves. However, the attack being mitigated by this \napproach is specifically looking  at the possibility that a vendor receives a special version \ntailored just for them. An adversary in this scenario may have identified that JTAG is \nunused in a given system. This attack also could be detected by extensive testing and \nevaluation of the fina l system. As stated above, reviewers should assume that no test \nwould have involved JTAG if the system was not implementing JTAG intentionally.  \n\n \nU/OO/170670 -23 | PP-23-1628  | JUN 2023 Ver. 1.0  8 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA3  \nExample that failed at lower level LoAs but does meet the  LoA3 criteria: A \nmodification to a communication proto col that  causes  a corruption of 0.01% of \npackets, degrading  system performance . \nAt earlier LoAs this scenario did not meet the criteria because there was no controlled \neffect, but instead degrades the performance at unknown times . At LoA3 this type of \nscenario qualifies.   \n4.4 Defining a functional a rea \nThe remainder of this report discusses how to evaluate each functional area. To do this , \nthe review team must determine the functional area boundaries they want to use. A \nfunctional area is a set of circu it designs that have a specific documented purpose. \nEach functional area should  be reviewed and tested independently. The evaluators can \nset the exact scale of these functional areas. However, as is specified below, depending \non the design practices employ ed, the process may necessitate that functional areas be \ncombined into larger functional areas for evaluation, or generate considerably more \nwork bouncing between regions.  \nThe review team should make a first attempt at defining functional areas based on th e \nfile structure, module documentation, and past experience. In general, small functional \nareas will be easier to evaluate. Design practices that limit the connections to a \nfunctional area to those needed to perform its function will enable small functiona l \nareas. In practice, design best practices encourage limiting connectivity between \nmodules and thus this process should be straightforward.  \n4.5 Criteria for functional areas  of interest  \nBased on the effe cts described at  LoA3, all functions contained withi n the 3PIP are of \ninterest and should be reviewed.  This is in contrast to the limited reviews required at \nlower LoAs due to the fact that failure of the system can cause grave harm to the U .S. \n5. Perform m anual code r eview  \nEach region requires two kinds of reviews: manual and test -driven. These two reviews \nmay influence each other and do not need to be completed in a particular sequence. A \nmanual review is a traditional code review where knowledgeable designers read the \ncode to evaluate it for correctness.  \n\n \nU/OO/170670 -23 | PP-23-1628  | JUN 2023 Ver. 1.0  9 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA3  \nBefore beginning manual code review , automatic tools should be used to generate lines \nof interest within the code. Both a lint tool and the synthesis process must be run. A lint \ntool can be run with settings derived from the apparent code standard used in  \ndevelopment of the source. Any lines identified by the lint code should be marked for \nextra review during the review process. Additionally, the 3PIP should be synthesized \nwith an appropriate FPGA synthesis tool. Any warnings generated should be marked for  \nextra review.  \nEach functional area of interest must be reviewed independently. For functional areas \nthat are inspected for a controlled effect, this review should focus on looking at the data \npath for unexpected behaviors or unexpected connections from th e functional area to \nother functional areas. For persistent effects, the specific hard -to-test function should be \nevaluated in depth for correctness.  \nFor assessing a controllable effect, the following factors must be evaluated : \n Where does the information f rom the I/O propagate?  \n If it propagates largely intact or is transformed in a simple way to other \nfunctional modules, then they must be added as functional areas of \ninterest.  \n What logic handles the I/O? The logic within the functional area that processes \nthe I/O must be evaluated for signs of a trigger : \n If the logic is simple , then accounting for the state elements can be \nsufficient to remove it from consideration. This is possible when many bits \nof previous input are not stored natively within the functio nal area. In these \ncases, that storage, or a finite state machine  (FSM) , would need to be \nadded in order to add a trigger.  \n If the logic is complex, it will require additional investigation : \n One phase of the investigation should focus on reviewing the code \nfor an unexpected, but explicit trigger. That is to say, specific lines \nof HDL or specific groups of gates, waiting for a specific command.  \n The second phase of the investigation should focus on more subtle \neffects. In particular, this should look at each FSM state not \ncovered by a test, and validate that it has a well -documented and \nmeaningful purpose. Special attention is needed if multiple \ninteracting FSMs  are present. In those cases, additional in -depth \n\n \nU/OO/170670 -23 | PP-23-1628  | JUN 2023 Ver. 1.0  10 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA3  \nconsideration must be paid to the interactions betw een FSMs . \nParticular attention must be given to out -of-spec behavior in known \nprotocols.  \n Commercial tools used to identify trigger -like logic can also be used as \nan additional check for controllable effect logic.  This tool type se arches  for \nlogic that ha s a very low probability of being toggled by a very specific set \nof values. Its use can be very helpful in highlighting logic for closer \ninspection.  Contact JFAC for more information on these tools.  \n State machines  state spaces must be fully understood and  must prevent \nentrance into an unknown state or loop. Additionally, their output control signals \nmust be understood.  \nFor investigating a persistent effect, the process is more complex  and appropriate \nexperts must determine whether each such block is proper ly implemented. They must \nfocus on the correctness and suitability of the implementation, with the entire \nimplementation being carefully reviewed. Review of persistent effects is more \ndependent on the test-driven code review.  Of particular focus, state machines and \ncounters must be fully evaluated and understood.  \n6. Perform t est-driven code r eview  \nAs part of the review process, simulation tests must be used. The simulation testing can \nbe performed at the top 3PIP level, the functional module level, or a mix  of the two. With \nline-by-line code review, the test  benches that were provided with the 3PIP may be \nused as part of these tests.  JFAC will provide g uidance for blocks too large to efficiently \nsimulate.  \nFor regions that are being investigated for a control led effect, sufficient tests must be \ndeveloped such that all major desired functions are executed. In some cases, this may \ncorrespond to a requirements document or to the functions listed in a specification. In \naddition to that, code coverage5 must be reco rded and any lines not included in the final \ncode coverage must be reviewed in detail, with a specific note identifying why they are \nnot of interest in the context of their functional module.  \n                                                \n5 For the purposes of this specific document,  code coverage is measured by statement and branch coverage, not signal or gate.  \n\n \nU/OO/170670 -23 | PP-23-1628  | JUN 2023 Ver. 1.0  11 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA3  \nFor regions that are being investigated for a persistent effect,  sufficient tests must be \ndeveloped such that all major desired functions are executed. In addition to this, \nconstrained random testing must be performed on the 3PIP. Code coverage must be \nrecorded. In some cases , such as redundant serial safety checks, it  may be impossible \nto achieve 100% code coverage. For each line not covered in the final code coverage, a \nspecific note must be recorded  identifying why a test  bench did not reach that line and \nwhy it is acceptable in the context of the larger system.  \nIn both cases, formal methods that prove equivalency between another reference \ndesign that has been vetted through this process and a 3PIP or functional module can \nsubstitute for the use of testing as described above. Similarly reviewed software blocks \ncan als o be used as a reference in this formal verification process.  \n7. Additional c onsiderations  \nWhile code coverage, simulation, linting , and HDL review are powerful tools in detecting \nand avoiding the introduction of malicious functions into a system via 3PIP, there are \nsome additional steps that can be taken to help mitigate this possibility.  \nDisable u nused features   in the case of complex IP, it is often the case that not all \nthe functionality of the block is being used by the systems.  In these cases, it is \nbeneficial to disable those features that are not being used.  In some cases, this can be \naccomplished by cont rolling certain top  level 3PIP input pins.  \nRestrict 3PIP to l egal inputs   in cases where the application does not control all the \ninputs to the 3PIP, the application can be modified to restrict the inputs to a recognized \nset of approved values.  These kind s of inputs include any values that are generated off \nchip and delivered to the IO  (input/output)  of the 3PIP.  Communication protocols would \nbe included in this group.  These input values can be filtered to only allow legal, \nexpected values to reach the fun ction.  Examples of this could be a 32 -bit address bus \nthat only carries 32 valid addresses.  The others should be restricted from reaching the \n3PIP.  Another example would be formatted serial data with unused bits.  These could be \nforced to a known value prio r to being delivered to the IP to prevent their use as a \nmalicious trigger.  \n\n \nU/OO/170670 -23 | PP-23-1628  | JUN 2023 Ver. 1.0  12 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA3  \nRestri ct 3PIP to legal o utputs   in this case, the output of the 3PIP is monitored for \nexpected legal values and filtered out when the value is not expected.  This can assist in \npreventing the IP from being used to subvert the application.  \n8. Document  and sign the review p ackage  \nA review package should summarize the results of the review and minimally provide the \nfollowing  items : \n The name of the vendor who sells the 3PIP ; \n The name of the 3PIP ; \n Any version information of the 3PIP ; \n The hash of the 3PIP package delivered by the vendor6; \n A list of any use restrictions that were ident ified in the review of the 3PIP ; \n The hash of a summary document detailing the review process, including all \nauditable information specified in the prerequisites section of this document ; \n The auditing organization that maintains  this summary document ; \n The portion of the summary document above not containing personally \nidentifiable information ; \n The hash of all tes t benches and scripts used in the review process ; \n If allowed by the 3PIP vendor , the collection of the test benches and scripts used \nin the review process ; \n A summary hash of all the listed elements above, including hashes of packages \nreviewed but not the p ackage itself ; and  \n A cryptographic signature of the above information, tied to the reviewing \norganization.  \nThis set of information is a tradeoff designed to produce a distributable report not \nconstrained by agreements  that can be validated back to the spec ific files. The hashes \nin this report must be validated against the 3P IP for it to be used, and the report must \nbe maintained as a design artifact for future audits. For instance, explicitly include a \nhash of the 3PIP, but not the 3PIP itself, so that othe r users can determine if they have \nthe same files without needing to include the files themselves in the document.  \n                                                \n6 This review will be valid solely for 3PIP packages that match this hash.  \n\n \nU/OO/170670 -23 | PP-23-1628  | JUN 2023 Ver. 1.0  13 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA3  \nShould an issue be identified during the review process , it should be reported to the \norganization requesting the r eview.  In the case of Depa rtment of Defense  organizations, \nany identified issue must be shared with the JFAC.  \n9. Pre-compiler and other machine -generated c ode \nMachine -generated code refers to any code that takes a set of parameters and uses \nthem to construct or pre -process HDL before a synthesis process. This can be custom \nC code that generates Verilog. This can be compiler directives such as ifdef in Verilog \nor generate in VHDL.  \nReview of a 3PIP block containing machine -generated code can be substantially more \nchallenging, and, de pending on the complexity of the pre -compiler directives or scripts, \nmay not be possible. Instead, it may require review with specific settings in place. \nContact JFAC for additional guidance  for handling machine -generated code.  \nWhen reviewing 3PIP for use in a specific system, it is acceptable to run the pre -\ncompiler with the desired parameters and evaluate the result. In this case, the \ngenerated HDL must comply with the same standards as specified for any 3PIP block. \nIn addition, the review package must no te those precise settings as a use restriction on \nthe 3PIP.  \n10. Conclusion  \nFor review of a 3PIP in general, the following guidelines govern the review:  \n Assemble review p rerequisites:  \n In cases where software is used to write HDL, members of the review \nteam that review the generator code must have experience in both the \nlanguage that the generator is written in and the language that is \ngenerated. For instance, to review C code that generates VHDL, the \nreviewer must have expertise in both.  \n Establish suitability of the 3PIP for review:  \n Are the purposes of all uses of pre -compiler directives or scripts clear and \nwell documented?  \n If not, the code is not suitable for review.  \n Are the parameters of the IP well documented with clear constraints?  \n If not, the code is not sui table to review.  \n\n \nU/OO/170670 -23 | PP-23-1628  | JUN 2023 Ver. 1.0  14 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA3  \n Is the function of the 3PIP limited to a specific  function, rather than an \narbitrary programmable set of functions7? \n If so, the code is suitable to review.  \n Partition the design : \n Partitions must be made with specific regard to the boundarie s of the pre -\ncompiler directives. The I/O between partitions should not dynamically \nchange based on parameters outside of width changes.  \n Perform manual  code r eview : \n Manual code review must be performed with the pre -compiler directives in \nplace. The generat or itself must be reviewed.  \n Perform test-driven code r eview : \n Simulations must be done on a representative sample of \nparameterizations.  \n All conditional code blocks within ifdef, generate, or similar statements \nmust be included in at least one of the sam ples simulated, though \nindividual lines within a block may be reviewed as in the normal flow.  \n Document  and sign the review package : \n List the specific details of all parameter sets used in the evaluation.  \n Include this  data in the final summary hash.  \n  \n                                                \n7 For example, a parameter specifying the length of a filter is not problematic. A parameter indicat ing whether a CPU has a JTAG port is not problematic. A \nparameter that specifies a complex equation or programmatic behavior to implement is problematic.  \n\n \nU/OO/170670 -23 | PP-23-1628  | JUN 2023 Ver. 1.0  15 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA3  \nAppen dix A: Standardized terminology  \nThe following terms are used in the Joint Federated Assurance Center Field \nProgrammable Gate Array Best Practices documents. These terms are modified from \nDefense Acquisition University definitions to support common understanding.  \nApplication design   The collection of schematics, constraints, hardware description \nlanguage (HDL), and other implementation files developed to generate an FPGA \nconfiguration file for use on one or many FPGA platforms.  \nApplication domain   This is the area of technology of the s ystem itself, or a directly \nassociated area of technology. For instance, the system technology domain of a radar \nsystem implemented using FPGAs would be \"radar\" or \"electronic warfare.\"  \nConfiguration file   The set of all data produced by the application d esign team and \nloaded into an FPGA to personalize it. Referred to by some designers as a bitstream, \nthe configuration file includes that information, as well as additional configuration \nsettings and firmware, which some designers may not consider part of  their bitstream.  \nControllable effect   Program -specific, triggerable function allowing the adversary to \nattack a specific target.  \nDevice/FPGA device   A specific physical instantiation of an FPGA.  \nExternal facility   An unclassified facility that is out  of the control of the program or \ncontractor.  \nField programmable gate array (FPGA)   In this context FPGA includes the full range \nof devices containing substantial reprogrammable digital logic. This includes devices \nmarketed as FPGAs, complex programmable logic devices (CPLD), system -on-a-chip \n(SoC) FPGAs, as well as devices marketed as SoCs and containing reprogrammable \ndigital logic capable of representing arbitrary functions. In addition, some FPGAs \nincorporate analog/mixed signal elements alongside subs tantial amounts of \nreprogrammable logic.  \nFPGA platform   An FPGA platform refers to a specific device type or family of devices \nfrom a vendor.  \n\n \nU/OO/170670 -23 | PP-23-1628  | JUN 2023 Ver. 1.0  16 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA3  \nHard IP   Hard IP is a hardware design captured as a physical layout, intended to be \nintegrated into a hardware design in the layout process. Hard IP is most typically \ndistributed as Graphic Design System II (GDSII). In some cases, Hard IP is provided by \na fabrication company and the user of the IP does not have access to the full layout, but \nsimply a size and the i nformation needed to connect to it. Hard IP may be distributed \nwith simulation hardware description language (HDL) and other soft components, but is \ndefined by the fact that the portion that ends up in the final hardware was defined by a \nphysical layout by  the IP vendor.  \nLevel of assurance (LoA)   A Level of Assurance is an established guideline that \ndetails the appropriate mitigations necessary for the implementation given the impact to \nnational security associated with subversion of a specific system, wit hout the need for \nsystem -by-system custom evaluation.  \nPhysical unclonable function (PUF)   This function provides a random string of bits of \na predetermined length. In the context of FPGAs, the randomness of the bitstring is \nbased upon  variations in the si licon of the device due to manufacturing. These bitstrings \ncan be used for device IDs or keys.   \nPlatform design   The platform design is the set of design information that specifies \nthe FPGA platform, including physical layouts, code, etc.  \nSoft IP   Soft I P is a hardware design captured in hardware description language \n(HDL), intended to be integrated into a complete hardware design through a synthesis \nprocess. Soft IP can be distributed in a number of ways, as functional HDL or a netlist \nspecified in HDL, encrypted or unencrypted.  \nSystem   An aggregation of system elements and enabling system elements to achieve \na given purpose or provide a needed capability.  \nSystem design  System design is the set of information that defines the \nmanufacturing, behavior, a nd programming of a system. It may include board designs, \nfirmware, software, FPGA configuration files, etc.  \nTarget  A target refers to a specific deployed instance of a given system, or a specific \nset of systems with a common design and function.  \n\n \nU/OO/170670 -23 | PP-23-1628  | JUN 2023 Ver. 1.0  17 \nNational Security Agency | Cybersecurity Technical Report  \nDoD Microelectronics: Third -Party IP Review Process for LoA3  \nTargeta bility  The degree to which an attack may have an effect that only shows up in \ncircumstances the adversary chooses. An attack that is poorly targetable would be more \nlikely to be discovered accidentally, have unintended consequences, or be found in \nstanda rd testing.  \nThird -party intellectual property (3PIP)   Functions whose development are not \nunder the control of the designer. Use of the phrase intellectual property, IP, or 3PIP in \noutlining this methodology of design review does not refer to property r ights, such as, \nfor example, copyrights, patents, or trade secrets. It is the responsibility of the party \nseeking review and/or the reviewer to ensure that any rights needed to perform the \nreview in accordance with the methodology outlined are obtained.  \nThreat category  A threat category refers to a part of the supply chain with a specific \nattack surface and set of common vulnerabilities against which many specific attacks \nmay be possible.  \nUtility  The utility of an attack is the degree to which an effect  has value to an \nadversarial operation. Higher utility effects may subvert a system or provide major \ndenial of service effects. Lower utility attacks might degrade a capability to a limited \nextent.  \nVulnerability  A flaw in a software, firmware, hardware,  or service component \nresulting from a weakness that can be exploited, causing a negative impact to the \nconfidentiality, integrity, or availability of an impacted component or components.  \n\n",
  "cves": [],
  "techniques": [],
  "advisory": "cybersecurity-alerts",
  "title": "ctr_dod_microelectronics-third_party_ip_review_process_for_loa3",
  "source": "nsa",
  "id": "a810ba0177c4874e9b9f5c9f38ff7038398461f8cb00511000579927bcfb8ffa"
}